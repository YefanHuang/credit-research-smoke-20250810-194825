#!/usr/bin/env python3
"""
Unified Research Analysis Script
Extracted from GitHub Actions workflow to avoid expression length limits
"""

import asyncio
import os
import sys
import json
import smtplib
import time
from typing import List

from datetime import datetime
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email import encoders
from email.utils import formataddr
import urllib3
import urllib.parse
import aiofiles
from pathlib import Path

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

async def main():
    """Main analysis function"""
    try:
        # Get inputs from environment
        search_query_template = os.getenv('SEARCH_QUERY_TEMPLATE', 
            '{topic} research analysis report')
        topics = [t.strip() for t in os.getenv('TOPICS', 'credit rating methodology').split(',')]
        enable_vectorization = os.getenv('ENABLE_VECTORIZATION', 'true').lower() == 'true'
        strict_time_filter = os.getenv('STRICT_TIME_FILTER', 'true').lower() == 'true'

        recipient_email = os.getenv('RECIPIENT_EMAIL', '')
        sender_name = os.getenv('SENDER_NAME', 'Credit Research Bot')
        
        # ä¸¥æ ¼è´¹ç”¨å’Œè´¨é‡æ§åˆ¶
        MAX_REQUESTS_PER_RUN = int(os.getenv('MAX_SEARCH_REQUESTS', '5'))  # ç”¨æˆ·å¯é…ç½®ï¼Œæœ€å¤š5ä¸ª
        MAX_TOKENS_PER_RUN = 50000  # ç¡¬é™åˆ¶ï¼šæœ€å¤š50000 tokens  
        
        # éªŒè¯å¹¶è®¾ç½®æ—©åœé˜ˆå€¼ (0.3-0.98)
        threshold_input = os.getenv('EARLY_STOP_THRESHOLD', '0.85')
        try:
            EARLY_STOP_THRESHOLD = float(threshold_input)
            if not (0.3 <= EARLY_STOP_THRESHOLD <= 0.98):
                print(f"âš ï¸ Warning: threshold {EARLY_STOP_THRESHOLD} out of range (0.3-0.98), using 0.85")
                EARLY_STOP_THRESHOLD = 0.85
        except ValueError:
            print(f"âš ï¸ Warning: invalid threshold '{threshold_input}', using 0.85")
            EARLY_STOP_THRESHOLD = 0.85
        
        current_requests = 0
        current_tokens = 0
        early_stop_triggered = False
        best_score = 0.0
        
        print(f"ğŸ¯ Starting Unified Research Analysis")
        print(f"ğŸ“‹ Topics: {', '.join(topics)}")
        print(f"ğŸ§® Vector Enhancement: {'âœ… Enabled' if enable_vectorization else 'âŒ Disabled'}")
        print(f"ğŸ“… Strict 7-Day Filter: {'âœ… Enabled' if strict_time_filter else 'âŒ Disabled'}")
        print(f"ğŸ“§ Recipient: {recipient_email or 'Not set'}")
        print(f"ğŸ’° Hard Limits: {MAX_REQUESTS_PER_RUN} requests OR {MAX_TOKENS_PER_RUN} tokens â†’ forced abort")
        print(f"ğŸ¯ Early Stop: ChromaDB score â‰¥ {EARLY_STOP_THRESHOLD} â†’ task complete")  
        print(f"ğŸ” Search Model: sonar (æˆæœ¬æ•ˆç›Šæ¨¡å‹ï¼Œä¸²è¡Œæ‰§è¡Œ)")
        print(f"âš¡ Async: Only LLM + embedding allowed")
        
        # éªŒè¯å…³é”®ç¯å¢ƒå˜é‡
        required_keys = ['PERPLEXITY_API_KEY', 'QWEN_API_KEY']  # æœç´¢éœ€è¦PERPLEXITYï¼ŒLLMå’Œembeddingéœ€è¦QWEN
        qwen_key = os.getenv('QWEN_API_KEY')
        
        missing_keys = [key for key in required_keys if not os.getenv(key)]
        if missing_keys:
            print(f"âš ï¸ Missing API keys: {', '.join(missing_keys)}")
        else:
            print("âœ… All required API keys are present")
            
        # æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§
        if qwen_key:
            print("âœ… QWEN models available (llm + embedding)")
        else:
            print("âš ï¸ QWEN models unavailable - need QWEN_API_KEY for llm and embedding")
        
        # Import unified model manager
        # Try multiple possible paths
        possible_paths = [
            '/github/workspace/oop',
            './oop',
            '../oop',
            os.path.join(os.getcwd(), 'oop')
        ]
        
        print(f"ğŸ” Current working directory: {os.getcwd()}")
        
        model_manager_imported = False
        for path in possible_paths:
            try:
                if os.path.exists(path):
                    print(f"ğŸ” Trying path: {path}")
                    sys.path.insert(0, path)
                    from model_manager import call_search, call_embedding, call_llm
                    print("âœ… Unified model manager imported successfully")
                    model_manager_imported = True
                    break
            except ImportError as e:
                print(f"âš ï¸ Failed to import from {path}: {e}")
                continue
        
        if not model_manager_imported:
            print("âŒ Failed to import unified model manager from all paths")
            print("ğŸ“ Checking available files...")
            
            # Check all possible locations
            for base_path in ['/github/workspace', '.', '..']:
                oop_path = os.path.join(base_path, 'oop')
                if os.path.exists(oop_path):
                    print(f"ğŸ“‚ Found oop directory at: {oop_path}")
                    oop_files = os.listdir(oop_path)
                    for f in oop_files:
                        if f.endswith('.py'):
                            print(f"  - {f}")
                else:
                    print(f"ğŸ“‚ No oop directory at: {oop_path}")
            return False  # æ˜ç¡®è¿”å›å¤±è´¥çŠ¶æ€
        
        # Define helper functions for intelligent segmentation
        async def intelligent_segmentation_llm(text: str, max_chunk_size: int = 600) -> List[str]:
            """æ™ºèƒ½æ–‡æœ¬åˆ‡åˆ† - ä½¿ç”¨LLMæ¨¡å‹"""
            try:
                prompt = f"""Please intelligently segment the following text into semantically complete paragraphs, with each segment not exceeding {max_chunk_size} characters:

{text[:2000]}  # limit input length

Requirements:
1. Maintain semantic integrity and coherence
2. Keep each segment under {max_chunk_size} characters  
3. Preserve credit research technical terms and key information
4. Segment by logical topics (e.g., policy analysis, technology development, market data)
5. Remove reference numbers at the end of sentences or paragraphs (e.g., remove "1" from "investors.1" or "2" from "risk.2")
6. Clean up text transitions (e.g., "investors.1In this paper" should become "investors. In this paper")
7. Return format: one segment per line, separated by "---"

Ensure output is in English.
"""
                
                print(f"ğŸ”§ è°ƒç”¨LLMæ¨¡å‹è¿›è¡Œæ™ºèƒ½æ–‡æœ¬åˆ‡åˆ†...")
                result = await call_llm(prompt, model_alias="llm", max_tokens=2000, temperature=0.1)
                
                if result.get('success'):
                    response_text = result.get('content', '')
                    print(f"ğŸ“Š LLM APIå“åº”: æ¨¡å‹={result.get('model', 'unknown')}, æä¾›å•†={result.get('provider', 'unknown')}")
                    
                    chunks = [chunk.strip() for chunk in response_text.split("---") if chunk.strip()]
                    
                    if chunks and len(chunks) > 0:
                        # è¿‡æ»¤è¿‡é•¿çš„chunk
                        filtered_chunks = []
                        for chunk in chunks:
                            if len(chunk) <= max_chunk_size:
                                filtered_chunks.append(chunk)
                            else:
                                # å¯¹è¿‡é•¿chunkè¿›è¡Œç®€å•åˆ‡åˆ†
                                sub_chunks = simple_split(chunk, max_chunk_size)
                                filtered_chunks.extend(sub_chunks)
                        
                        print(f"âœ… LLMæ™ºèƒ½åˆ‡åˆ†å®Œæˆ: {len(text)} å­—ç¬¦ â†’ {len(filtered_chunks)} ä¸ªè¯­ä¹‰å—")
                        return filtered_chunks
                    else:
                        print("âš ï¸ LLMåˆ‡åˆ†è¿”å›ç©ºç»“æœï¼Œä½¿ç”¨ç®€å•åˆ‡åˆ†")
                else:
                    print(f"âŒ LLMåˆ‡åˆ†è°ƒç”¨å¤±è´¥: {result.get('error', 'Unknown error')}")
                    
            except Exception as e:
                print(f"âš ï¸ LLMåˆ‡åˆ†å¼‚å¸¸: {e}")
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•åˆ‡åˆ†
            return simple_split(text, max_chunk_size)
        
        def simple_split(text: str, max_size: int) -> List[str]:
            """ç®€å•ä½†æ™ºèƒ½çš„æ–‡æœ¬åˆ‡åˆ†ï¼ˆæŒ‰å¥å­è¾¹ç•Œï¼‰"""
            chunks = []
            import re
            
            # æŒ‰å¥å·ã€æ„Ÿå¹å·ã€é—®å·åˆ†å¥
            sentences = re.split(r'(?<=[.!?])\s+', text)
            
            current_chunk = ""
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                    
                test_chunk = current_chunk + " " + sentence if current_chunk else sentence
                
                if len(test_chunk) <= max_size:
                    current_chunk = test_chunk
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence
            
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            print(f"âœ‚ï¸ ç®€å•åˆ‡åˆ†: {len(text)} å­—ç¬¦ â†’ {len(chunks)} ä¸ªå—")
            return chunks
        
        # Step 1: Execute searches SERIALLY (not concurrent) with progress tracking
        print(f"\nğŸ” Step 1: Executing Serial Searches (Perplexityä¸å…è®¸å¼‚æ­¥)...")
        search_start_time = time.time()
        
        def print_progress(current, total, desc="Progress"):
            """æ‰“å°è¿›åº¦æ¡"""
            percent = (current / total) * 100
            bar_length = 20
            filled = int(bar_length * current / total)
            bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
            print(f"ğŸ“Š {desc}: [{bar}] {percent:.1f}% ({current}/{total})")
        
        async def search_topic_serial(topic, topic_index, total_topics):
            """ä¸²è¡Œæœç´¢å•ä¸ªtopic"""
            nonlocal current_requests, current_tokens, early_stop_triggered, best_score
            
            # æ£€æŸ¥æ—©åœæ¡ä»¶
            if early_stop_triggered:
                print(f"ğŸ¯ {topic}: SKIP - Early stop triggered (best score: {best_score:.3f})")
                return []
            
            # æ£€æŸ¥ç¡¬é™åˆ¶
            if current_requests >= MAX_REQUESTS_PER_RUN:
                print(f"ğŸš¨ {topic}: ABORT - Request limit reached ({current_requests}/{MAX_REQUESTS_PER_RUN})")
                return []
            
            if current_tokens >= MAX_TOKENS_PER_RUN:
                print(f"ğŸš¨ {topic}: ABORT - Token limit reached ({current_tokens}/{MAX_TOKENS_PER_RUN})")
                return []
            
            # æ˜¾ç¤ºè¿›åº¦
            print_progress(topic_index, total_topics, f"Searching Topics")
            
            search_query = search_query_template.format(topic=topic)
            print(f"ğŸ” [{topic_index}/{total_topics}] Searching: {topic} (sonar)")
            print(f"   Query: {search_query}")
            print(f"   Status: {current_requests}/{MAX_REQUESTS_PER_RUN} requests, {current_tokens}/{MAX_TOKENS_PER_RUN} tokens")
            
            try:
                # ä¸²è¡Œç­‰å¾…ï¼ˆä¸ä½¿ç”¨asyncio.sleepå› ä¸ºè¿™ä¸æ˜¯å¼‚æ­¥ï¼‰
                import time
                time.sleep(1.0)  # sonaræ¨¡å‹é—´éš”å¯ä»¥çŸ­ä¸€äº›
                
                response = await call_search(
                    prompt=search_query,  # ä¿®å¤å‚æ•°åï¼špromptè€Œä¸æ˜¯query
                    model_alias="search",  # ä½¿ç”¨æœ€æ–°æ¨¡å‹
                    strict_time_filter=strict_time_filter,
                    topic=topic
                )
                
                if response.get('success'):
                    results = response.get('results', [])
                    # é™åˆ¶æ¯ä¸ªtopicæœ€å¤šè¿”å›5ä¸ªç»“æœ
                    limited_results = results[:5]
                    
                    # æ›´æ–°è®¡æ•°å™¨å¹¶æ£€æŸ¥é™åˆ¶
                    current_requests += 1
                    usage = response.get('usage', {})
                    if usage:
                        topic_tokens = usage.get('total_tokens', 0)
                        current_tokens += topic_tokens
                        # sonarè®¡è´¹: $1 per 1M tokens + $5 per 1K requests
                        cost_tokens = (topic_tokens / 1000000) * 1.0  # sonar: $1/1M tokens
                        cost_request = 0.005  # $5/1000
                        print(f"ğŸ’° {topic}: 1 req (${cost_request:.3f}) + {topic_tokens} tokens (~${cost_tokens:.3f})")
                    else:
                        print(f"ğŸ’° {topic}: 1 request (~$0.005)")
                    
                    # æ£€æŸ¥tokené™åˆ¶
                    if current_tokens >= MAX_TOKENS_PER_RUN:
                        print(f"ğŸš¨ TOKEN LIMIT REACHED! {current_tokens}/{MAX_TOKENS_PER_RUN} - FORCE ABORT")
                        # å†™å…¥å¼ºåˆ¶ä¸­æ–­è­¦å‘Š
                        with open(os.environ.get('GITHUB_STEP_SUMMARY', '/dev/null'), 'a') as f:
                            f.write(f"\nğŸš¨ **FORCE ABORT**: Token limit reached ({current_tokens}/{MAX_TOKENS_PER_RUN})\n")
                    
                    print(f"âœ… {topic}: Found {len(results)} results, using {len(limited_results)}")
                    print(f"ğŸ“Š Running totals: {current_requests}/{MAX_REQUESTS_PER_RUN} requests, {current_tokens}/{MAX_TOKENS_PER_RUN} tokens")
                    
                    # è¿”å›ç»“æœï¼Œç¨ååœ¨å‘é‡åŒ–åæ£€æŸ¥æ—©åœæ¡ä»¶
                    return limited_results
                else:
                    error_msg = response.get('error', 'Unknown error')
                    print(f"âš ï¸ {topic}: Search failed - {error_msg}")
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è¯·æ±‚é™åˆ¶é”™è¯¯
                    if 'rate' in error_msg.lower() or 'limit' in error_msg.lower() or '429' in error_msg:
                        print(f"ğŸš¨ {topic}: Rate limit or quota error - {error_msg}")
                        # å†™å…¥workflowæ‘˜è¦
                        with open(os.environ.get('GITHUB_STEP_SUMMARY', '/dev/null'), 'a') as f:
                            f.write(f"\nğŸš¨ **Rate Limit Error**: Topic '{topic}' failed with rate limit: {error_msg}\n")
                    
                    return []
                    
            except Exception as e:
                print(f"âŒ {topic}: Search exception - {e}")
                return []
        
        # æ£€æŸ¥è¯·æ±‚æ•°é‡é™åˆ¶
        if len(topics) > MAX_REQUESTS_PER_RUN:
            print(f"âš ï¸ Too many topics ({len(topics)}) - limiting to {MAX_REQUESTS_PER_RUN}")
            topics = topics[:MAX_REQUESTS_PER_RUN]
            # å†™å…¥workflowè­¦å‘Š
            github_summary = os.environ.get('GITHUB_STEP_SUMMARY', '/dev/null')
            if github_summary != '/dev/null':
                with open(github_summary, 'a') as f:
                    f.write(f"\nâš ï¸ **Budget Control**: Limited topics to {MAX_REQUESTS_PER_RUN}\n")
        
        # Define helper functions first
        async def check_early_stop_condition(topic_results, topic_name):
            """æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ—©åœæ¡ä»¶"""
            nonlocal early_stop_triggered, best_score
            
            if not topic_results:
                return
                
            print(f"ğŸ§® Quick vector check for early stop: {topic_name}")
            try:
                # å¿«é€Ÿå‘é‡åŒ–ç¬¬ä¸€ä¸ªç»“æœæ£€æŸ¥è´¨é‡
                first_result = topic_results[0]
                full_text = f"{first_result.get('title', '')} {first_result.get('content', '')}"
                
                # æ™ºèƒ½åˆ‡åˆ†ç¬¬ä¸€ä¸ªç»“æœ
                print(f"ğŸ”§ Early stopæ£€æŸ¥: æ™ºèƒ½åˆ‡åˆ†æ–‡æœ¬...")
                chunks = await intelligent_segmentation_llm(full_text, max_chunk_size=600)
                
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªchunkè¿›è¡Œæ—©åœæ£€æŸ¥
                test_text = chunks[0] if chunks else full_text[:500]
                
                print(f"ğŸ”§ è°ƒç”¨embeddingæ¨¡å‹è¿›è¡Œæ—©åœæ£€æŸ¥...")
                embedding_response = await call_embedding([test_text])
                
                if embedding_response.get('success'):
                    embeddings = embedding_response.get('embeddings', [])
                    print(f"ğŸ“Š Embedding APIå“åº”: æ¨¡å‹={embedding_response.get('model', 'unknown')}, æä¾›å•†={embedding_response.get('provider', 'unknown')}")
                    if embeddings:
                        # æ‰§è¡ŒChromaDBæŸ¥è¯¢æ£€æŸ¥è¯„åˆ†
                        score = await quick_chromadb_check(embeddings[0])
                        if score > best_score:
                            best_score = score
                            
                        if score >= EARLY_STOP_THRESHOLD:
                            early_stop_triggered = True
                            print(f"ğŸ¯ EARLY STOP! ChromaDB score {score:.3f} â‰¥ {EARLY_STOP_THRESHOLD}")
                            # å†™å…¥early stopé€šçŸ¥
                            with open(os.environ.get('GITHUB_STEP_SUMMARY', '/dev/null'), 'a') as f:
                                f.write(f"\nğŸ¯ **EARLY STOP**: Quality threshold reached (score: {score:.3f} â‰¥ {EARLY_STOP_THRESHOLD})\n")
                        else:
                            print(f"ğŸ“Š Quality check: {score:.3f} < {EARLY_STOP_THRESHOLD} (continue searching)")
            except Exception as e:
                print(f"âš ï¸ Early stop check failed: {e}")
        
        async def quick_chromadb_check(embedding):
            """å¿«é€ŸChromaDBè¯„åˆ†æ£€æŸ¥"""
            try:
                import chromadb
                
                chromadb_paths = [
                    "/github/workspace/traindb/chroma_db",
                    "./traindb/chroma_db",
                    "./chroma_db"
                ]
                
                for path in chromadb_paths:
                    try:
                        if os.path.exists(path):
                            chroma_client = chromadb.PersistentClient(path=path)
                            collections = chroma_client.list_collections()
                            if collections:
                                collection = collections[0]
                                query_result = collection.query(
                                    query_embeddings=[embedding],
                                    n_results=1,
                                    include=['distances']
                                )
                                
                                if query_result and query_result.get('distances'):
                                    distances = query_result['distances'][0]
                                    if distances:
                                        similarity = 1.0 - min(distances)
                                        return max(0.0, min(1.0, similarity))
                                break
                    except Exception:
                        continue
                        
                return 0.5  # é»˜è®¤è¯„åˆ†
            except Exception:
                return 0.5
        
        # Execute searches SERIALLY (one by one, not concurrent)
        all_results = []
        total_topics = len(topics)
        
        for i, topic in enumerate(topics, 1):
            if early_stop_triggered:
                print(f"ğŸ¯ Early stop activated - skipping remaining {total_topics - i + 1} topics")
                break
                
            result = await search_topic_serial(topic, i, total_topics)
            all_results.append(result)
            
            # å¦‚æœæ­¤topicæœ‰ç»“æœï¼Œè¿›è¡Œå³æ—¶å‘é‡åŒ–æ£€æŸ¥early stop
            if result and enable_vectorization and qwen_key:
                await check_early_stop_condition(result, topic)
        
        search_time = time.time() - search_start_time
        print(f"â±ï¸ Serial search completed in {search_time:.2f}s")
        if early_stop_triggered:
            print(f"ğŸ¯ Early stop was triggered! Best score: {best_score:.3f}")
        
        # Flatten results  
        search_results = []
        for i, results in enumerate(all_results):
            if isinstance(results, Exception):
                if i < len(topics):
                    print(f"âš ï¸ Topic {topics[i]} failed with exception: {results}")
                continue
            
            for result in results:
                if i < len(topics):
                    result['topic'] = topics[i]
                search_results.append(result)
        
        print(f"ğŸ“Š Total results collected: {len(search_results)}")
        
        # é™åˆ¶æ€»ç»“æœæ•°é‡ï¼Œé˜²æ­¢è¿‡è½½
        if len(search_results) > 25:  # æœ€å¤šå¤„ç†25ä¸ªç»“æœ
            search_results = search_results[:25]
            print(f"âš ï¸ Limited to {len(search_results)} results to prevent overload")
        
        # ä¸å†æ·»åŠ æ¨¡æ‹Ÿæ•°æ®ï¼Œç›´æ¥ä½¿ç”¨å®é™…æœç´¢ç»“æœ
        if len(search_results) == 0:
            print("âš ï¸ No search results found - check API connectivity and prompts")
        else:
            print(f"âœ… Found {len(search_results)} real search results")
        
        # Step 2: Vector Enhancement (if enabled)
        enhanced_results = search_results
        if enable_vectorization and len(search_results) > 0 and qwen_key:
            print("\nğŸ§® Step 2: Vector Enhancement...")
            
            try:
                # Step 2.1: Intelligent Segmentation before vectorization
                print(f"ğŸ”§ Step 2.1: æ™ºèƒ½æ–‡æœ¬åˆ‡åˆ†...")
                
                texts_to_vectorize = []
                for i, result in enumerate(search_results[:15]):  # æœ€å¤šå¤„ç†15ä¸ªç»“æœ
                    content = result.get('content', '')
                    title = result.get('title', '')
                    
                    if content:
                        print(f"ğŸ“„ å¤„ç†ç»“æœ {i+1}: {title[:50]}...")
                        
                        # Call LLM for intelligent segmentation
                        full_text = f"{title}\n\n{content}"
                        chunks = await intelligent_segmentation_llm(full_text, max_chunk_size=600)
                        
                        # Add chunks to vectorization list
                        texts_to_vectorize.extend(chunks)
                        print(f"âœ‚ï¸ åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªè¯­ä¹‰å—")
                    else:
                        # Fallback: use title only
                        if title:
                            texts_to_vectorize.append(title)
                
                if texts_to_vectorize:
                    print(f"ğŸ”§ Vectorizing {len(texts_to_vectorize)} results...")
                    print(f"ğŸ” ä½¿ç”¨æ¨¡å‹: alias='embedding' (åº”æ˜ å°„åˆ°text-embedding-v4)")
                    
                    embedding_response = await call_embedding(texts_to_vectorize)
                    
                    if embedding_response.get('success'):
                        embeddings = embedding_response.get('embeddings', [])
                        print(f"âœ… Vectorization complete, generated {len(embeddings)} vectors")
                        print(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {embedding_response.get('model', 'unknown')} (æä¾›å•†: {embedding_response.get('provider', 'unknown')})")
                        
                        # Real ChromaDB similarity matching
                        try:
                            import chromadb
                            import numpy as np
                            
                            # Connect to ChromaDB (check multiple possible paths)
                            chromadb_paths = [
                                "/github/workspace/traindb/chroma_db",
                                "./traindb/chroma_db",
                                "./chroma_db", 
                                "../chroma_db",
                                os.path.expanduser("~/Downloads/creditmonitor/traindb/chroma_db"),
                                "/github/workspace/traindb"
                            ]
                            
                            chroma_client = None
                            collection = None
                            
                            for path in chromadb_paths:
                                try:
                                    if os.path.exists(path):
                                        print(f"ğŸ” Trying ChromaDB path: {path}")
                                        chroma_client = chromadb.PersistentClient(path=path)
                                        collections = chroma_client.list_collections()
                                        if collections:
                                            collection = collections[0]  # Use first available collection
                                            print(f"âœ… Connected to ChromaDB collection: {collection.name}")
                                            break
                                except Exception as e:
                                    print(f"âš ï¸ Failed to connect to {path}: {e}")
                                    continue
                            
                            if collection:
                                # Query ChromaDB for real similarity scores
                                for i, result in enumerate(enhanced_results):
                                    if i < len(embeddings):
                                        try:
                                            # Query ChromaDB with the embedding
                                            query_result = collection.query(
                                                query_embeddings=[embeddings[i]],
                                                n_results=3,
                                                include=['distances', 'documents', 'metadatas']
                                            )
                                            
                                            if query_result and query_result.get('distances'):
                                                distances = query_result['distances'][0]
                                                if distances:
                                                    # Convert distance to similarity (distance = 1 - cosine_similarity)
                                                    min_distance = min(distances)
                                                    similarity = 1.0 - min_distance
                                                    result['relevance_score'] = max(0.1, min(0.95, similarity))
                                                    result['chromadb_matched'] = True
                                                    result['min_distance'] = min_distance
                                                    result['match_count'] = len(distances)
                                                else:
                                                    result['relevance_score'] = 0.5
                                                    result['chromadb_matched'] = False
                                            else:
                                                result['relevance_score'] = 0.5  
                                                result['chromadb_matched'] = False
                                        except Exception as e:
                                            print(f"âš ï¸ ChromaDB query failed for result {i}: {e}")
                                            result['relevance_score'] = 0.5
                                            result['chromadb_matched'] = False
                                            
                                        result['vector_enhanced'] = True
                                        result['embedding_dimension'] = len(embeddings[i])
                            else:
                                print("âš ï¸ No ChromaDB collection found, using dynamic scoring")
                                # åŠ¨æ€è¯„åˆ†æ›¿ä»£å›ºå®šé€’å‡åˆ†æ•°
                                for i, result in enumerate(enhanced_results):
                                    if i < len(embeddings):
                                        # ä½¿ç”¨å†…å®¹è´¨é‡æŒ‡æ ‡è¿›è¡ŒåŠ¨æ€è¯„åˆ†
                                        content_length = len(result.get('content', ''))
                                        title_length = len(result.get('title', ''))
                                        
                                        # åŸºç¡€åˆ†æ•°ï¼šä½¿ç”¨PerplexityåŸå§‹ç›¸å…³æ€§
                                        base_score = result.get('relevance_score', 0.6)
                                        
                                        # å†…å®¹è´¨é‡å› å­
                                        content_quality = min(0.25, content_length / 4000)  # æœ€é«˜0.25åˆ†
                                        title_quality = min(0.05, title_length / 150)       # æœ€é«˜0.05åˆ†
                                        
                                        # åŸºäºæ ‡é¢˜çš„ç¡®å®šæ€§è°ƒæ•´ï¼ˆé¿å…éšæœºä½†ä¿è¯å·®å¼‚ï¼‰
                                        title_hash = hash(result.get('title', '')) % 100
                                        hash_adjustment = (title_hash - 50) / 1000  # -0.05 åˆ° +0.05
                                        
                                        final_score = min(0.95, base_score + content_quality + title_quality + hash_adjustment)
                                        result['relevance_score'] = round(final_score, 3)
                                        result['vector_enhanced'] = True
                                        result['chromadb_matched'] = False
                                        result['embedding_dimension'] = len(embeddings[i])
                                        result['scoring_method'] = 'dynamic_no_chromadb'
                                        
                        except Exception as e:
                            print(f"âš ï¸ ChromaDB integration failed: {e}")
                            # åŠ¨æ€fallback scoring - åŸºäºå†…å®¹è´¨é‡è€Œéå›ºå®šæ•°å€¼
                            for i, result in enumerate(enhanced_results):
                                if i < len(embeddings):
                                    # åŸºäºå†…å®¹é•¿åº¦å’Œå…³é”®è¯å¯†åº¦çš„åŠ¨æ€è¯„åˆ†
                                    content_length = len(result.get('content', ''))
                                    title_length = len(result.get('title', ''))
                                    
                                    # åŸºç¡€åˆ†æ•° + å†…å®¹è´¨é‡è°ƒæ•´
                                    base_score = result.get('relevance_score', 0.5)  # ä½¿ç”¨PerplexityåŸå§‹åˆ†æ•°
                                    
                                    # å†…å®¹è´¨é‡è°ƒæ•´ (0.05-0.2)
                                    content_bonus = min(0.2, content_length / 5000)  # å†…å®¹è¶Šé•¿è´¨é‡è®¤ä¸ºè¶Šé«˜
                                    title_bonus = min(0.05, title_length / 200)      # æ ‡é¢˜é€‚ä¸­é•¿åº¦åŠ åˆ†
                                    
                                    # æœ€ç»ˆåˆ†æ•° = åŸºç¡€åˆ†æ•° + è´¨é‡è°ƒæ•´ + å°‘é‡éšæœºæ€§ï¼ˆé˜²æ­¢å®Œå…¨ç›¸åŒï¼‰
                                    import random
                                    random.seed(hash(result.get('title', '')) % 2147483647)  # åŸºäºæ ‡é¢˜çš„ç¡®å®šæ€§éšæœº
                                    randomness = random.uniform(-0.02, 0.02)
                                    
                                    final_score = min(0.95, base_score + content_bonus + title_bonus + randomness)
                                    result['relevance_score'] = round(final_score, 3)
                                    result['vector_enhanced'] = True 
                                    result['chromadb_matched'] = False
                                    result['embedding_dimension'] = len(embeddings[i])
                                    result['scoring_method'] = 'dynamic_fallback'
                    else:
                        print(f"âš ï¸ Vectorization failed: {embedding_response.get('error', 'Unknown error')}")
                        
            except Exception as e:
                print(f"âš ï¸ Vector enhancement failed: {e}")
        elif enable_vectorization and not qwen_key:
            print("âš ï¸ Skipping vector enhancement - QWEN_API_KEY required for embedding model")
        else:
            print("âš ï¸ Skipping vector enhancement")
        
        # Step 3: Generate Summary
        print("\nğŸ“„ Step 3: Generating Summary...")
        
        # Sort by relevance score
        sorted_results = sorted(enhanced_results, 
                              key=lambda x: x.get('relevance_score', 0), 
                              reverse=True)
        
        # æ”¶é›†æ‰€æœ‰PerplexityåŸå§‹æœç´¢ç»“æœç”¨äºé‚®ä»¶
        perplexity_raw_results = []
        for result in search_results:
            if result.get('source') == 'Perplexity AI':
                perplexity_raw_results.append(result)
        
        # ç¡®ä¿è‡³å°‘æœ‰5ä¸ªPerplexityç»“æœæ˜¾ç¤ºåœ¨é‚®ä»¶ä¸­
        perplexity_results_to_show = perplexity_raw_results[:5] if len(perplexity_raw_results) >= 5 else perplexity_raw_results
        
        print(f"ğŸ“Š é‚®ä»¶å†…å®¹ä¿è¯:")
        print(f"  - PerplexityåŸå§‹ç»“æœ: {len(perplexity_results_to_show)}/5 ä¸ª")
        print(f"  - ç»¼åˆæ’åºç»“æœ: {len(sorted_results)} ä¸ª")
        print(f"  - åŠ¨æ€è¯„åˆ†èŒƒå›´: {min([r.get('relevance_score', 0) for r in sorted_results]):.3f} - {max([r.get('relevance_score', 0) for r in sorted_results]):.3f}")
        
        # Generate summary
        summary_lines = [
            f"# ğŸ¯ Unified Research Analysis Report",
            f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"",
            f"## ğŸ“Š Analysis Overview",
            f"- **Topics Analyzed**: {', '.join(topics)}",
            f"- **Total Results**: {len(enhanced_results)}",
            f"- **Perplexity Search Results**: {len(perplexity_raw_results)}",
            f"- **Vector Enhancement**: {'âœ… Enabled' if enable_vectorization else 'âŒ Disabled'}",
            f"- **Search Time**: {search_time:.2f}s",
            f"",
            f"## ğŸ” Perplexity APIæœç´¢ç»“æœ (æœ€æ–°5ä¸ª)",
            f""
        ]
        
        # é¦–å…ˆæ˜¾ç¤ºPerplexityåŸå§‹æœç´¢ç»“æœ
        for i, result in enumerate(perplexity_results_to_show, 1):
            citation_info = f" | **å¼•ç”¨**: [{result.get('citation_index', 'N/A')}]" if result.get('citation_index') else ""
            
            # æŸ¥æ‰¾å¯¹åº”çš„å‘é‡è¯„åˆ†
            vector_score = None
            for enhanced_result in enhanced_results:
                # é€šè¿‡æ ‡é¢˜æˆ–å†…å®¹åŒ¹é…æ‰¾åˆ°å¯¹åº”çš„å‘é‡å¢å¼ºç»“æœ
                if (enhanced_result.get('title') == result.get('title') or 
                    enhanced_result.get('url') == result.get('url')):
                    if enhanced_result.get('vector_enhanced') or enhanced_result.get('relevance_score'):
                        vector_score = enhanced_result.get('relevance_score')
                    break
            
            # æ ¼å¼åŒ–scoreæ˜¾ç¤º
            score_display = f"{vector_score:.3f}" if vector_score is not None else "null"
            
            summary_lines.extend([
                f"### {i}. {result.get('title', 'Untitled')}",
                f"**æ¥æº**: Perplexity API | **æ¨¡å‹**: {result.get('model_used', 'sonar')}{citation_info}",
                f"**è¯é¢˜**: {result.get('topic', 'Unknown')}",
                f"**çœŸå®é“¾æ¥**: {result.get('url', 'N/A')}",
                f"**Score**: {score_display} | **å‘é‡å¢å¼º**: {'âœ…' if vector_score is not None else 'âŒ'}",
                f"**å†…å®¹**: {result.get('content', 'No content')[:300]}...",
                f"**æ—¶é—´èŒƒå›´**: {result.get('date_range', 'Unknown')}",
                f""
            ])
        
        # ç„¶åæ˜¾ç¤ºç»è¿‡å¤„ç†çš„ç»¼åˆç»“æœ
        summary_lines.extend([
            f"## ğŸ† ç»¼åˆæ’åºç»“æœ (å‘é‡å¢å¼º)",
            f""
        ])
        
        for i, result in enumerate(sorted_results[:10], 1):
            relevance = result.get('relevance_score', 0.8)
            chromadb_status = "âœ… ChromaDB" if result.get('chromadb_matched') else "âš ï¸ Fallback"
            vector_status = "ğŸ§® Vector" if result.get('vector_enhanced') else "ğŸ“ Basic"
            
            summary_lines.extend([
                f"### {i}. {result.get('title', 'Untitled')}",
                f"**Score**: {relevance:.3f} | **Status**: {chromadb_status} {vector_status}",
                f"**Topic**: {result.get('topic', 'Unknown')}",
                f"**URL**: {result.get('url', 'N/A')}",
                f"**Content**: {result.get('content', 'No content')[:200]}...",
                f""
            ])
        
        summary_content = '\n'.join(summary_lines)
        
        # Step 4: Save Report
        print("\nğŸ’¾ Step 4: Saving Report...")
        
        # Save detailed JSON report
        report_data = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "topics": topics,
                "total_results": len(enhanced_results),
                "vector_enhancement": enable_vectorization,
                "search_time_seconds": search_time
            },
            "results": sorted_results
        }
        
        os.makedirs('reports', exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Save JSON report
        json_filepath = f'reports/unified_research_{timestamp}.json'
        with open(json_filepath, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        # Save markdown summary
        md_filepath = f'reports/unified_research_{timestamp}.md'
        with open(md_filepath, 'w', encoding='utf-8') as f:
            f.write(summary_content)
        
        print(f"ğŸ“ Reports saved:")
        print(f"  JSON: {json_filepath}")
        print(f"  Markdown: {md_filepath}")
        
        # Step 5: Send Email Notification
        print("\nğŸ“§ Step 5: Sending Email Notification...")
        
        def send_email():
            try:
                # SMTP config check with debugging
                smtp_server = os.getenv('SMTP_SERVER')
                smtp_port = int(os.getenv('SMTP_PORT', '587'))
                smtp_user = os.getenv('SMTP_USER')
                smtp_password = os.getenv('SMTP_PASSWORD')
                sender_name = os.getenv('SENDER_NAME', 'Credit Research Bot')
                recipient_email = os.getenv('RECIPIENT_EMAIL')
                
                print(f"ğŸ” SMTP Config Debug:")
                print(f"  SMTP_SERVER: {'âœ… Set' if smtp_server else 'âŒ Missing'}")
                print(f"  SMTP_PORT: {smtp_port}")
                print(f"  SMTP_USER: {'âœ… Set' if smtp_user else 'âŒ Missing'}")
                print(f"  SMTP_PASSWORD: {'âœ… Set' if smtp_password else 'âŒ Missing'}")
                
                if not all([smtp_server, smtp_user, smtp_password]):
                    print("âš ï¸ Incomplete SMTP config, skipping email")
                    print("ğŸ“‹ Required GitHub Secrets:")
                    print("  - SMTP_SERVER (e.g., smtp.gmail.com)")
                    print("  - SMTP_USER (your email address)")  
                    print("  - SMTP_PASSWORD (app-specific password)")
                    print("  - SMTP_PORT (usually 587)")
                    return True
                
                # Create email
                msg = MIMEMultipart()
                msg['From'] = formataddr((sender_name, smtp_user))
                subject = f"ğŸ¯ Unified Research Report - {'/'.join(topics)} ({datetime.now().strftime('%Y-%m-%d')})"
                msg['Subject'] = subject
                
                if recipient_email:
                    msg['To'] = recipient_email
                else:
                    msg['To'] = smtp_user
                
                # Email body
                body = f"""
{summary_content}

---
ğŸ¤– Automated by Unified Research Analysis
ğŸ“ Full reports available in the GitHub Actions artifacts
                """
                
                msg.attach(MIMEText(body, 'plain', 'utf-8'))
                
                # Attach JSON report
                try:
                    with open(json_filepath, 'rb') as attachment:
                        part = MIMEBase('application', 'octet-stream')
                        part.set_payload(attachment.read())
                        encoders.encode_base64(part)
                        part.add_header(
                            'Content-Disposition',
                            f'attachment; filename= {os.path.basename(json_filepath)}'
                        )
                        msg.attach(part)
                except Exception as e:
                    print(f"âš ï¸ Failed to attach JSON report: {e}")
                
                # Send email with improved connection handling
                print(f"ğŸ“¤ Sending email to {msg['To']}...")
                
                # Try different SMTP approaches based on server
                smtp_methods = []
                
                if smtp_port == 465:
                    # SSL connection for port 465
                    smtp_methods.append(('SMTP_SSL', lambda: smtplib.SMTP_SSL(smtp_server, smtp_port, timeout=60)))
                elif smtp_port == 587:
                    # STARTTLS for port 587
                    smtp_methods.append(('SMTP+STARTTLS', lambda: smtplib.SMTP(smtp_server, smtp_port, timeout=60)))
                else:
                    # Try both methods for unknown ports
                    smtp_methods.extend([
                        ('SMTP+STARTTLS', lambda: smtplib.SMTP(smtp_server, smtp_port, timeout=60)),
                        ('SMTP_SSL', lambda: smtplib.SMTP_SSL(smtp_server, smtp_port, timeout=60))
                    ])
                
                last_error = None
                for method_name, server_factory in smtp_methods:
                    try:
                        print(f"ğŸ”— Trying {method_name} connection to {smtp_server}:{smtp_port}")
                        
                        with server_factory() as server:
                            server.set_debuglevel(1)  # Enable detailed SMTP debug output
                            
                            # Only call starttls for non-SSL connections
                            if method_name == 'SMTP+STARTTLS':
                                print("ğŸ” Starting TLS encryption")
                                server.starttls()
                            
                            print("ğŸ”‘ Attempting login")
                            server.login(smtp_user, smtp_password)
                            print("âœ… SMTP login successful")
                            
                            print("ğŸ“¨ Sending message")
                            server.send_message(msg)
                            print("ğŸ“¨ Message sent successfully")
                            break  # Success, exit loop
                            
                    except smtplib.SMTPAuthenticationError as e:
                        last_error = f"SMTP Authentication failed: {e}"
                        print(f"âŒ {method_name}: {last_error}")
                        break  # Auth error, don't retry other methods
                    except (smtplib.SMTPConnectError, smtplib.SMTPServerDisconnected, ConnectionRefusedError, OSError) as e:
                        last_error = f"SMTP Connection failed ({method_name}): {e}"
                        print(f"âš ï¸ {method_name}: {last_error}")
                        continue  # Try next method
                    except Exception as e:
                        last_error = f"SMTP Error ({method_name}): {e}"
                        print(f"âŒ {method_name}: {last_error}")
                        continue  # Try next method
                else:
                    # All methods failed
                    raise Exception(last_error or "All SMTP connection methods failed")
                
                print("âœ… Email sent successfully!")
                return True
                
            except Exception as e:
                print(f"âŒ Email sending failed: {e}")
                return False
        
        email_sent = send_email()
        
        # Step 6: Summary
        total_time = time.time() - search_start_time
        print(f"\nğŸ‰ Analysis Complete!")
        print(f"â±ï¸ Total Time: {total_time:.2f}s")
        print(f"ğŸ“Š Results: {len(enhanced_results)} processed")
        print(f"ğŸ’¾ Reports: Saved to {json_filepath}")
        print(f"ğŸ“§ Email: {'âœ… Sent' if email_sent else 'âŒ Failed'}")
        
        # å†™å…¥workflowæ‘˜è¦ä¸­çš„è´¹ç”¨ç›‘æ§ä¿¡æ¯
        github_summary = os.environ.get('GITHUB_STEP_SUMMARY', '/dev/null')
        if github_summary != '/dev/null':
            with open(github_summary, 'a') as f:
                cost_requests = current_requests * 0.005  # $5/1000 requests 
                cost_tokens = (current_tokens / 1000000) * 1.0  # sonar: $1/1M tokens
                total_cost = cost_requests + cost_tokens
                
                f.write(f"\n## ğŸ’° Perplexity Cost Analysis\n")
                f.write(f"- **Model Used**: sonar (æˆæœ¬æ•ˆç›Šæ¨¡å‹)\n")
                f.write(f"- **Usage**: {current_requests}/{MAX_REQUESTS_PER_RUN} requests, {current_tokens}/{MAX_TOKENS_PER_RUN} tokens\n")
                f.write(f"- **Cost Breakdown**: ${cost_requests:.3f} (requests) + ${cost_tokens:.3f} (tokens) = ${total_cost:.3f}\n")
                f.write(f"- **Hard Limits**: {'ğŸš¨ BREACHED' if current_requests >= MAX_REQUESTS_PER_RUN or current_tokens >= MAX_TOKENS_PER_RUN else 'âœ… Within limits'}\n")
                f.write(f"- **Results Processed**: {len(enhanced_results)}\n")
                f.write(f"- **Vector Enhancement**: {'âœ… Enabled' if enable_vectorization else 'âŒ Disabled'}\n")
                f.write(f"- **Email Status**: {'âœ… Sent' if email_sent else 'âŒ Failed'}\n")
        
        return True
        
    except Exception as e:
        print(f"âŒ Analysis failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)