#!/usr/bin/env python3
"""
å‘é‡æ¨¡å‹ä¸€è‡´æ€§åˆ†æå’Œè§£å†³æ–¹æ¡ˆ
åˆ†æä¸åŒAPIå‘é‡åŒ–æ¨¡å‹çš„å…¼å®¹æ€§é—®é¢˜
"""

import numpy as np
import json
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import hashlib

@dataclass
class VectorModelMetadata:
    """å‘é‡æ¨¡å‹å…ƒæ•°æ®"""
    model_name: str
    api_provider: str
    model_version: str
    dimension: int
    tokenizer: str
    max_tokens: int
    embedding_method: str
    created_at: datetime
    model_hash: str  # æ¨¡å‹æŒ‡çº¹

class VectorConsistencyAnalyzer:
    """å‘é‡ä¸€è‡´æ€§åˆ†æå™¨"""
    
    def __init__(self):
        self.supported_models = {
            "qwen": {
                "api_provider": "dashscope",
                "dimension": 1536,  # åƒé—®åµŒå…¥ç»´åº¦
                "tokenizer": "qwen_tokenizer",
                "max_tokens": 2048,
                "stability": "high"  # æ¨¡å‹ç¨³å®šæ€§
            },
            "deepseek": {
                "api_provider": "deepseek",
                "dimension": 1024,  # DeepSeekåµŒå…¥ç»´åº¦
                "tokenizer": "deepseek_tokenizer", 
                "max_tokens": 4096,
                "stability": "medium"
            },
            "text-embedding-ada-002": {
                "api_provider": "openai",
                "dimension": 1536,
                "tokenizer": "tiktoken",
                "max_tokens": 8191,
                "stability": "high"
            }
        }
    
    def analyze_compatibility(self, model1: str, model2: str) -> Dict:
        """åˆ†æä¸¤ä¸ªæ¨¡å‹çš„å…¼å®¹æ€§"""
        
        if model1 not in self.supported_models or model2 not in self.supported_models:
            return {"compatible": False, "reason": "ä¸æ”¯æŒçš„æ¨¡å‹"}
        
        m1_info = self.supported_models[model1]
        m2_info = self.supported_models[model2]
        
        # ç»´åº¦å¿…é¡»ç›¸åŒ
        dimension_match = m1_info["dimension"] == m2_info["dimension"]
        
        # åŒä¸€æä¾›å•†æ›´å…¼å®¹
        provider_match = m1_info["api_provider"] == m2_info["api_provider"]
        
        # åˆ†è¯å™¨å½±å“æ–‡æœ¬å¤„ç†
        tokenizer_match = m1_info["tokenizer"] == m2_info["tokenizer"]
        
        compatibility_score = 0
        if dimension_match: compatibility_score += 40
        if provider_match: compatibility_score += 30  
        if tokenizer_match: compatibility_score += 30
        
        return {
            "compatible": compatibility_score >= 70,
            "compatibility_score": compatibility_score,
            "dimension_match": dimension_match,
            "provider_match": provider_match,
            "tokenizer_match": tokenizer_match,
            "recommendation": self._get_recommendation(compatibility_score)
        }
    
    def _get_recommendation(self, score: int) -> str:
        """è·å–å…¼å®¹æ€§å»ºè®®"""
        if score >= 90:
            return "å®Œå…¨å…¼å®¹ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨"
        elif score >= 70:
            return "åŸºæœ¬å…¼å®¹ï¼Œå»ºè®®æµ‹è¯•åä½¿ç”¨"
        elif score >= 40:
            return "éƒ¨åˆ†å…¼å®¹ï¼Œéœ€è¦é‡æ–°å‘é‡åŒ–"
        else:
            return "ä¸å…¼å®¹ï¼Œå¿…é¡»é‡æ–°æ„å»ºå‘é‡æ•°æ®åº“"

class VectorDatabaseVersionManager:
    """å‘é‡æ•°æ®åº“ç‰ˆæœ¬ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.version_file = f"{db_path}/version_metadata.json"
        
    def create_version_metadata(self, model_metadata: VectorModelMetadata) -> str:
        """åˆ›å»ºç‰ˆæœ¬å…ƒæ•°æ®"""
        version_id = f"{model_metadata.model_name}_v{model_metadata.model_version}_{datetime.now().strftime('%Y%m%d')}"
        
        metadata = {
            "version_id": version_id,
            "model_name": model_metadata.model_name,
            "api_provider": model_metadata.api_provider,
            "model_version": model_metadata.model_version,
            "dimension": model_metadata.dimension,
            "tokenizer": model_metadata.tokenizer,
            "max_tokens": model_metadata.max_tokens,
            "embedding_method": model_metadata.embedding_method,
            "created_at": model_metadata.created_at.isoformat(),
            "model_hash": model_metadata.model_hash,
            "data_stats": {
                "total_documents": 0,
                "total_chunks": 0,
                "avg_chunk_length": 0
            }
        }
        
        with open(self.version_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
            
        return version_id
    
    def check_model_compatibility(self, current_model: str) -> Dict:
        """æ£€æŸ¥å½“å‰æ¨¡å‹ä¸æ•°æ®åº“çš„å…¼å®¹æ€§"""
        try:
            with open(self.version_file, 'r', encoding='utf-8') as f:
                db_metadata = json.load(f)
                
            analyzer = VectorConsistencyAnalyzer()
            return analyzer.analyze_compatibility(
                db_metadata["model_name"], 
                current_model
            )
        except FileNotFoundError:
            return {"compatible": False, "reason": "æ•°æ®åº“å…ƒæ•°æ®ä¸å­˜åœ¨"}
    
    def plan_migration(self, new_model: str) -> Dict:
        """è§„åˆ’æ•°æ®åº“è¿ç§»ç­–ç•¥"""
        compatibility = self.check_model_compatibility(new_model)
        
        if compatibility["compatible"]:
            return {
                "migration_needed": False,
                "strategy": "direct_use",
                "estimated_time": "å³æ—¶",
                "cost": "æ— "
            }
        else:
            return {
                "migration_needed": True,
                "strategy": "full_reprocessing",
                "estimated_time": "æ•°å°æ—¶",
                "cost": "APIè°ƒç”¨è´¹ç”¨",
                "steps": [
                    "å¤‡ä»½å½“å‰æ•°æ®åº“",
                    "æå–åŸå§‹æ–‡æ¡£",
                    "ä½¿ç”¨æ–°æ¨¡å‹é‡æ–°å‘é‡åŒ–",
                    "æ„å»ºæ–°æ•°æ®åº“",
                    "éªŒè¯è¿ç§»ç»“æœ"
                ]
            }

class TextSummarizationAnalyzer:
    """æ–‡æœ¬æ¦‚æ‹¬åˆ†æå™¨"""
    
    def analyze_information_loss(self, original_length: int, summary_length: int, 
                                domain: str = "credit_research") -> Dict:
        """åˆ†ææ¦‚æ‹¬è¿‡ç¨‹ä¸­çš„ä¿¡æ¯æŸå¤±"""
        
        compression_ratio = summary_length / original_length
        
        # åŸºäºé¢†åŸŸçš„ä¿¡æ¯å¯†åº¦åˆ†æ
        domain_factors = {
            "credit_research": {
                "key_info_density": 0.3,  # å…³é”®ä¿¡æ¯å¯†åº¦
                "technical_terms_importance": 0.8,  # æŠ€æœ¯æœ¯è¯­é‡è¦æ€§
                "data_preservation_need": 0.9  # æ•°æ®ä¿çœŸéœ€æ±‚
            }
        }
        
        domain_factor = domain_factors.get(domain, domain_factors["credit_research"])
        
        # è®¡ç®—ä¿¡æ¯ä¿çœŸåº¦
        if compression_ratio >= 0.8:
            fidelity = 0.95
        elif compression_ratio >= 0.5:
            fidelity = 0.85
        elif compression_ratio >= 0.3:
            fidelity = 0.70
        elif compression_ratio >= 0.1:
            fidelity = 0.50
        else:
            fidelity = 0.30
            
        # è€ƒè™‘é¢†åŸŸç‰¹å®šå› ç´ 
        adjusted_fidelity = fidelity * (1 - domain_factor["technical_terms_importance"] * 0.1)
        
        return {
            "compression_ratio": compression_ratio,
            "estimated_fidelity": adjusted_fidelity,
            "information_loss": 1 - adjusted_fidelity,
            "recommendation": self._get_summarization_recommendation(adjusted_fidelity),
            "optimal_length_range": self._calculate_optimal_length(original_length)
        }
    
    def _get_summarization_recommendation(self, fidelity: float) -> str:
        """è·å–æ¦‚æ‹¬å»ºè®®"""
        if fidelity >= 0.9:
            return "é€‚åˆæ¦‚æ‹¬ï¼Œä¿¡æ¯æŸå¤±è¾ƒå°"
        elif fidelity >= 0.7:
            return "å¯ä»¥æ¦‚æ‹¬ï¼Œä½†éœ€è¦ä¿ç•™å…³é”®æœ¯è¯­"
        elif fidelity >= 0.5:
            return "è°¨æ…æ¦‚æ‹¬ï¼Œå»ºè®®åˆ†æ®µå¤„ç†"
        else:
            return "ä¸å»ºè®®æ¦‚æ‹¬ï¼Œä½¿ç”¨åŸæ–‡æˆ–é•¿æ‘˜è¦"
    
    def _calculate_optimal_length(self, original_length: int) -> Tuple[int, int]:
        """è®¡ç®—æœ€ä¼˜æ¦‚æ‹¬é•¿åº¦èŒƒå›´"""
        min_length = max(100, int(original_length * 0.3))
        max_length = max(300, int(original_length * 0.7))
        return (min_length, max_length)

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºåˆ†æè¿‡ç¨‹"""
    print("ğŸ” å‘é‡æ¨¡å‹ä¸€è‡´æ€§å’Œæ–‡æœ¬æ¦‚æ‹¬åˆ†æ")
    print("=" * 50)
    
    # 1. æ¨¡å‹å…¼å®¹æ€§åˆ†æ
    analyzer = VectorConsistencyAnalyzer()
    compatibility = analyzer.analyze_compatibility("qwen", "deepseek")
    
    print("ğŸ“Š æ¨¡å‹å…¼å®¹æ€§åˆ†æ (åƒé—® vs DeepSeek):")
    print(f"   å…¼å®¹æ€§è¯„åˆ†: {compatibility['compatibility_score']}/100")
    print(f"   æ˜¯å¦å…¼å®¹: {compatibility['compatible']}")
    print(f"   å»ºè®®: {compatibility['recommendation']}")
    print()
    
    # 2. æ–‡æœ¬æ¦‚æ‹¬åˆ†æ
    summarizer = TextSummarizationAnalyzer()
    analysis = summarizer.analyze_information_loss(
        original_length=2000,  # åŸæ–‡2000å­—
        summary_length=400,    # æ¦‚æ‹¬400å­—
        domain="credit_research"
    )
    
    print("ğŸ“ æ–‡æœ¬æ¦‚æ‹¬ä¿¡æ¯ä¿çœŸåº¦åˆ†æ:")
    print(f"   å‹ç¼©æ¯”: {analysis['compression_ratio']:.2f}")
    print(f"   ä¿¡æ¯ä¿çœŸåº¦: {analysis['estimated_fidelity']:.2f}")
    print(f"   ä¿¡æ¯æŸå¤±: {analysis['information_loss']:.2f}")
    print(f"   å»ºè®®: {analysis['recommendation']}")
    print(f"   æœ€ä¼˜é•¿åº¦èŒƒå›´: {analysis['optimal_length_range']}")
    print()
    
    # 3. ç‰ˆæœ¬ç®¡ç†ç¤ºä¾‹
    version_manager = VectorDatabaseVersionManager("./chroma_db")
    
    # æ¨¡æ‹Ÿæ£€æŸ¥å…¼å®¹æ€§
    print("ğŸ”„ æ•°æ®åº“ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥:")
    print("   å½“å‰æ•°æ®åº“: åƒé—®æ¨¡å‹")
    print("   æ–°æ¨¡å‹: DeepSeek")
    print("   å…¼å®¹æ€§: éœ€è¦é‡æ–°å‘é‡åŒ–")
    print("   è¿ç§»ç­–ç•¥: å…¨é‡é‡å¤„ç†")

if __name__ == "__main__":
    main()
 
 
 