name: ğŸ§ª Test Text Segmentation & Embedding

on:
  workflow_dispatch:
    inputs:
      test_text:
        description: 'Text to test segmentation and embedding'
        required: false
        default: 'Credit risk management involves identifying, assessing, and mitigating potential financial losses from borrower defaults. Financial institutions implement comprehensive frameworks including credit scoring, portfolio diversification, and regulatory compliance. Modern approaches integrate machine learning algorithms with traditional risk assessment methodologies to enhance predictive accuracy and operational efficiency.'
        type: string
      
      max_chunk_size:
        description: 'Maximum chunk size for segmentation'
        required: false
        default: '300'
        type: string

env:
  QWEN_API_KEY: ${{ secrets.QWEN_API_KEY }}
  PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}

jobs:
  test-segmentation-embedding:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - name: ğŸ“¥ Checkout Code
      uses: actions/checkout@v4
    
    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: ğŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests urllib3 aiofiles asyncio python-dotenv httpx chromadb numpy openai
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt
        fi
    
    - name: ğŸ”§ Execute Segmentation & Embedding Test
      id: segmentation_test
      run: |
        python3 << 'EOF'
        import asyncio
        import sys
        import os
        from typing import List, Dict, Any

        # Add oop directory to path
        possible_paths = [
            '/github/workspace/oop',
            './oop',
            '../oop',
            os.path.join(os.getcwd(), 'oop')
        ]
        
        model_manager_imported = False
        for path in possible_paths:
            try:
                if os.path.exists(path):
                    print(f"ğŸ” Trying path: {path}")
                    sys.path.insert(0, path)
                    from model_manager import call_llm, call_embedding, get_model_status
                    print("âœ… Model manager imported successfully")
                    model_manager_imported = True
                    break
            except ImportError as e:
                print(f"âš ï¸ Failed to import from {path}: {e}")
                continue
        
        if not model_manager_imported:
            print("âŒ Failed to import model manager")
            sys.exit(1)

        async def intelligent_segmentation_llm(text: str, max_chunk_size: int = 300) -> List[str]:
            """æ™ºèƒ½æ–‡æœ¬åˆ‡åˆ† - ä½¿ç”¨LLMæ¨¡å‹"""
            try:
                prompt = f"""Please intelligently segment the following text into semantically complete paragraphs, with each segment not exceeding {max_chunk_size} characters:

        {text[:2000]}

        Requirements:
        1. Maintain semantic integrity and coherence
        2. Keep each segment under {max_chunk_size} characters  
        3. Preserve credit research technical terms and key information
        4. Segment by logical topics (e.g., policy analysis, technology development, market data)
        5. Remove reference numbers at the end of sentences or paragraphs
        6. Clean up text transitions
        7. Return format: one segment per line, separated by "---"

        Ensure output is in English.
        """
                
                print(f"ğŸ”§ è°ƒç”¨LLMæ¨¡å‹è¿›è¡Œæ™ºèƒ½æ–‡æœ¬åˆ‡åˆ†...")
                result = await call_llm(prompt, model_alias="llm", max_tokens=2000, temperature=0.1)
                
                if result.get('success'):
                    response_text = result.get('content', '')
                    print(f"ğŸ“Š LLM APIå“åº”: æ¨¡å‹={result.get('model', 'unknown')}, æä¾›å•†={result.get('provider', 'unknown')}")
                    
                    chunks = [chunk.strip() for chunk in response_text.split("---") if chunk.strip()]
                    
                    if chunks and len(chunks) > 0:
                        print(f"âœ… LLMæ™ºèƒ½åˆ‡åˆ†å®Œæˆ: {len(text)} å­—ç¬¦ â†’ {len(chunks)} ä¸ªè¯­ä¹‰å—")
                        for i, chunk in enumerate(chunks):
                            print(f"  ğŸ“„ å—{i+1} ({len(chunk)}å­—ç¬¦): {chunk[:100]}...")
                        return chunks
                    else:
                        print("âš ï¸ LLMåˆ‡åˆ†è¿”å›ç©ºç»“æœ")
                else:
                    print(f"âŒ LLMåˆ‡åˆ†è°ƒç”¨å¤±è´¥: {result.get('error', 'Unknown error')}")
                    
            except Exception as e:
                print(f"âš ï¸ LLMåˆ‡åˆ†å¼‚å¸¸: {e}")
            
            return []

        async def test_embedding(texts: List[str]) -> Dict[str, Any]:
            """æµ‹è¯•å‘é‡åŒ–"""
            try:
                print(f"ğŸ”§ è°ƒç”¨embeddingæ¨¡å‹è¿›è¡Œå‘é‡åŒ–...")
                print(f"ğŸ” ä½¿ç”¨æ¨¡å‹: alias='embedding' (åº”æ˜ å°„åˆ°text-embedding-v4)")
                
                result = await call_embedding(texts, model_alias="embedding")
                
                if result.get('success'):
                    embeddings = result.get('embeddings', [])
                    print(f"âœ… å‘é‡åŒ–æˆåŠŸ: {len(texts)} ä¸ªæ–‡æœ¬ â†’ {len(embeddings)} ä¸ªå‘é‡")
                    print(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {result.get('model', 'unknown')} (æä¾›å•†: {result.get('provider', 'unknown')})")
                    
                    if embeddings:
                        print(f"ğŸ“ å‘é‡ç»´åº¦: {len(embeddings[0]) if embeddings[0] else 0}")
                    
                    return result
                else:
                    print(f"âŒ å‘é‡åŒ–å¤±è´¥: {result.get('error', 'Unknown error')}")
                    return {"success": False, "error": result.get('error', 'Unknown error')}
                    
            except Exception as e:
                print(f"âš ï¸ å‘é‡åŒ–å¼‚å¸¸: {e}")
                return {"success": False, "error": str(e)}

        async def main():
            print("ğŸ§ª Text Segmentation & Embedding Test")
            print("=" * 50)
            
            # Check model status
            print("\nğŸ¤– Model Status Check:")
            try:
                status = get_model_status()
                for alias, info in status.items():
                    available = "âœ…" if info["available"] else "âŒ"
                    print(f"  {available} {alias}: {info['provider']}-{info['model_id']} ({info['type']})")
            except Exception as e:
                print(f"âš ï¸ Model status check failed: {e}")
            
            # Test parameters
            test_text = """${{ github.event.inputs.test_text }}"""
            max_chunk_size = int("${{ github.event.inputs.max_chunk_size || '300' }}")
            
            print(f"\nğŸ“ Test Text ({len(test_text)} characters):")
            print(f"  {test_text[:200]}...")
            print(f"\nâš™ï¸ Max Chunk Size: {max_chunk_size}")
            
            # Step 1: Text Segmentation
            print(f"\nğŸ”¸ Step 1: Intelligent Text Segmentation")
            chunks = await intelligent_segmentation_llm(test_text, max_chunk_size)
            
            if not chunks:
                print("âŒ Segmentation failed, test cannot continue")
                return False
            
            # Step 2: Embedding
            print(f"\nğŸ”¸ Step 2: Vector Embedding")
            embedding_result = await test_embedding(chunks)
            
            if embedding_result.get('success'):
                print("\nâœ… Complete Test Summary:")
                print(f"  ğŸ“ Original Text: {len(test_text)} characters")
                print(f"  âœ‚ï¸ Segmentation: {len(chunks)} chunks")
                print(f"  ğŸ§  Embedding: {len(embedding_result.get('embeddings', []))} vectors")
                print(f"  ğŸ“Š Model Used: {embedding_result.get('model', 'unknown')}")
                return True
            else:
                print(f"\nâŒ Test Failed at embedding step: {embedding_result.get('error', 'Unknown error')}")
                return False

        # Run the test
        try:
            success = asyncio.run(main())
            if not success:
                sys.exit(1)
        except Exception as e:
            print(f"âŒ Test execution failed: {e}")
            sys.exit(1)
        EOF
    
    - name: ğŸ“‹ Generate Test Summary
      if: always()
      run: |
        # Calculate text length using a safer method
        TEST_TEXT="${{ github.event.inputs.test_text }}"
        TEXT_LENGTH=${#TEST_TEXT}
        
        echo "## ğŸ§ª Text Segmentation & Embedding Test Results" >> $GITHUB_STEP_SUMMARY
        echo "**Test Time**: $(date)" >> $GITHUB_STEP_SUMMARY
        echo "**Input Text Length**: $TEXT_LENGTH characters" >> $GITHUB_STEP_SUMMARY
        echo "**Max Chunk Size**: ${{ github.event.inputs.max_chunk_size || '300' }}" >> $GITHUB_STEP_SUMMARY
        
        # Check the exit code of the previous step
        if [ "${{ steps.segmentation_test.outcome }}" = "success" ]; then
          echo "**Test Status**: âœ… Success" >> $GITHUB_STEP_SUMMARY
          echo "Both LLM segmentation and embedding calls worked correctly with OOP model manager." >> $GITHUB_STEP_SUMMARY
        else
          echo "**Test Status**: âŒ Failed" >> $GITHUB_STEP_SUMMARY
          echo "Check logs for detailed error information." >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "This test verifies:" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… LLM model call for intelligent text segmentation" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… Embedding model call for vectorization" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… OOP model manager integration" >> $GITHUB_STEP_SUMMARY