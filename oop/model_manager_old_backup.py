#!/usr/bin/env python3
"""
ğŸ¯ ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨ - ä¿®å¤ç‰ˆæœ¬
è§£å†³å¼‚æ­¥HTTPè°ƒç”¨å’Œé‡å¤ä»£ç é—®é¢˜
"""

import os
import json
import asyncio
import urllib.request
import urllib.error
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
from enum import Enum
from datetime import datetime

# å°è¯•å¯¼å…¥å¼‚æ­¥HTTPå®¢æˆ·ç«¯
try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False


class ModelType(Enum):
    """ç»Ÿä¸€æ¨¡å‹ç±»å‹æŠ½è±¡"""
    LLM = "llm"                    # å¤§è¯­è¨€æ¨¡å‹
    EMBEDDING = "embedding"        # å‘é‡åŒ–æ¨¡å‹
    SEARCH = "search"             # æœç´¢æ¨¡å‹


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    alias: str                    # æŠ½è±¡åˆ«å
    provider: str                 # æä¾›å•†
    model_id: str                # æ¨¡å‹ID
    model_type: ModelType        # æ¨¡å‹ç±»å‹
    api_key: str                 # APIå¯†é’¥
    base_url: str                # APIåŸºç¡€URL
    headers: Dict[str, str] = None  # é¢å¤–è¯·æ±‚å¤´
    
    def __post_init__(self):
        if self.headers is None:
            self.headers = {}


class UnifiedModelManager:
    """ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.models: Dict[str, ModelConfig] = {}
        self.usage_stats: Dict[str, Dict] = {}
        self.token_usage: Dict[str, Dict] = {}
        
        # è‡ªåŠ¨æ³¨å†Œå¯ç”¨æ¨¡å‹
        self._register_default_models()
        
    def _register_default_models(self):
        """æ³¨å†Œé»˜è®¤æ¨¡å‹é…ç½®"""
        
        # ğŸ¤– å¤§è¯­è¨€æ¨¡å‹ (LLM) - ä½¿ç”¨å®˜æ–¹OpenAIå…¼å®¹æ¥å£
        qwen_api_key = os.getenv('DASHSCOPE_API_KEY') or os.getenv('QWEN_API_KEY')  # å…¼å®¹ä¸¤ç§é…ç½®
        if qwen_api_key:
            self.register_model(ModelConfig(
                alias="llm",
                provider="qwen", 
                model_id="qwen-plus",  # ä½¿ç”¨å®˜æ–¹æ¨èçš„qwen-plusæ¨¡å‹
                model_type=ModelType.LLM,
                api_key=qwen_api_key,
                base_url="https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"  # åƒé—®åŸç”ŸAPI
            ))
            
        # ğŸ§  å‘é‡åŒ–æ¨¡å‹ (EMBEDDING) - ä½¿ç”¨text-embedding-v4
        if qwen_api_key:
            self.register_model(ModelConfig(
                alias="embedding",
                provider="qwen",
                model_id="text-embedding-v4",  # ä½¿ç”¨ç¨³å®šçš„v4æ¨¡å‹ï¼Œä¸è®­ç»ƒæ—¶ä¸€è‡´
                model_type=ModelType.EMBEDDING,
                api_key=qwen_api_key,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings"  # ä½¿ç”¨embeddingsä¸“ç”¨endpoint
            ))
            
        # ğŸ” æœç´¢æ¨¡å‹ (SEARCH) - ä½¿ç”¨sonaræœ€ä¾¿å®œçš„æ¨¡å‹
        perplexity_api_key = os.getenv('PERPLEXITY_API_KEY')
        if perplexity_api_key:
            self.register_model(ModelConfig(
                alias="search",
                provider="perplexity",
                model_id="sonar",  # ä½¿ç”¨æœ€ä¾¿å®œçš„sonaræ¨¡å‹æ§åˆ¶æˆæœ¬
                model_type=ModelType.SEARCH,
                api_key=perplexity_api_key,
                base_url="https://api.perplexity.ai/chat/completions"
            ))
        
    def register_model(self, config: ModelConfig):
        """æ³¨å†Œæ¨¡å‹"""
        self.models[config.alias] = config
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        self.usage_stats[config.alias] = {
            "calls": 0,
            "success": 0,
            "errors": 0,
            "last_used": None
        }
        
        self.token_usage[config.alias] = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0
        }
        
    def get_model(self, alias: str) -> Optional[ModelConfig]:
        """è·å–æ¨¡å‹é…ç½®"""
        return self.models.get(alias)
        
    def get_model_status(self) -> Dict[str, Dict]:
        """è·å–æ‰€æœ‰æ¨¡å‹çŠ¶æ€"""
        status = {}
        for alias, config in self.models.items():
            status[alias] = {
                "available": bool(config.api_key),
                "provider": config.provider,
                "model_id": config.model_id,
                "type": config.model_type.value,
                "usage": self.usage_stats.get(alias, {}),
                "tokens": self.token_usage.get(alias, {})
            }
        return status
    
    async def call_llm(self, prompt: str, model_alias: str = "llm", **kwargs) -> Dict[str, Any]:
        """è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹"""
        config = self.get_model(model_alias)
        if not config or config.model_type != ModelType.LLM:
            raise ValueError(f"LLMæ¨¡å‹ {model_alias} ä¸å¯ç”¨")
        
        # æ„å»ºåƒé—®LLMè¯·æ±‚æ ¼å¼
        if config.provider == "qwen":
            # ä½¿ç”¨åƒé—®åŸç”ŸAPIæ ¼å¼
            payload = {
                "model": config.model_id,
                "input": {
                    "messages": [{"role": "user", "content": prompt}]
                },
                "parameters": {
                    "max_tokens": kwargs.get("max_tokens", 2000),
                    "temperature": kwargs.get("temperature", 0.1)
                }
            }
        else:
            # å…¶ä»–æä¾›å•†ä½¿ç”¨OpenAIæ ¼å¼
            payload = {
                "model": config.model_id,
                "messages": [{"role": "user", "content": prompt}],
                **kwargs
            }
            
        return await self._make_api_call(config, payload)
    
    async def call_embedding(self, texts: List[str], model_alias: str = "embedding", **kwargs) -> Dict[str, Any]:
        """è°ƒç”¨å‘é‡åŒ–æ¨¡å‹ - æ”¯æŒåƒé—®text-embedding-v4å®˜æ–¹æ ¼å¼"""
        config = self.get_model(model_alias)
        if not config or config.model_type != ModelType.EMBEDDING:
            raise ValueError(f"å‘é‡åŒ–æ¨¡å‹ {model_alias} ä¸å¯ç”¨")
            
        # ä½¿ç”¨å®˜æ–¹OpenAIå…¼å®¹æ ¼å¼
        if config.provider == "qwen":
            # æ„å»ºç¬¦åˆå®˜æ–¹ç¤ºä¾‹çš„è¯·æ±‚æ ¼å¼
            payload = {
                "model": config.model_id,  # "text-embedding-v4"
                "input": texts[0] if len(texts) == 1 else texts,  # å•ä¸ªæ–‡æœ¬ç›´æ¥ä¼ å­—ç¬¦ä¸²ï¼Œå¤šä¸ªä¼ æ•°ç»„
                "dimensions": kwargs.get("dimensions", 1024),  # v4æ”¯æŒæŒ‡å®šç»´åº¦ï¼Œé»˜è®¤1024
                "encoding_format": kwargs.get("encoding_format", "float")
            }
        else:
            # å…¶ä»–æä¾›å•†çš„æ ‡å‡†æ ¼å¼
            payload = {
                "model": config.model_id, 
                "input": texts
            }
            
        result = await self._make_api_call(config, payload)
        
        # å¤„ç†å“åº”æ ¼å¼ï¼ˆOpenAIå…¼å®¹æ ¼å¼ï¼‰
        if result.get("success", True):
            if "data" in result:
                # æ ‡å‡†OpenAIæ ¼å¼
                embeddings = [item.get("embedding", []) for item in result["data"]]
            else:
                # å¯èƒ½çš„å…¶ä»–æ ¼å¼
                embeddings = result.get("embeddings", [])
                if not isinstance(embeddings, list):
                    embeddings = [embeddings]
        else:
            embeddings = []
            
        return {
            "success": True,
            "embeddings": embeddings,
            "usage": result.get("usage", {}),
            "model": config.model_id,
            "provider": config.provider
        }
    
    async def call_search(self, query: str, model_alias: str = "search", **kwargs) -> Dict[str, Any]:
        """è°ƒç”¨æœç´¢æ¨¡å‹ - æ”¯æŒPerplexityç‰¹æœ‰å‚æ•°"""
        config = self.get_model(model_alias)
        if not config or config.model_type != ModelType.SEARCH:
            raise ValueError(f"æœç´¢æ¨¡å‹ {model_alias} ä¸å¯ç”¨")
            
        # æ„å»ºåŸºç¡€payload
        payload = {
            "model": config.model_id,
            "messages": [{"role": "user", "content": query}]
        }
        
        # å¤„ç†Perplexityç‰¹æœ‰å‚æ•°
        if config.provider == "perplexity":
            payload.update(self._build_perplexity_params(**kwargs))
        else:
            payload.update(kwargs)
        
        return await self._make_api_call(config, payload)
    
    def _build_perplexity_params(self, **kwargs) -> Dict[str, Any]:
        """æ„å»ºPerplexityç‰¹æœ‰å‚æ•°"""
        perplexity_params = {}
        
        # æ—¶é—´è¿‡æ»¤
        if "search_recency_filter" in kwargs:
            perplexity_params["search_recency_filter"] = kwargs["search_recency_filter"]
        elif "time_filter" in kwargs:
            perplexity_params["search_recency_filter"] = kwargs["time_filter"]
            
        # åŸŸåè¿‡æ»¤åŠŸèƒ½å·²ç§»é™¤ - é€šè¿‡promptå¼•å¯¼AIæœç´¢æƒå¨æ¥æº
            
        # è¿”å›ç›¸å…³é—®é¢˜
        if "return_related_questions" in kwargs:
            perplexity_params["return_related_questions"] = kwargs["return_related_questions"]
            
        # æœç´¢é€‰é¡¹
        if "web_search_options" in kwargs:
            perplexity_params["web_search_options"] = kwargs["web_search_options"]
        elif "search_context_size" in kwargs:
            perplexity_params["web_search_options"] = {
                "search_context_size": kwargs["search_context_size"]
            }
            
        # æ—¥æœŸè¿‡æ»¤
        for date_param in ["search_after_date_filter", "search_before_date_filter"]:
            if date_param in kwargs:
                perplexity_params[date_param] = kwargs[date_param]
                
        # è¿”å›å›¾ç‰‡
        if "return_images" in kwargs:
            perplexity_params["return_images"] = kwargs["return_images"]
            
        # å…¶ä»–å‚æ•°
        for param in ["temperature", "max_tokens", "top_p"]:
            if param in kwargs:
                perplexity_params[param] = kwargs[param]
                
        # ç¡®ä¿max_tokensæœ‰é»˜è®¤å€¼
        if "max_tokens" not in perplexity_params:
            perplexity_params["max_tokens"] = 4000
        
        return perplexity_params
    
    async def _make_api_call(self, config: ModelConfig, payload: Dict) -> Dict[str, Any]:
        """æ‰§è¡ŒAPIè°ƒç”¨ - æ”¯æŒå¼‚æ­¥HTTPå®¢æˆ·ç«¯"""
        self.usage_stats[config.alias]["calls"] += 1
        
        try:
            # æ„å»ºè¯·æ±‚å¤´
            headers = {
                "Authorization": f"Bearer {config.api_key}",
                "Content-Type": "application/json"
            }
            headers.update(config.headers)
            
            # ä¼˜å…ˆä½¿ç”¨å¼‚æ­¥HTTPå®¢æˆ·ç«¯
            if HTTPX_AVAILABLE:
                return await self._make_async_request(config, payload, headers)
            else:
                # é™çº§åˆ°åŒæ­¥è¯·æ±‚ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰
                return await self._make_sync_request(config, payload, headers)
                
        except Exception as e:
            self.usage_stats[config.alias]["errors"] += 1
            error_msg = f"APIè°ƒç”¨å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            print(f"ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯: {type(e).__name__}: {str(e)}")
            print(f"ğŸ“ è¯·æ±‚URL: {config.base_url}")
            print(f"ğŸ”‘ API KeyçŠ¶æ€: {'å·²è®¾ç½®' if config.api_key else 'æœªè®¾ç½®'}")
            if hasattr(e, 'response'):
                print(f"ğŸ“„ å“åº”çŠ¶æ€: {getattr(e.response, 'status_code', 'N/A')}")
                print(f"ğŸ“„ å“åº”å†…å®¹: {getattr(e.response, 'text', 'N/A')[:500]}")
            return {"success": False, "error": error_msg}
    
    async def _make_async_request(self, config: ModelConfig, payload: Dict, headers: Dict) -> Dict[str, Any]:
        """ä½¿ç”¨httpxæ‰§è¡Œå¼‚æ­¥APIè°ƒç”¨"""
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                config.base_url,
                json=payload,
                headers=headers
            )
            
            if response.status_code == 200:
                result = response.json()
                self._update_stats(config, result)
                return self._process_response(config, result)
            else:
                raise Exception(f"HTTP {response.status_code}: {response.text}")
    
    async def _make_sync_request(self, config: ModelConfig, payload: Dict, headers: Dict) -> Dict[str, Any]:
        """åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥è¯·æ±‚"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._sync_request_impl, config, payload, headers)
    
    def _sync_request_impl(self, config: ModelConfig, payload: Dict, headers: Dict) -> Dict[str, Any]:
        """åŒæ­¥è¯·æ±‚çš„å®é™…å®ç°"""
        request_data = json.dumps(payload).encode('utf-8')
        req = urllib.request.Request(
            config.base_url,
            data=request_data,
            headers=headers
        )
        
        with urllib.request.urlopen(req, timeout=120) as response:
            if response.status == 200:
                result_data = response.read().decode('utf-8')
                result = json.loads(result_data)
                self._update_stats(config, result)
                return self._process_response(config, result)
            else:
                error_text = response.read().decode('utf-8')
                raise Exception(f"APIé”™è¯¯ {response.status}: {error_text}")
    
    def _update_stats(self, config: ModelConfig, result: Dict):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.usage_stats[config.alias]["success"] += 1
        self.usage_stats[config.alias]["last_used"] = datetime.now().isoformat()
        
        # ç»Ÿè®¡tokenä½¿ç”¨
        if "usage" in result:
            usage = result["usage"]
            self.token_usage[config.alias]["input_tokens"] += usage.get("prompt_tokens", 0)
            self.token_usage[config.alias]["output_tokens"] += usage.get("completion_tokens", 0)
            self.token_usage[config.alias]["total_tokens"] += usage.get("total_tokens", 0)
    
    def _process_response(self, config: ModelConfig, result: Dict) -> Dict[str, Any]:
        """å¤„ç†ä¸åŒæä¾›å•†çš„å“åº”æ ¼å¼"""
        if config.provider == "perplexity":
            # æ ‡å‡†åŒ–Perplexityå“åº”
            content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
            search_results = result.get("search_results", [])
            related_questions = result.get("related_questions", [])
            
            return {
                "success": True,
                "content": content,
                "search_results": search_results,
                "related_questions": related_questions,
                "usage": result.get("usage", {}),
                "model": config.model_id,
                "provider": config.provider,
                "results": search_results  # å…¼å®¹æ€§å­—æ®µ
            }
        
        # åƒé—®LLMå“åº”å¤„ç†
        if config.provider == "qwen" and config.model_type == ModelType.LLM:
            # åƒé—®åŸç”ŸAPIå“åº”æ ¼å¼
            if "output" in result:
                content = result["output"].get("text", "")
                return {
                    "success": True,
                    "content": content,
                    "usage": result.get("usage", {}),
                    "model": config.model_id,
                    "provider": config.provider,
                    "choices": [{"message": {"content": content}}]  # å…¼å®¹æ€§å­—æ®µ
                }
            else:
                return {
                    "success": False,
                    "error": f"Invalid Qwen response format: {result}"
                }
        
        # å…¶ä»–æ¨¡å‹çš„æ ‡å‡†å“åº”
        if config.model_type == ModelType.EMBEDDING:
            # å‘é‡åŒ–å“åº”å·²åœ¨call_embeddingä¸­å¤„ç†
            return result
        else:
            # LLMå“åº”
            content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
            return {
                "success": True,
                "content": content,
                "usage": result.get("usage", {}),
                "model": config.model_id,
                "provider": config.provider
            }


# å…¨å±€å®ä¾‹
model_manager = UnifiedModelManager()

# ä¾¿åˆ©å‡½æ•°
def get_model_status() -> Dict[str, Dict]:
    """è·å–æ¨¡å‹çŠ¶æ€"""
    return model_manager.get_model_status()

async def call_llm(prompt: str, model_alias: str = "llm", **kwargs) -> Dict[str, Any]:
    """è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹"""
    return await model_manager.call_llm(prompt, model_alias, **kwargs)

async def call_embedding(texts: List[str], model_alias: str = "embedding", **kwargs) -> Dict[str, Any]:
    """è°ƒç”¨å‘é‡åŒ–æ¨¡å‹"""
    return await model_manager.call_embedding(texts, model_alias, **kwargs)

async def call_search(query: str, model_alias: str = "search", **kwargs) -> Dict[str, Any]:
    """è°ƒç”¨æœç´¢æ¨¡å‹"""
    return await model_manager.call_search(query, model_alias, **kwargs)

def log_tokens(alias: str, input_tokens: int, output_tokens: int):
    """è®°å½•tokenä½¿ç”¨"""
    if alias in model_manager.token_usage:
        model_manager.token_usage[alias]["input_tokens"] += input_tokens
        model_manager.token_usage[alias]["output_tokens"] += output_tokens
        model_manager.token_usage[alias]["total_tokens"] += input_tokens + output_tokens