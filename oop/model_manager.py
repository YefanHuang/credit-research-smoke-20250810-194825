#!/usr/bin/env python3
"""
ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨ - åŸºäºæˆåŠŸçš„Qwenè°ƒç”¨æ¨¡å¼é‡æ–°è®¾è®¡
ä½¿ç”¨OpenAI Python SDK + Qwen Compatible API
"""
import json
import os
import asyncio
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum

# å¯¼å…¥OpenAIå®¢æˆ·ç«¯ï¼ˆåŸºäºæˆåŠŸç¤ºä¾‹ï¼‰
try:
    from openai import OpenAI
    OPENAI_CLIENT_AVAILABLE = True
except ImportError:
    OPENAI_CLIENT_AVAILABLE = False
    print("âš ï¸ OpenAI Python SDKæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install openai")

class ModelType(Enum):
    LLM = "llm"
    EMBEDDING = "embedding"
    SEARCH = "search"

@dataclass
class ModelConfig:
    alias: str
    provider: str
    model_id: str
    model_type: ModelType
    api_key: str
    base_url: str = ""
    headers: Dict[str, str] = None

    def __post_init__(self):
        if self.headers is None:
            self.headers = {}

class UnifiedModelManager:
    """ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨ - åŸºäºOpenAI SDKçš„æˆåŠŸæ¨¡å¼"""
    
    def __init__(self):
        self.models: Dict[str, ModelConfig] = {}
        self.clients: Dict[str, OpenAI] = {}  # å­˜å‚¨OpenAIå®¢æˆ·ç«¯
        self.usage_stats: Dict[str, Dict] = {}
        self.token_usage: Dict[str, Dict] = {}
        
        # æ³¨å†Œé»˜è®¤æ¨¡å‹
        self._register_default_models()
    
    def _register_default_models(self):
        """æ³¨å†Œé»˜è®¤æ¨¡å‹é…ç½® - åŸºäºæˆåŠŸç¤ºä¾‹"""
        
        # è·å–APIå¯†é’¥
        qwen_api_key = os.getenv('DASHSCOPE_API_KEY') or os.getenv('QWEN_API_KEY')
        perplexity_api_key = os.getenv('PERPLEXITY_API_KEY')
        
        # ğŸ¤– å¤§è¯­è¨€æ¨¡å‹ (LLM) - ä½¿ç”¨OpenAIå…¼å®¹æ¨¡å¼
        if qwen_api_key:
            self.register_model(ModelConfig(
                alias="llm",
                provider="qwen", 
                model_id="qwen-turbo",  # åŸºäºæˆåŠŸç¤ºä¾‹
                model_type=ModelType.LLM,
                api_key=qwen_api_key,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"  # æˆåŠŸçš„endpoint
            ))
            
        # ğŸ§  å‘é‡åŒ–æ¨¡å‹ (EMBEDDING) - ä½¿ç”¨OpenAIå…¼å®¹æ¨¡å¼
        if qwen_api_key:
            self.register_model(ModelConfig(
                alias="embedding",
                provider="qwen",
                model_id="text-embedding-v4",  # éªŒè¯æˆåŠŸçš„æ¨¡å‹
                model_type=ModelType.EMBEDDING,
                api_key=qwen_api_key,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"  # ç»Ÿä¸€endpoint
            ))
            
        # ğŸ” æœç´¢æ¨¡å‹ (SEARCH) - Perplexity
        if perplexity_api_key:
            self.register_model(ModelConfig(
                alias="search",
                provider="perplexity",
                model_id="sonar",
                model_type=ModelType.SEARCH,
                api_key=perplexity_api_key,
                base_url="https://api.perplexity.ai"
            ))
    
    def register_model(self, config: ModelConfig):
        """æ³¨å†Œæ¨¡å‹é…ç½®"""
        self.models[config.alias] = config
        
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.usage_stats[config.alias] = {
            "calls": 0,
            "success": 0,
            "errors": 0,
            "last_used": None
        }
        self.token_usage[config.alias] = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0
        }
        
        # ä¸ºQwenæ¨¡å‹åˆ›å»ºOpenAIå®¢æˆ·ç«¯
        if config.provider == "qwen" and OPENAI_CLIENT_AVAILABLE:
            try:
                self.clients[config.alias] = OpenAI(
                    api_key=config.api_key,
                    base_url=config.base_url
                )
                print(f"âœ… å·²åˆ›å»º {config.alias} çš„OpenAIå®¢æˆ·ç«¯")
            except Exception as e:
                print(f"âš ï¸ åˆ›å»º {config.alias} å®¢æˆ·ç«¯å¤±è´¥: {e}")
    
    def get_model(self, alias: str) -> Optional[ModelConfig]:
        """è·å–æ¨¡å‹é…ç½®"""
        return self.models.get(alias)
    
    def get_client(self, alias: str) -> Optional[OpenAI]:
        """è·å–OpenAIå®¢æˆ·ç«¯"""
        return self.clients.get(alias)
    
    async def call_llm(self, prompt: str, model_alias: str = "llm", **kwargs) -> Dict[str, Any]:
        """è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ - åŸºäºæˆåŠŸçš„OpenAI SDKæ¨¡å¼"""
        config = self.get_model(model_alias)
        if not config or config.model_type != ModelType.LLM:
            return {"success": False, "error": f"LLMæ¨¡å‹ {model_alias} ä¸å¯ç”¨"}
        
        client = self.get_client(model_alias)
        if not client:
            return {"success": False, "error": f"OpenAIå®¢æˆ·ç«¯ {model_alias} ä¸å¯ç”¨"}
        
        try:
            # ä½¿ç”¨æˆåŠŸç¤ºä¾‹çš„è°ƒç”¨æ ¼å¼
            messages = [
                {"role": "system", "content": "You are a professional text analysis assistant, proficient in finance, credit, and risk management."},
                {"role": "user", "content": prompt}
            ]
            
            completion = client.chat.completions.create(
                model=config.model_id,
                messages=messages,
                max_tokens=kwargs.get("max_tokens", 2000),
                temperature=kwargs.get("temperature", 0.1)
            )
            
            content = completion.choices[0].message.content
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.usage_stats[model_alias]["calls"] += 1
            self.usage_stats[model_alias]["success"] += 1
            self.usage_stats[model_alias]["last_used"] = datetime.now().isoformat()
            
            if hasattr(completion, 'usage') and completion.usage:
                usage = completion.usage
                self.token_usage[model_alias]["input_tokens"] += getattr(usage, 'prompt_tokens', 0)
                self.token_usage[model_alias]["output_tokens"] += getattr(usage, 'completion_tokens', 0) 
                self.token_usage[model_alias]["total_tokens"] += getattr(usage, 'total_tokens', 0)
            
            return {
                "success": True,
                "content": content,
                "usage": getattr(completion, 'usage', {}),
                "model": config.model_id,
                "provider": config.provider,
                "choices": [{"message": {"content": content}}]  # å…¼å®¹æ€§å­—æ®µ
            }
            
        except Exception as e:
            self.usage_stats[model_alias]["errors"] += 1
            error_msg = f"LLMè°ƒç”¨å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            return {"success": False, "error": error_msg}
    
    async def call_embedding(self, texts: List[str], model_alias: str = "embedding", **kwargs) -> Dict[str, Any]:
        """è°ƒç”¨å‘é‡åŒ–æ¨¡å‹ - åŸºäºæˆåŠŸçš„OpenAI SDKæ¨¡å¼"""
        config = self.get_model(model_alias)
        if not config or config.model_type != ModelType.EMBEDDING:
            return {"success": False, "error": f"å‘é‡åŒ–æ¨¡å‹ {model_alias} ä¸å¯ç”¨"}
        
        client = self.get_client(model_alias)
        if not client:
            return {"success": False, "error": f"OpenAIå®¢æˆ·ç«¯ {model_alias} ä¸å¯ç”¨"}
        
        try:
            # å¤„ç†å•ä¸ªæ–‡æœ¬è¿˜æ˜¯å¤šä¸ªæ–‡æœ¬
            if len(texts) == 1:
                input_text = texts[0]
            else:
                # å¯¹äºå¤šä¸ªæ–‡æœ¬ï¼Œéœ€è¦åˆ†åˆ«å¤„ç†
                input_text = texts[0]  # å…ˆå¤„ç†ç¬¬ä¸€ä¸ª
            
            # ä½¿ç”¨æˆåŠŸç¤ºä¾‹çš„è°ƒç”¨æ ¼å¼
            embedding_response = client.embeddings.create(
                model=config.model_id,
                input=input_text,
                dimensions=kwargs.get("dimensions", 1024)
            )
            
            # æå–å‘é‡æ•°æ®
            embeddings = []
            if len(texts) == 1:
                embeddings = [embedding_response.data[0].embedding]
            else:
                # å¤„ç†å¤šä¸ªæ–‡æœ¬ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
                for text in texts:
                    resp = client.embeddings.create(
                        model=config.model_id,
                        input=text,
                        dimensions=kwargs.get("dimensions", 1024)
                    )
                    embeddings.append(resp.data[0].embedding)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.usage_stats[model_alias]["calls"] += 1
            self.usage_stats[model_alias]["success"] += 1
            self.usage_stats[model_alias]["last_used"] = datetime.now().isoformat()
            
            if hasattr(embedding_response, 'usage') and embedding_response.usage:
                usage = embedding_response.usage
                self.token_usage[model_alias]["input_tokens"] += getattr(usage, 'prompt_tokens', 0)
                self.token_usage[model_alias]["total_tokens"] += getattr(usage, 'total_tokens', 0)
            
            return {
                "success": True,
                "embeddings": embeddings,
                "usage": getattr(embedding_response, 'usage', {}),
                "model": config.model_id,
                "provider": config.provider
            }
            
        except Exception as e:
            self.usage_stats[model_alias]["errors"] += 1
            error_msg = f"å‘é‡åŒ–è°ƒç”¨å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            return {"success": False, "error": error_msg}
    
    def get_model_status(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ¨¡å‹çŠ¶æ€"""
        status = {}
        for alias, config in self.models.items():
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
            available = False
            if config.provider == "qwen":
                available = bool(config.api_key and self.clients.get(alias))
            elif config.provider == "perplexity":
                available = bool(config.api_key)
            
            status[alias] = {
                "provider": config.provider,
                "model_id": config.model_id,
                "type": config.model_type.value,
                "available": available,
                "usage": self.usage_stats.get(alias, {}),
                "tokens": self.token_usage.get(alias, {})
            }
        return status

# åˆ›å»ºå…¨å±€æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
model_manager = UnifiedModelManager()

# å…¨å±€è¾…åŠ©å‡½æ•°ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
async def call_llm(prompt: str, model_alias: str = "llm", **kwargs) -> Dict[str, Any]:
    """è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹"""
    return await model_manager.call_llm(prompt, model_alias, **kwargs)

async def call_embedding(texts: List[str], model_alias: str = "embedding", **kwargs) -> Dict[str, Any]:
    """è°ƒç”¨å‘é‡åŒ–æ¨¡å‹"""
    return await model_manager.call_embedding(texts, model_alias, **kwargs)

def get_model_status() -> Dict[str, Dict[str, Any]]:
    """è·å–æ¨¡å‹çŠ¶æ€"""
    return model_manager.get_model_status()

# ä¸ºäº†å…¼å®¹æ€§ï¼Œæ·»åŠ å…¶ä»–å¿…è¦çš„å‡½æ•°
async def call_search(prompt: str, model_alias: str = "search", **kwargs) -> Dict[str, Any]:
    """è°ƒç”¨æœç´¢æ¨¡å‹ - å®ç°Perplexity APIè°ƒç”¨ï¼Œæ”¯æŒä¸¥æ ¼7å¤©æ—¶é—´é™åˆ¶"""
    config = model_manager.get_model(model_alias)
    if not config or config.model_type != ModelType.SEARCH:
        return {"success": False, "error": f"æœç´¢æ¨¡å‹ {model_alias} ä¸å¯ç”¨"}
    
    try:
        from openai import OpenAI
        from datetime import datetime, timedelta
        
        # åˆ›å»ºPerplexityå®¢æˆ·ç«¯
        client = OpenAI(
            api_key=config.api_key,
            base_url="https://api.perplexity.ai"
        )
        
        # æ„å»ºä¸¥æ ¼çš„7å¤©æ—¶é—´é™åˆ¶æŸ¥è¯¢
        today = datetime.now()
        seven_days_ago = today - timedelta(days=7)
        
        # æ·»åŠ æ—¶é—´é™åˆ¶åˆ°æŸ¥è¯¢ä¸­
        time_constraint = f"published after {seven_days_ago.strftime('%Y-%m-%d')} and before {today.strftime('%Y-%m-%d')}"
        enhanced_query = f"{prompt} {time_constraint}"
        
        print(f"ğŸ” æœç´¢æŸ¥è¯¢ (ä¸¥æ ¼7å¤©é™åˆ¶): {enhanced_query}")
        print(f"ğŸ”§ ä½¿ç”¨æ¨¡å‹: {config.model_id}")
        print(f"ğŸ“… æ—¥æœŸèŒƒå›´: {seven_days_ago.strftime('%Y-%m-%d')} åˆ° {today.strftime('%Y-%m-%d')}")
        
        # è°ƒç”¨Perplexity API
        response = client.chat.completions.create(
            model=config.model_id,
            messages=[
                {
                    "role": "system",
                    "content": "You are a professional financial research assistant. Provide accurate, recent information with authoritative sources."
                },
                {
                    "role": "user", 
                    "content": enhanced_query
                }
            ],
            extra_body={
                "search_recency_filter": "week",  # Perplexity APIè¦æ±‚: hour/day/week/month/year
                "return_citations": True,
                "return_images": False,
                "temperature": 0.2,
                "max_tokens": kwargs.get("max_tokens", 4000)
            }
        )
        
        content = response.choices[0].message.content
        
        # æå–citationsï¼ˆçœŸå®URLï¼‰
        citations = []
        if hasattr(response, 'citations') and response.citations:
            citations = response.citations
        elif isinstance(response, dict) and 'citations' in response:
            citations = response['citations']
        
        print(f"ğŸ“Š APIå“åº”æˆåŠŸ: å†…å®¹é•¿åº¦ {len(content)} å­—ç¬¦")
        print(f"ğŸ“ å“åº”é¢„è§ˆ: {content[:200]}...")
        print(f"ğŸ”— æ‰¾åˆ° {len(citations)} ä¸ªå¼•ç”¨é“¾æ¥")
        
        # è§£ææœç´¢ç»“æœä¸ºæ ‡å‡†æ ¼å¼
        results = []
        try:
            # æ”¹è¿›çš„ç»“æœè§£æé€»è¾‘ - ä»Perplexityå“åº”ä¸­æå–å¤šä¸ªç»“æœ
            import re
            
            # å°è¯•æŒ‰æ®µè½å’Œæ ‡é¢˜åˆ†å‰²å†…å®¹
            paragraphs = [p.strip() for p in content.split('\n\n') if len(p.strip()) > 100]
            
            # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„æ®µè½ï¼ŒæŒ‰å¥å­åˆ†å‰²
            if len(paragraphs) < 3:
                sentences = [s.strip() for s in content.split('. ') if len(s.strip()) > 80]
                paragraphs = ['. '.join(sentences[i:i+3]) for i in range(0, min(len(sentences), 15), 3)]
            
            # ç”Ÿæˆå¤šä¸ªç»“æœï¼Œä½¿ç”¨çœŸå®çš„citations URL
            for i, paragraph in enumerate(paragraphs[:5]):  # æœ€å¤š5ä¸ªç»“æœ
                if len(paragraph.strip()) > 50:
                    # ä»å†…å®¹ä¸­æå–å¯èƒ½çš„æ ‡é¢˜
                    lines = paragraph.split('\n')
                    title_candidate = lines[0] if lines else f"Credit Research Analysis {i+1}"
                    
                    # æ¸…ç†æ ‡é¢˜
                    if len(title_candidate) > 100:
                        title_candidate = title_candidate[:97] + "..."
                    
                    # ä½¿ç”¨çœŸå®çš„citation URLï¼Œå¦‚æœå¯ç”¨çš„è¯
                    real_url = citations[i] if i < len(citations) else "https://perplexity.ai/search"
                    
                    results.append({
                        "title": title_candidate,
                        "content": paragraph.strip(),
                        "url": real_url,  # ä½¿ç”¨çœŸå®çš„å¼•ç”¨URL
                        "relevance_score": max(0.85 - i*0.05, 0.5),  # æ›´é«˜çš„åŸºç¡€åˆ†æ•°
                        "source": "Perplexity AI",
                        "date_range": f"{seven_days_ago.strftime('%Y-%m-%d')} to {today.strftime('%Y-%m-%d')}",
                        "search_query": enhanced_query,
                        "model_used": config.model_id,
                        "citation_index": i + 1 if i < len(citations) else None
                    })
            
            # å¦‚æœä»ç„¶æ²¡æœ‰è¶³å¤Ÿçš„ç»“æœï¼Œåˆ†è§£ä¸»è¦å†…å®¹
            if len(results) < 3 and content:
                # ä½¿ç”¨å‰©ä½™çš„citationsæˆ–Perplexityé“¾æ¥
                remaining_url = citations[len(results)] if len(results) < len(citations) else "https://perplexity.ai/search"
                
                results.append({
                    "title": f"Comprehensive Analysis: {kwargs.get('topic', prompt)[:60]}",
                    "content": content[:2000] + "..." if len(content) > 2000 else content,
                    "url": remaining_url,  # ä½¿ç”¨çœŸå®URL
                    "relevance_score": 0.9,
                    "source": "Perplexity AI",
                    "date_range": f"{seven_days_ago.strftime('%Y-%m-%d')} to {today.strftime('%Y-%m-%d')}",
                    "search_query": enhanced_query,
                    "model_used": config.model_id,
                    "citation_index": len(results) + 1 if len(results) < len(citations) else None
                })
                
        except Exception as e:
            print(f"âš ï¸ ç»“æœè§£æé”™è¯¯: {e}")
            # å•ä¸ªç»“æœå›é€€ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªcitationå¦‚æœæœ‰çš„è¯
            fallback_url = citations[0] if len(citations) > 0 else "https://perplexity.ai/search"
            results = [{
                "title": f"Credit Research Analysis: {kwargs.get('topic', 'Financial Research')}",
                "content": content[:1500] + "..." if len(content) > 1500 else content,
                "url": fallback_url,  # ä½¿ç”¨çœŸå®URLæˆ–Perplexityæœç´¢é“¾æ¥
                "relevance_score": 0.75,
                "source": "Perplexity AI",
                "date_range": f"{seven_days_ago.strftime('%Y-%m-%d')} to {today.strftime('%Y-%m-%d')}",
                "search_query": enhanced_query,
                "model_used": config.model_id,
                "citation_index": 1 if len(citations) > 0 else None
            }]
        
        print(f"âœ… è§£æå¾—åˆ° {len(results)} ä¸ªæœç´¢ç»“æœ")
        for i, result in enumerate(results):
            print(f"  ç»“æœ {i+1}: {result['title'][:50]}... (ç›¸å…³æ€§: {result['relevance_score']:.3f})")
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        model_manager.usage_stats[model_alias]["calls"] += 1
        model_manager.usage_stats[model_alias]["success"] += 1
        model_manager.usage_stats[model_alias]["last_used"] = datetime.now().isoformat()
        
        if hasattr(response, 'usage') and response.usage:
            usage = response.usage
            model_manager.token_usage[model_alias]["input_tokens"] += getattr(usage, 'prompt_tokens', 0)
            model_manager.token_usage[model_alias]["output_tokens"] += getattr(usage, 'completion_tokens', 0)
            model_manager.token_usage[model_alias]["total_tokens"] += getattr(usage, 'total_tokens', 0)
        
        return {
            "success": True,
            "results": results,
            "content": content,
            "usage": {
                "prompt_tokens": getattr(response.usage, 'prompt_tokens', 0) if hasattr(response, 'usage') and response.usage else 0,
                "completion_tokens": getattr(response.usage, 'completion_tokens', 0) if hasattr(response, 'usage') and response.usage else 0,
                "total_tokens": getattr(response.usage, 'total_tokens', 0) if hasattr(response, 'usage') and response.usage else 0
            },
            "model": config.model_id,
            "provider": "perplexity",
            "time_filter_applied": "strict_7_days",
            "date_range": f"{seven_days_ago.strftime('%Y-%m-%d')} to {today.strftime('%Y-%m-%d')}",
            "query": enhanced_query
        }
        
    except Exception as e:
        model_manager.usage_stats[model_alias]["errors"] += 1
        error_msg = f"æœç´¢è°ƒç”¨å¤±è´¥: {e}"
        print(f"âŒ {error_msg}")
        return {"success": False, "error": error_msg}

def log_tokens(usage_data: Dict[str, Any]):
    """è®°å½•tokenä½¿ç”¨æƒ…å†µ"""
    pass  # æš‚æ—¶ä¿æŒå…¼å®¹æ€§

if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    async def test():
        print("ğŸ§ª æµ‹è¯•ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨")
        status = get_model_status()
        for alias, info in status.items():
            available = "âœ…" if info["available"] else "âŒ"
            print(f"  {available} {alias}: {info['provider']}-{info['model_id']} ({info['type']})")
    
    asyncio.run(test())