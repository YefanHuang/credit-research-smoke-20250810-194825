#!/usr/bin/env python3
"""
ğŸ¯ ç»Ÿä¸€ChromaDBè®­ç»ƒå™¨ - ä½¿ç”¨æ–°çš„æ¨¡å‹ç®¡ç†å™¨
æ›¿ä»£workflowä¸­çš„å†…åµŒè„šæœ¬ï¼Œæä¾›æ›´æ¸…æ´çš„æ¥å£
"""

import asyncio
import json
import os
import sys
from datetime import datetime
from pathlib import Path
import hashlib
from typing import List, Dict, Any

# å¯¼å…¥ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨
try:
    from model_manager import call_embedding, call_llm, log_tokens, get_model_status, model_manager
    UNIFIED_MANAGER_AVAILABLE = True
    print("âœ… ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨å·²åŠ è½½")
except ImportError as e:
    print(f"âš ï¸ æ— æ³•å¯¼å…¥ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨: {e}")
    UNIFIED_MANAGER_AVAILABLE = False

# å¯¼å…¥å®æ—¶ç›‘æ§
try:
    from realtime_token_monitor import init_monitor, start_monitoring, stop_monitoring, log_api_call
    TOKEN_MONITOR_AVAILABLE = True
    print("âœ… Tokenç›‘æ§å™¨å·²åŠ è½½")
except ImportError as e:
    print(f"âš ï¸ æ— æ³•å¯¼å…¥Tokenç›‘æ§å™¨: {e}")
    TOKEN_MONITOR_AVAILABLE = False


class UnifiedChromaDBTrainer:
    """ç»Ÿä¸€ChromaDBè®­ç»ƒå™¨"""
    
    def __init__(self, traindb_folder: str = "traindb"):
        self.traindb_folder = Path(traindb_folder)
        self.processed_files_record = "processed_files.json"
        self.training_stats = {
            "total_files": 0,
            "processed_files": 0,
            "skipped_files": 0,
            "total_chunks": 0,
            "total_vectors": 0,
            "api_calls": 0,
            "errors": []
        }
        
    def get_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹é‡å¤å¤„ç†"""
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    def load_processed_files(self) -> Dict[str, str]:
        """åŠ è½½å·²å¤„ç†æ–‡ä»¶è®°å½•"""
        try:
            if os.path.exists(self.processed_files_record):
                with open(self.processed_files_record, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½å¤„ç†è®°å½•: {e}")
        return {}
    
    def save_processed_files(self, processed_files: Dict[str, str]):
        """ä¿å­˜å·²å¤„ç†æ–‡ä»¶è®°å½•"""
        try:
            with open(self.processed_files_record, 'w', encoding='utf-8') as f:
                json.dump(processed_files, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ æ— æ³•ä¿å­˜å¤„ç†è®°å½•: {e}")
    
    async def intelligent_segmentation(self, text: str, max_chunk_size: int = 800) -> List[str]:
        """æ™ºèƒ½æ–‡æœ¬åˆ‡åˆ†"""
        if UNIFIED_MANAGER_AVAILABLE and call_llm:
            try:
                prompt = f"""Please intelligently segment the following text into semantically complete paragraphs, with each segment not exceeding {max_chunk_size} characters:

{text[:2000]}  # limit input length

Requirements:
1. Maintain semantic integrity and coherence
2. Keep each segment under {max_chunk_size} characters
3. Remove reference numbers at the end of sentences or paragraphs (e.g., remove "1" from "investors.1" or "2" from "risk.2")
4. Clean up text transitions (e.g., "investors.1In this paper" should become "investors. In this paper")
5. Return format: one segment per line, separated by "---"

Ensure output is in English.
"""
                print(f"ğŸ”§ è°ƒç”¨LLMæ¨¡å‹è¿›è¡Œæ™ºèƒ½æ–‡æœ¬åˆ‡åˆ†...")
                result = await call_llm(prompt, model_alias="llm", max_tokens=2000, temperature=0.1)
                
                # è§£æLLMè¿”å›çš„åˆ‡åˆ†ç»“æœ
                if result.get('success'):
                    response_text = result.get('content', '')
                    print(f"ğŸ“Š LLM APIå“åº”: æ¨¡å‹={result.get('model', 'unknown')}, æä¾›å•†={result.get('provider', 'unknown')}")
                else:
                    response_text = ""
                    print(f"âŒ LLMåˆ‡åˆ†è°ƒç”¨å¤±è´¥: {result.get('error', 'Unknown error')}")
                chunks = [chunk.strip() for chunk in response_text.split("---") if chunk.strip()]
                
                if chunks:
                    print(f"ğŸ§  LLMæ™ºèƒ½åˆ‡åˆ†: {len(text)} å­—ç¬¦ â†’ {len(chunks)} ä¸ªè¯­ä¹‰å—")
                    return chunks
                else:
                    print("âš ï¸ LLMåˆ‡åˆ†å¤±è´¥ï¼Œä½¿ç”¨ç®€å•åˆ‡åˆ†")
                    
            except Exception as e:
                print(f"âš ï¸ LLMåˆ‡åˆ†å‡ºé”™: {e}")
        
        # ç®€å•åˆ‡åˆ†ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        chunks = []
        for i in range(0, len(text), max_chunk_size):
            chunk = text[i:i + max_chunk_size]
            if chunk.strip():
                chunks.append(chunk.strip())
        
        print(f"âœ‚ï¸ ç®€å•åˆ‡åˆ†: {len(text)} å­—ç¬¦ â†’ {len(chunks)} ä¸ªå—")
        return chunks
    
    async def create_embeddings(self, texts: List[str]) -> Dict[str, Any]:
        """åˆ›å»ºå‘é‡åµŒå…¥"""
        if UNIFIED_MANAGER_AVAILABLE and call_embedding:
            try:
                print(f"ğŸ”§ è°ƒç”¨embeddingæ¨¡å‹å‘é‡åŒ– {len(texts)} ä¸ªæ–‡æœ¬...")
                print(f"ğŸ” ä½¿ç”¨æ¨¡å‹: alias='embedding' (åº”æ˜ å°„åˆ°text-embedding-v4)")
                
                # ä½¿ç”¨é¢å‘å¯¹è±¡æ–¹å¼è°ƒç”¨ï¼Œæ˜ç¡®æŒ‡å®šæ¨¡å‹åˆ«å
                result = await call_embedding(texts, model_alias="embedding")
                
                if not result.get('success', True):
                    raise Exception(f"APIè¿”å›å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
                embeddings = result.get('embeddings', [])
                if not embeddings:
                    raise Exception("è¿”å›çš„å‘é‡æ•°æ®ä¸ºç©º")
                
                # è®°å½•APIä½¿ç”¨
                input_tokens = sum(len(text.split()) for text in texts)
                if TOKEN_MONITOR_AVAILABLE:
                    log_api_call("embedding", "text-embedding-v4", input_tokens, 0)
                
                print(f"âœ… ç»Ÿä¸€ç®¡ç†å™¨å‘é‡åŒ–æˆåŠŸ: {len(texts)} ä¸ªæ–‡æœ¬ â†’ {len(embeddings)} ä¸ªå‘é‡")
                print(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {result.get('model', 'unknown')} (æä¾›å•†: {result.get('provider', 'unknown')})")
                self.training_stats["api_calls"] += 1
                return result
                
            except Exception as e:
                print(f"âŒ ç»Ÿä¸€ç®¡ç†å™¨å‘é‡åŒ–å¤±è´¥: {e}")
                print(f"ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯: {type(e).__name__}: {str(e)}")
                self.training_stats["errors"].append(f"å‘é‡åŒ–å¤±è´¥: {str(e)}")
        
        # æ¨¡æ‹Ÿå‘é‡åŒ–ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        import random
        embeddings = [[random.random() for _ in range(1536)] for _ in texts]
        print(f"ğŸ² æ¨¡æ‹Ÿå‘é‡åŒ–: {len(texts)} ä¸ªæ–‡æœ¬ â†’ {len(embeddings)} ä¸ªå‘é‡")
        
        return {
            "embeddings": embeddings,
            "success": True,
            "model": "mock-embedding"
        }
    
    async def process_file(self, file_path: Path, processed_files: Dict[str, str]) -> bool:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†
            file_hash = self.get_file_hash(file_path)
            if str(file_path) in processed_files and processed_files[str(file_path)] == file_hash:
                print(f"â­ï¸ è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {file_path.name}")
                self.training_stats["skipped_files"] += 1
                return True
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if not content.strip():
                print(f"âš ï¸ ç©ºæ–‡ä»¶: {file_path.name}")
                return False
            
            print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name} ({len(content)} å­—ç¬¦)")
            
            # æ™ºèƒ½åˆ‡åˆ†
            chunks = await self.intelligent_segmentation(content)
            self.training_stats["total_chunks"] += len(chunks)
            
            # å‘é‡åŒ–
            if chunks:
                embeddings_result = await self.create_embeddings(chunks)
                if embeddings_result.get("success"):
                    self.training_stats["total_vectors"] += len(embeddings_result["embeddings"])
                    
                    # è¿™é‡Œåº”è¯¥ä¿å­˜åˆ°ChromaDB
                    # ç›®å‰åªæ˜¯æ¨¡æ‹Ÿä¿å­˜
                    print(f"ğŸ’¾ æ¨¡æ‹Ÿä¿å­˜åˆ°ChromaDB: {len(chunks)} ä¸ªå‘é‡")
            
            # æ ‡è®°æ–‡ä»¶å·²å¤„ç†
            processed_files[str(file_path)] = file_hash
            self.training_stats["processed_files"] += 1
            
            return True
            
        except Exception as e:
            error_msg = f"å¤„ç†æ–‡ä»¶ {file_path.name} å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            self.training_stats["errors"].append(error_msg)
            return False
    
    async def train_from_folder(self, force_retrain: bool = False) -> Dict[str, Any]:
        """ä»æ–‡ä»¶å¤¹è®­ç»ƒChromaDB"""
        
        if not self.traindb_folder.exists():
            print(f"âŒ è®­ç»ƒæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {self.traindb_folder}")
            return {"success": False, "error": "è®­ç»ƒæ–‡ä»¶å¤¹ä¸å­˜åœ¨"}
        
        # è·å–æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶
        doc_files = []
        for ext in ['*.md', '*.txt', '*.pdf', '*.docx']:
            doc_files.extend(self.traindb_folder.glob(ext))
        
        if not doc_files:
            print(f"ğŸ“ {self.traindb_folder} ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£æ–‡ä»¶")
            return {"success": False, "error": "æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£æ–‡ä»¶"}
        
        self.training_stats["total_files"] = len(doc_files)
        print(f"ğŸ“š å‘ç° {len(doc_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
        
        # åŠ è½½å·²å¤„ç†æ–‡ä»¶è®°å½•
        processed_files = {} if force_retrain else self.load_processed_files()
        
        # åˆå§‹åŒ–Tokenç›‘æ§
        if TOKEN_MONITOR_AVAILABLE:
            estimated_tokens = len(doc_files) * 2000  # ç²—ç•¥ä¼°ç®—
            init_monitor(qwen_limit=estimated_tokens * 2, cost_limit=1.0)
            start_monitoring()
            print(f"ğŸ” Tokenç›‘æ§å·²å¯åŠ¨ï¼Œé¢„ä¼°ä½¿ç”¨: {estimated_tokens:,} tokens")
        
        # é€ä¸ªå¤„ç†æ–‡ä»¶
        start_time = datetime.now()
        
        for i, file_path in enumerate(doc_files, 1):
            print(f"\nğŸ“‚ [{i}/{len(doc_files)}] å¤„ç†æ–‡ä»¶...")
            success = await self.process_file(file_path, processed_files)
            
            # ä¿å­˜è¿›åº¦
            if i % 5 == 0 or i == len(doc_files):
                self.save_processed_files(processed_files)
                print(f"ğŸ’¾ å·²ä¿å­˜è¿›åº¦: {i}/{len(doc_files)}")
        
        # åœæ­¢Tokenç›‘æ§
        if TOKEN_MONITOR_AVAILABLE:
            stop_monitoring()
        
        # è®¡ç®—è€—æ—¶
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # æœ€ç»ˆç»Ÿè®¡
        result = {
            "success": True,
            "timestamp": end_time.isoformat(),
            "duration_seconds": duration,
            "stats": self.training_stats,
            "summary": {
                "æ€»æ–‡ä»¶æ•°": self.training_stats["total_files"],
                "å·²å¤„ç†": self.training_stats["processed_files"],
                "å·²è·³è¿‡": self.training_stats["skipped_files"],
                "æ€»æ–‡æœ¬å—": self.training_stats["total_chunks"],
                "æ€»å‘é‡æ•°": self.training_stats["total_vectors"],
                "APIè°ƒç”¨": self.training_stats["api_calls"],
                "é”™è¯¯æ•°": len(self.training_stats["errors"])
            }
        }
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸ æ€»è€—æ—¶: {duration:.1f} ç§’")
        for key, value in result["summary"].items():
            print(f"ğŸ“Š {key}: {value}")
        
        return result


async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€ChromaDBè®­ç»ƒå™¨")
    parser.add_argument("--traindb", default="traindb", help="è®­ç»ƒæ–‡ä»¶å¤¹è·¯å¾„")
    parser.add_argument("--force-retrain", action="store_true", help="å¼ºåˆ¶é‡æ–°è®­ç»ƒæ‰€æœ‰æ–‡ä»¶")
    parser.add_argument("--token-limit", type=int, default=100000, help="Tokené™åˆ¶")
    
    args = parser.parse_args()
    
    print("ğŸ¯ ç»Ÿä¸€ChromaDBè®­ç»ƒå™¨")
    print("=" * 40)
    print(f"ğŸ“ è®­ç»ƒæ–‡ä»¶å¤¹: {args.traindb}")
    print(f"ğŸ”„ å¼ºåˆ¶é‡è®­: {args.force_retrain}")
    print(f"ğŸ« Tokené™åˆ¶: {args.token_limit:,}")
    
    # æ£€æŸ¥æ¨¡å‹ç®¡ç†å™¨çŠ¶æ€
    if UNIFIED_MANAGER_AVAILABLE:
        try:
            status = get_model_status()
            print(f"ğŸ¤– å¯ç”¨æ¨¡å‹: {len(status)}")
            for alias, info in status.items():
                if info["available"]:
                    print(f"  âœ… {alias}: {info['provider']}-{info['model_id']}")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è·å–æ¨¡å‹çŠ¶æ€: {e}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = UnifiedChromaDBTrainer(args.traindb)
    
    # å¼€å§‹è®­ç»ƒ
    result = await trainer.train_from_folder(args.force_retrain)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_file = f"training_result_{timestamp}.json"
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“„ è®­ç»ƒç»“æœå·²ä¿å­˜: {result_file}")
    
    if result.get("success"):
        print("âœ… è®­ç»ƒæˆåŠŸå®Œæˆ!")
        return 0
    else:
        print("âŒ è®­ç»ƒå¤±è´¥!")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)