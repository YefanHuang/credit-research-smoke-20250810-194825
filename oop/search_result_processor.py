#!/usr/bin/env python3
"""
ğŸ” æœç´¢ç»“æœå¤„ç†å™¨
å¤„ç†Perplexityæœç´¢ç»“æœï¼šæ™ºèƒ½åˆ‡åˆ†ã€å‘é‡åŒ–åŒ¹é…ã€æ™ºèƒ½è¿‡æ»¤
"""

import os
import sys
import asyncio
import json
from typing import List, Dict, Any, Tuple
from datetime import datetime

# å°è¯•å¯¼å…¥ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨
try:
    from model_manager import call_embedding, call_llm, get_model_status
    UNIFIED_MANAGER_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨ä¸å¯ç”¨: {e}")
    UNIFIED_MANAGER_AVAILABLE = False

class SearchResultProcessor:
    """æœç´¢ç»“æœå¤„ç†å™¨ - é«˜çº§æ–‡æœ¬å¤„ç†å’Œå‘é‡åŒ¹é…"""
    
    def __init__(self):
        self.processor_name = "SearchResultProcessor"
        self.version = "1.0.0"
        
    def _clean_content_for_processing(self, content: str) -> str:
        """
        æ¸…ç†å†…å®¹ç”¨äºæ™ºèƒ½å¤„ç†ï¼ˆå»é™¤é“¾æ¥ç­‰ï¼‰
        
        Args:
            content: åŸå§‹å†…å®¹
            
        Returns:
            æ¸…ç†åçš„çº¯æ–‡æœ¬å†…å®¹
        """
        import re
        
        if not content:
            return ""
        
        # å»é™¤URLé“¾æ¥
        content = re.sub(r'https?://[^\s\]ï¼‰)]+', '', content)
        
        # å»é™¤å¼•ç”¨æ ‡è®° [1], [2], (1), (2) ç­‰
        content = re.sub(r'[\[\(]\d+[\]\)]', '', content)
        
        # å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content)
        
        # å»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½
        content = content.strip()
        
        return content
    
    def _is_primarily_english(self, text: str) -> bool:
        """
        æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸»è¦ä¸ºè‹±æ–‡
        
        Args:
            text: è¦æ£€æµ‹çš„æ–‡æœ¬
            
        Returns:
            Trueè¡¨ç¤ºä¸»è¦ä¸ºè‹±æ–‡ï¼ŒFalseè¡¨ç¤ºä¸»è¦ä¸ºä¸­æ–‡æˆ–å…¶ä»–è¯­è¨€
        """
        if not text:
            return False
            
        # è®¡ç®—è‹±æ–‡å­—æ¯å’Œä¸­æ–‡å­—ç¬¦çš„æ¯”ä¾‹
        english_chars = sum(1 for c in text if c.isalpha() and ord(c) < 128)
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        total_meaningful_chars = english_chars + chinese_chars
        
        if total_meaningful_chars == 0:
            return False
            
        english_ratio = english_chars / total_meaningful_chars
        return english_ratio > 0.7  # å¦‚æœè‹±æ–‡å­—ç¬¦å æ¯”è¶…è¿‡70%ï¼Œè®¤ä¸ºæ˜¯è‹±æ–‡
    
    async def intelligent_segmentation(self, content: str, max_chunk_size: int = 600) -> List[str]:
        """
        æ™ºèƒ½åˆ‡åˆ†Perplexityæ¦‚æ‹¬ç»“æœ
        
        Args:
            content: Perplexity APIè¿”å›çš„æ¦‚æ‹¬å†…å®¹
            max_chunk_size: æ¯ä¸ªchunkçš„æœ€å¤§å­—ç¬¦æ•°
            
        Returns:
            åˆ‡åˆ†åçš„è¯­ä¹‰å®Œæ•´å—åˆ—è¡¨
        """
        if not content or not content.strip():
            return []
        
        # æ¸…ç†å†…å®¹ï¼ˆå»é™¤é“¾æ¥ç­‰ï¼‰
        clean_content = self._clean_content_for_processing(content)
        if not clean_content:
            return []
            
        print(f"ğŸ§  å¼€å§‹æ™ºèƒ½åˆ‡åˆ†: {len(content)} å­—ç¬¦ â†’ {len(clean_content)} å­—ç¬¦ (å·²æ¸…ç†)")
        
        # ä½¿ç”¨æ¸…ç†åçš„å†…å®¹è¿›è¡Œåˆ‡åˆ†
        content_to_process = clean_content
        
        # æ£€æµ‹å†…å®¹è¯­è¨€
        is_english = self._is_primarily_english(content_to_process)
        
        if UNIFIED_MANAGER_AVAILABLE and call_llm:
            try:
                if is_english:
                    prompt = f"""Please intelligently segment the following Perplexity search summary into semantically complete paragraphs.

Content:
{content_to_process}

Requirements:
1. Maintain semantic integrity, do not truncate sentences
2. Keep each segment under {max_chunk_size} characters
3. Preserve credit research technical terms and key information
4. Segment by logical topics (e.g., policy analysis, technology development, market data)
5. Remove reference numbers at the end of sentences or paragraphs (e.g., remove "1" from "investors.1" or "2" from "risk.2")
6. Clean up text transitions (e.g., "investors.1In this paper" should become "investors. In this paper")
7. Return format: one paragraph per line, separated by "---"

Example output format:
First paragraph content...
---
Second paragraph content...
---
Third paragraph content...

Ensure output is in English.
"""
                else:
                    prompt = f"""Please intelligently segment the following Perplexity search summary into semantically complete paragraphs.

Content:
{content_to_process}

Requirements:
1. Maintain semantic integrity, do not truncate sentences
2. Keep each segment under {max_chunk_size} characters
3. Preserve credit research technical terms and key information
4. Segment by logical topics (e.g., policy analysis, technology development, market data)
5. Remove reference numbers at the end of sentences or paragraphs (e.g., remove "1" from "investors.1" or "2" from "risk.2")
6. Clean up text transitions (e.g., "investors.1In this paper" should become "investors. In this paper")
7. Return format: one paragraph per line, separated by "---"

Example output format:
First paragraph content...
---
Second paragraph content...
---
Third paragraph content...

Ensure output is in English.
"""
                
                result = await call_llm(
                    prompt,
                    model_alias="llm",
                    max_tokens=2000,
                    temperature=0.1  # ä½æ¸©åº¦ç¡®ä¿ç¨³å®šè¾“å‡º
                )
                
                if result.get('success'):
                    response_text = result.get('content', '')
                    chunks = [chunk.strip() for chunk in response_text.split("---") if chunk.strip()]
                    
                    # éªŒè¯åˆ‡åˆ†è´¨é‡
                    if chunks and len(chunks) > 0:
                        total_chars = sum(len(chunk) for chunk in chunks)
                        lang_indicator = "EN" if is_english else "CN"
                        print(f"âœ… LLMæ™ºèƒ½åˆ‡åˆ†({lang_indicator})å®Œæˆ: {len(clean_content)} å­—ç¬¦ â†’ {len(chunks)} ä¸ªè¯­ä¹‰å—")
                        print(f"   ğŸ“Š å­—ç¬¦ä¿ç•™ç‡: {(total_chars/len(clean_content)*100):.1f}%")
                        
                        # è¿‡æ»¤è¿‡é•¿çš„chunk
                        filtered_chunks = []
                        for chunk in chunks:
                            if len(chunk) <= max_chunk_size:
                                filtered_chunks.append(chunk)
                            else:
                                # å¯¹è¿‡é•¿chunkè¿›è¡Œç®€å•åˆ‡åˆ†
                                sub_chunks = self._simple_split(chunk, max_chunk_size)
                                filtered_chunks.extend(sub_chunks)
                        
                        return filtered_chunks
                    else:
                        print("âš ï¸ LLMåˆ‡åˆ†è¿”å›ç©ºç»“æœï¼Œä½¿ç”¨ç®€å•åˆ‡åˆ†")
                        
                else:
                    print(f"âš ï¸ LLMåˆ‡åˆ†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    
            except Exception as e:
                print(f"âš ï¸ LLMåˆ‡åˆ†å¼‚å¸¸: {e}")
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•æ™ºèƒ½åˆ‡åˆ†
        return self._simple_split(content_to_process, max_chunk_size)
    
    def _simple_split(self, text: str, max_size: int) -> List[str]:
        """ç®€å•ä½†æ™ºèƒ½çš„æ–‡æœ¬åˆ‡åˆ†ï¼ˆæŒ‰å¥å­è¾¹ç•Œï¼Œæ”¯æŒä¸­è‹±æ–‡ï¼‰"""
        chunks = []
        
        # æ£€æµ‹è¯­è¨€å¹¶é€‰æ‹©åˆé€‚çš„åˆ†å¥ç­–ç•¥
        is_english = self._is_primarily_english(text)
        
        if is_english:
            # è‹±æ–‡åˆ†å¥ï¼šæŒ‰å¥å·ã€æ„Ÿå¹å·ã€é—®å·åˆ†å¥
            import re
            sentences = re.split(r'(?<=[.!?])\s+', text)
        else:
            # ä¸­æ–‡åˆ†å¥ï¼šæŒ‰ä¸­æ–‡æ ‡ç‚¹åˆ†å¥
            sentences = text.replace('ã€‚', 'ã€‚\n').replace('ï¼', 'ï¼\n').replace('ï¼Ÿ', 'ï¼Ÿ\n').split('\n')
        
        current_chunk = ""
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # æ·»åŠ é€‚å½“çš„åˆ†éš”ç¬¦
            separator = " " if is_english else ""
            test_chunk = current_chunk + separator + sentence if current_chunk else sentence
            
            if len(test_chunk) <= max_size:
                current_chunk = test_chunk
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        lang_indicator = "EN" if is_english else "CN"
        print(f"âœ‚ï¸ ç®€å•åˆ‡åˆ†({lang_indicator}): {len(text)} å­—ç¬¦ â†’ {len(chunks)} ä¸ªå—")
        return chunks
    
    async def intelligent_summarization(self, content: str, max_length: int = 300, topic: str = "") -> str:
        """
        æ™ºèƒ½æ¦‚æ‹¬æ–‡æœ¬å†…å®¹
        
        Args:
            content: åŸå§‹å†…å®¹
            max_length: æ¦‚æ‹¬æœ€å¤§é•¿åº¦
            topic: ç›¸å…³ä¸»é¢˜ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ¦‚æ‹¬åçš„æ–‡æœ¬
        """
        if not content:
            return ""
        
        # æ¸…ç†å†…å®¹ï¼ˆå»é™¤é“¾æ¥ç­‰ï¼‰
        clean_content = self._clean_content_for_processing(content)
        if not clean_content:
            return content  # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œè¿”å›åŸå†…å®¹
        
        if len(clean_content) <= max_length:
            return clean_content
            
        # æ£€æµ‹å†…å®¹è¯­è¨€
        is_english = self._is_primarily_english(clean_content)
        
        if UNIFIED_MANAGER_AVAILABLE and call_llm:
            try:
                if is_english:
                    topic_context = f"about {topic}" if topic else "credit research"
                    
                    prompt = f"""Please summarize the following {topic_context} content into a precise summary of no more than {max_length} characters:

{clean_content}

Summary requirements:
1. Retain core data and key conclusions
2. Highlight credit industry characteristics and professional terminology
3. Maintain clear logical structure
4. Keep character count under {max_length}
5. Concise language but high information density

Please return the summary directly without additional explanation.
"""
                else:
                    topic_context = f"on {topic}" if topic else "on credit research"
                    
                    prompt = f"""Please summarize the following {topic_context} content into a precise summary of no more than {max_length} characters:

{clean_content}

Summary requirements:
1. Retain core data and key conclusions
2. Highlight credit industry characteristics and professional terminology
3. Maintain clear logical structure
4. Keep character count under {max_length}
5. Concise language but high information density

Please return the summary directly without additional explanation. Ensure output is in English.
"""
                
                result = await call_llm(
                    prompt,
                    model_alias="llm",
                    max_tokens=max_length + 100,
                    temperature=0.2
                )
                
                if result.get('success'):
                    summary = result.get('content', '').strip()
                    if summary and len(summary) <= max_length + 50:  # å…è®¸å°å¹…è¶…å‡º
                        lang_indicator = "EN" if is_english else "CN"  
                        print(f"ğŸ¯ æ™ºèƒ½æ¦‚æ‹¬({lang_indicator}): {len(clean_content)} â†’ {len(summary)} å­—ç¬¦")
                        return summary
                        
            except Exception as e:
                print(f"âš ï¸ æ™ºèƒ½æ¦‚æ‹¬å¤±è´¥: {e}")
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šæ™ºèƒ½æˆªæ–­ï¼ˆä¿æŒå¥å­å®Œæ•´ï¼‰
        if len(clean_content) <= max_length:
            return clean_content
            
        truncated = clean_content[:max_length]
        
        # æ ¹æ®è¯­è¨€é€‰æ‹©åˆé€‚çš„å¥å­ç»“æŸç¬¦
        if is_english:
            # è‹±æ–‡å¥å­ç»“æŸç¬¦
            last_period = max(truncated.rfind('.'), truncated.rfind('!'), truncated.rfind('?'))
        else:
            # ä¸­æ–‡å¥å­ç»“æŸç¬¦
            last_period = max(truncated.rfind('ã€‚'), truncated.rfind('ï¼'), truncated.rfind('ï¼Ÿ'))
        
        if last_period > max_length * 0.7:  # å¦‚æœå¥å·ä½ç½®åˆç†
            return truncated[:last_period + 1]
        else:
            return truncated + "..."
    
    async def vectorize_and_match_chromadb(self, 
                                         search_chunks: List[str], 
                                         chromadb_collection=None,
                                         similarity_threshold: float = 0.7) -> Dict[str, Any]:
        """
        å‘é‡åŒ–æœç´¢å†…å®¹å¹¶ä¸ChromaDBåŒ¹é…
        
        Args:
            search_chunks: åˆ‡åˆ†åçš„æœç´¢å†…å®¹å—
            chromadb_collection: ChromaDBé›†åˆï¼ˆå¯é€‰ï¼‰
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            åŒ¹é…ç»“æœå’Œå‘é‡ä¿¡æ¯
        """
        if not search_chunks:
            return {"vectors": [], "matches": [], "error": "æ²¡æœ‰å†…å®¹å¯å‘é‡åŒ–"}
            
        print(f"ğŸ§® å¼€å§‹å‘é‡åŒ–: {len(search_chunks)} ä¸ªæ–‡æœ¬å—")
        
        if not UNIFIED_MANAGER_AVAILABLE:
            return {"vectors": [], "matches": [], "error": "ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨ä¸å¯ç”¨"}
        
        try:
            # 1. å‘é‡åŒ–æœç´¢å†…å®¹ (ä½¿ç”¨ä¸ChromaDBè®­ç»ƒç›¸åŒçš„embeddingæ¨¡å‹)
            result = await call_embedding(
                texts=search_chunks,
                model_alias="embedding"  # ç¡®ä¿ä¸€è‡´æ€§
            )
            
            if not result.get('success'):
                return {"vectors": [], "matches": [], "error": f"å‘é‡åŒ–å¤±è´¥: {result.get('error')}"}
            
            vectors = result.get('embeddings', [])
            if not vectors:
                return {"vectors": [], "matches": [], "error": "å‘é‡åŒ–è¿”å›ç©ºç»“æœ"}
            
            print(f"âœ… å‘é‡åŒ–å®Œæˆ: {len(vectors)} ä¸ªå‘é‡")
            
            # 2. ä¸ChromaDBåŒ¹é…ï¼ˆå¦‚æœæä¾›äº†collectionï¼‰
            matches = []
            if chromadb_collection:
                try:
                    print("ğŸ” å¼€å§‹ChromaDBç›¸ä¼¼åº¦åŒ¹é…...")
                    
                    # æ‰§è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢
                    query_results = chromadb_collection.query(
                        query_embeddings=vectors,
                        n_results=min(5, len(vectors)),  # æ¯ä¸ªå‘é‡æœ€å¤šè¿”å›5ä¸ªåŒ¹é…
                        include=['metadatas', 'documents', 'distances']
                    )
                    
                    # å¤„ç†åŒ¹é…ç»“æœ
                    for i, (distances, documents, metadatas) in enumerate(zip(
                        query_results.get('distances', []),
                        query_results.get('documents', []), 
                        query_results.get('metadatas', [])
                    )):
                        chunk_matches = []
                        for j, (distance, doc, meta) in enumerate(zip(distances, documents, metadatas)):
                            similarity = 1 - distance  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                            if similarity >= similarity_threshold:
                                chunk_matches.append({
                                    "similarity": similarity,
                                    "document": doc,
                                    "metadata": meta,
                                    "distance": distance
                                })
                        
                        matches.append({
                            "chunk_index": i,
                            "chunk_text": search_chunks[i],
                            "matches": chunk_matches
                        })
                    
                    total_matches = sum(len(m['matches']) for m in matches)
                    print(f"ğŸ¯ åŒ¹é…å®Œæˆ: æ‰¾åˆ° {total_matches} ä¸ªé«˜ç›¸ä¼¼åº¦åŒ¹é…")
                    
                except Exception as e:
                    print(f"âš ï¸ ChromaDBåŒ¹é…å¤±è´¥: {e}")
                    matches = []
            
            return {
                "success": True,
                "vectors": vectors,
                "matches": matches,
                "chunks": search_chunks,
                "vector_count": len(vectors),
                "match_count": len(matches),
                "processing_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            error_msg = f"å‘é‡åŒ–å¤„ç†å¼‚å¸¸: {e}"
            print(f"âŒ {error_msg}")
            return {"vectors": [], "matches": [], "error": error_msg}
    
    async def intelligent_filtering(self, 
                                  search_results: List[Dict], 
                                  vector_matches: List[Dict],
                                  max_results: int = 5) -> List[Dict]:
        """
        åŸºäºå‘é‡åŒ¹é…ç»“æœè¿›è¡Œæ™ºèƒ½è¿‡æ»¤
        
        Args:
            search_results: åŸå§‹æœç´¢ç»“æœ
            vector_matches: å‘é‡åŒ¹é…ç»“æœ
            max_results: æœ€å¤§è¿”å›ç»“æœæ•°
            
        Returns:
            è¿‡æ»¤åçš„é«˜è´¨é‡ç»“æœ
        """
        print(f"ğŸ›ï¸ å¼€å§‹æ™ºèƒ½è¿‡æ»¤: {len(search_results)} ä¸ªæœç´¢ç»“æœ")
        
        if not vector_matches:
            # å¦‚æœæ²¡æœ‰å‘é‡åŒ¹é…ï¼Œä½¿ç”¨åŸºç¡€è¿‡æ»¤
            return self._basic_filtering(search_results, max_results)
        
        # è®¡ç®—æ¯ä¸ªæœç´¢ç»“æœçš„ç»¼åˆè¯„åˆ†
        enhanced_results = []
        
        for result in search_results:
            # åŸºç¡€è¯„åˆ†
            base_score = result.get('relevance_score', 0.5)
            
            # å‘é‡åŒ¹é…åŠ åˆ†
            vector_boost = 0.0
            matching_count = 0
            
            result_content = result.get('content', '')
            for match in vector_matches:
                chunk_text = match.get('chunk_text', '')
                # ç®€å•æ–‡æœ¬é‡å æ£€æŸ¥
                if self._text_overlap(result_content, chunk_text) > 0.3:
                    chunk_matches = match.get('matches', [])
                    if chunk_matches:
                        # ä½¿ç”¨æœ€é«˜ç›¸ä¼¼åº¦åŠ åˆ†
                        max_similarity = max(m.get('similarity', 0) for m in chunk_matches)
                        vector_boost += max_similarity * 0.3  # 30%æƒé‡
                        matching_count += len(chunk_matches)
            
            # ç»¼åˆè¯„åˆ†
            final_score = base_score + vector_boost
            
            enhanced_result = result.copy()
            enhanced_result.update({
                'final_score': final_score,
                'vector_boost': vector_boost,
                'matching_count': matching_count,
                'enhanced': True
            })
            
            enhanced_results.append(enhanced_result)
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        enhanced_results.sort(key=lambda x: x.get('final_score', 0), reverse=True)
        
        # è¿”å›topç»“æœ
        filtered_results = enhanced_results[:max_results]
        
        print(f"âœ¨ æ™ºèƒ½è¿‡æ»¤å®Œæˆ: {len(search_results)} â†’ {len(filtered_results)} ä¸ªé«˜è´¨é‡ç»“æœ")
        
        return filtered_results
    
    def _basic_filtering(self, search_results: List[Dict], max_results: int) -> List[Dict]:
        """åŸºç¡€è¿‡æ»¤ï¼ˆæ²¡æœ‰å‘é‡åŒ¹é…æ—¶ä½¿ç”¨ï¼‰"""
        # æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åº
        sorted_results = sorted(
            search_results, 
            key=lambda x: x.get('relevance_score', 0), 
            reverse=True
        )
        return sorted_results[:max_results]
    
    def _text_overlap(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„é‡å åº¦"""
        if not text1 or not text2:
            return 0.0
        
        # ç®€å•è¯æ±‡é‡å è®¡ç®—
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    async def process_search_results(self, 
                                   search_results: List[Dict],
                                   chromadb_collection=None,
                                   enable_summarization: bool = True,
                                   max_chunk_size: int = 600,
                                   max_summary_length: int = 300) -> Dict[str, Any]:
        """
        å®Œæ•´çš„æœç´¢ç»“æœå¤„ç†æµç¨‹
        
        Args:
            search_results: Perplexityæœç´¢ç»“æœ
            chromadb_collection: ChromaDBé›†åˆ
            enable_summarization: æ˜¯å¦å¯ç”¨æ™ºèƒ½æ¦‚æ‹¬
            max_chunk_size: åˆ‡åˆ†å—å¤§å°
            max_summary_length: æ¦‚æ‹¬æœ€å¤§é•¿åº¦
            
        Returns:
            å¤„ç†åçš„ç»“æœ
        """
        print(f"\nğŸš€ å¼€å§‹å®Œæ•´çš„æœç´¢ç»“æœå¤„ç†: {len(search_results)} ä¸ªç»“æœ")
        
        processing_results = {
            "original_count": len(search_results),
            "processed_results": [],
            "chunks": [],
            "vectors": [],
            "matches": [],
            "errors": []
        }
        
        # 1. å¯¹æ¯ä¸ªæœç´¢ç»“æœè¿›è¡Œå¤„ç†
        all_chunks = []
        result_chunk_mapping = []
        
        for i, result in enumerate(search_results):
            try:
                content = result.get('content', '')
                topic = result.get('topic', '')
                
                print(f"\nğŸ“„ å¤„ç†ç»“æœ {i+1}: {result.get('title', 'æ— æ ‡é¢˜')[:50]}...")
                
                # æ™ºèƒ½åˆ‡åˆ†
                chunks = await self.intelligent_segmentation(content, max_chunk_size)
                
                # æ™ºèƒ½æ¦‚æ‹¬ï¼ˆå¯é€‰ï¼‰
                summary = content
                if enable_summarization and content:
                    summary = await self.intelligent_summarization(content, max_summary_length, topic)
                
                # è®°å½•chunkæ˜ å°„
                chunk_start = len(all_chunks)
                all_chunks.extend(chunks)
                chunk_end = len(all_chunks)
                
                result_chunk_mapping.append({
                    "result_index": i,
                    "chunk_range": (chunk_start, chunk_end),
                    "chunk_count": len(chunks)
                })
                
                # æ›´æ–°ç»“æœ
                enhanced_result = result.copy()
                enhanced_result.update({
                    "processed_content": summary,
                    "chunks": chunks,
                    "chunk_count": len(chunks),
                    "processing_timestamp": datetime.now().isoformat()
                })
                
                processing_results["processed_results"].append(enhanced_result)
                
            except Exception as e:
                error_msg = f"å¤„ç†ç»“æœ {i+1} å¤±è´¥: {e}"
                print(f"âŒ {error_msg}")
                processing_results["errors"].append(error_msg)
        
        # 2. å‘é‡åŒ–å’ŒChromaDBåŒ¹é…
        if all_chunks:
            vector_result = await self.vectorize_and_match_chromadb(
                all_chunks, 
                chromadb_collection
            )
            
            processing_results.update({
                "chunks": all_chunks,
                "vectors": vector_result.get("vectors", []),
                "matches": vector_result.get("matches", []),
                "vector_success": vector_result.get("success", False)
            })
            
            # 3. æ™ºèƒ½è¿‡æ»¤
            if vector_result.get("success"):
                filtered_results = await self.intelligent_filtering(
                    processing_results["processed_results"],
                    vector_result.get("matches", [])
                )
                processing_results["filtered_results"] = filtered_results
        
        # å¤„ç†ç»Ÿè®¡
        processing_results.update({
            "total_chunks": len(all_chunks),
            "processing_time": datetime.now().isoformat(),
            "success": len(processing_results["errors"]) == 0
        })
        
        print(f"\nâœ… æœç´¢ç»“æœå¤„ç†å®Œæˆ:")
        print(f"   ğŸ“Š åŸå§‹ç»“æœ: {processing_results['original_count']}")
        print(f"   ğŸ“ å¤„ç†ç»“æœ: {len(processing_results['processed_results'])}")
        print(f"   âœ‚ï¸ ç”Ÿæˆå—æ•°: {processing_results['total_chunks']}")
        print(f"   ğŸ§® å‘é‡æ•°é‡: {len(processing_results['vectors'])}")
        print(f"   ğŸ¯ åŒ¹é…æ•°é‡: {len(processing_results['matches'])}")
        
        return processing_results

# ä¾¿åˆ©å‡½æ•°
async def process_perplexity_results(search_results: List[Dict], **kwargs) -> Dict[str, Any]:
    """ä¾¿åˆ©å‡½æ•°ï¼šå¤„ç†Perplexityæœç´¢ç»“æœ"""
    processor = SearchResultProcessor()
    return await processor.process_search_results(search_results, **kwargs)

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test_processor():
        processor = SearchResultProcessor()
        
        # æ¨¡æ‹Ÿæœç´¢ç»“æœ
        test_results = [
            {
                "title": "å¾ä¿¡è¡Œä¸šå‘å±•è¶‹åŠ¿åˆ†æ",
                "content": "å½“å‰å¾ä¿¡è¡Œä¸šæ­£åœ¨ç»å†æ•°å­—åŒ–è½¬å‹ã€‚äººå·¥æ™ºèƒ½å’Œå¤§æ•°æ®æŠ€æœ¯çš„åº”ç”¨ä½¿å¾—ä¿¡ç”¨è¯„ä¼°æ›´åŠ ç²¾å‡†ã€‚ç›‘ç®¡æœºæ„ä¹Ÿåœ¨å®Œå–„ç›¸å…³æ³•è§„ï¼Œç¡®ä¿æ•°æ®å®‰å…¨å’Œéšç§ä¿æŠ¤ã€‚å¸‚åœºç«äº‰åŠ å‰§ï¼Œä¼ ç»Ÿå¾ä¿¡æœºæ„é¢ä¸´æ–°å…´ç§‘æŠ€å…¬å¸çš„æŒ‘æˆ˜ã€‚",
                "topic": "å¾ä¿¡å‘å±•",
                "relevance_score": 0.9
            }
        ]
        
        result = await processor.process_search_results(test_results)
        print(f"\nğŸ§ª æµ‹è¯•ç»“æœ: {json.dumps(result, ensure_ascii=False, indent=2)}")
    
    asyncio.run(test_processor())