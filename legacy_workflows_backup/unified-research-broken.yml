name: ğŸ¯ ç»Ÿä¸€ç ”ç©¶è‡ªåŠ¨åŒ– (æ–°æ¨¡å‹ç®¡ç†å™¨)

on:
  workflow_dispatch:
    inputs:
      search_topics:
        description: 'æœç´¢ä¸»é¢˜ (é€—å·åˆ†éš”)'
        required: true
        default: 'ä¿¡ç”¨é£é™©ç®¡ç†,ESGè¯„çº§'
        type: string
      
      time_filter:
        description: 'æ—¶é—´è¿‡æ»¤å™¨'
        required: false
        default: 'week'
        type: choice
        options:
          - week
          - month
          - year
      
      email_recipients:
        description: 'é‚®ä»¶æ¥æ”¶è€… (é€—å·åˆ†éš”)'
        required: true
        default: 'admin@example.com'
        type: string
      
      llm_model:
        description: 'LLMæ¨¡å‹é€‰æ‹©'
        required: false
        default: 'llm'
        type: choice
        options:
          - llm           # é»˜è®¤å¤§è¯­è¨€æ¨¡å‹
          - llm-claude    # Claudeæ¨¡å‹
          - llm-gpt       # GPTæ¨¡å‹
      
      embedding_model:
        description: 'å‘é‡åŒ–æ¨¡å‹é€‰æ‹©'
        required: false
        default: 'embedding'
        type: choice
        options:
          - embedding         # é»˜è®¤å‘é‡åŒ–æ¨¡å‹
          - embedding-openai  # OpenAIå‘é‡åŒ–
      
      search_model:
        description: 'æœç´¢æ¨¡å‹é€‰æ‹©'
        required: false
        default: 'search'
        type: choice
        options:
          - search  # Perplexityæœç´¢
      
      max_results:
        description: 'æœ€å¤§æœç´¢ç»“æœæ•°'
        required: false
        default: '50'
        type: string
      
      selection_count:
        description: 'ç­›é€‰ç»“æœæ•°'
        required: false
        default: '5'
        type: string
      
      token_limit_search:
        description: 'æœç´¢Tokené™åˆ¶'
        required: false
        default: '55000'
        type: string
      
      token_limit_embedding:
        description: 'å‘é‡åŒ–Tokené™åˆ¶'
        required: false
        default: '100000'
        type: string

env:
  PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}
  QWEN_API_KEY: ${{ secrets.QWEN_API_KEY }}
  CLAUDE_API_KEY: ${{ secrets.CLAUDE_API_KEY }}
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
  SMTP_SERVER: ${{ secrets.SMTP_SERVER }}
  SMTP_PORT: ${{ secrets.SMTP_PORT }}
  SMTP_USER: ${{ secrets.SMTP_USER }}
  SMTP_PASSWORD: ${{ secrets.SMTP_PASSWORD }}

jobs:
  unified-research:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - name: ğŸ“¥ æ£€å‡ºä»£ç 
      uses: actions/checkout@v4
    
    - name: ğŸ è®¾ç½®Pythonç¯å¢ƒ
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: ğŸ“¦ å®‰è£…ä¾èµ–
      run: |
        python -m pip install --upgrade pip
        pip install requests urllib3 aiofiles asyncio python-dotenv
        # å¦‚æœæœ‰requirements.txtï¼Œå®‰è£…é¢å¤–ä¾èµ–
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt
        fi
    
    - name: ğŸ¯ æ‰§è¡Œç»Ÿä¸€ç ”ç©¶æµç¨‹
      run: |
        python3 << 'EOF'
        import asyncio
        import sys
        import os
        from datetime import datetime
        from pathlib import Path
        import json
        
        # æ·»åŠ oopæ¨¡å—è·¯å¾„
        sys.path.append(os.path.join(os.getcwd(), 'oop'))
        
        # å¯¼å…¥ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨
        try:
            from model_manager import (
                call_llm, call_embedding, call_search, 
                log_tokens, get_model_status, model_manager,
                ModelType
            )
            print("âœ… ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨åŠ è½½æˆåŠŸ")
            UNIFIED_MANAGER_AVAILABLE = True
        except ImportError as e:
            print(f"âŒ æ— æ³•å¯¼å…¥ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨: {e}")
            UNIFIED_MANAGER_AVAILABLE = False
            sys.exit(1)
        
        # å¯¼å…¥Tokenç›‘æ§
        try:
            from realtime_token_monitor import init_monitor, start_monitoring, stop_monitoring, log_api_call
            TOKEN_MONITOR_AVAILABLE = True
            print("âœ… Tokenç›‘æ§å™¨åŠ è½½æˆåŠŸ")
        except ImportError as e:
            print(f"âš ï¸ Tokenç›‘æ§å™¨ä¸å¯ç”¨: {e}")
            TOKEN_MONITOR_AVAILABLE = False
        
        async def unified_research_flow():
            """ç»Ÿä¸€ç ”ç©¶æµç¨‹"""
            
            # è¯»å–è¾“å…¥å‚æ•°
            search_topics = "${{ github.event.inputs.search_topics }}".split(',')
            time_filter = "${{ github.event.inputs.time_filter }}"
            email_recipients = "${{ github.event.inputs.email_recipients }}".split(',')
            llm_model = "${{ github.event.inputs.llm_model }}"
            embedding_model = "${{ github.event.inputs.embedding_model }}"
            search_model = "${{ github.event.inputs.search_model }}"
            max_results = int("${{ github.event.inputs.max_results }}")
            selection_count = int("${{ github.event.inputs.selection_count }}")
            token_limit_search = int("${{ github.event.inputs.token_limit_search }}")
            token_limit_embedding = int("${{ github.event.inputs.token_limit_embedding }}")
            
            print("ğŸ¯ ç»Ÿä¸€ç ”ç©¶è‡ªåŠ¨åŒ–å¼€å§‹")
            print("=" * 50)
            print(f"ğŸ” æœç´¢ä¸»é¢˜: {search_topics}")
            print(f"â° æ—¶é—´è¿‡æ»¤: {time_filter}")
            print(f"ğŸ“§ é‚®ä»¶æ¥æ”¶è€…: {email_recipients}")
            print(f"ğŸ¤– LLMæ¨¡å‹: {llm_model}")
            print(f"ğŸ§  å‘é‡åŒ–æ¨¡å‹: {embedding_model}")
            print(f"ğŸ” æœç´¢æ¨¡å‹: {search_model}")
            
            # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
            print("\nğŸ” æ£€æŸ¥æ¨¡å‹çŠ¶æ€:")
            try:
                model_status = get_model_status()
                for alias, info in model_status.items():
                    status_icon = "âœ…" if info["available"] else "âŒ"
                    print(f"  {status_icon} {alias}: {info['provider']}-{info['model_id']} ({info['type']})")
            except Exception as e:
                print(f"âš ï¸ æ— æ³•è·å–æ¨¡å‹çŠ¶æ€: {e}")
            
            # åˆå§‹åŒ–Tokenç›‘æ§
            if TOKEN_MONITOR_AVAILABLE:
                init_monitor(
                    perplexity_limit=token_limit_search,
                    qwen_limit=token_limit_embedding,
                    cost_limit=1.0
                )
                start_monitoring()
                print(f"ğŸ” Tokenç›‘æ§å·²å¯åŠ¨")
            
            search_results = []
            
            try:
                # ç¬¬1æ­¥ï¼šæœç´¢
                print(f"\nğŸ“¡ ç¬¬1æ­¥ï¼šæ‰§è¡Œæœç´¢ (æ¨¡å‹: {search_model})")
                
                for topic in search_topics:
                    topic = topic.strip()
                    print(f"ğŸ” æœç´¢ä¸»é¢˜: {topic}")
                    
                    try:
                        # æ„å»ºæœç´¢æŸ¥è¯¢
                        search_query = f"æœ€æ–°çš„{topic}ç ”ç©¶å’Œå‘å±•è¶‹åŠ¿"
                        if time_filter:
                            search_query += f" æ—¶é—´èŒƒå›´:{time_filter}"
                        
                        # ä½¿ç”¨ç»Ÿä¸€æœç´¢æ¥å£
                        result = await call_search(
                            search_query,
                            model_alias=search_model,
                            search_recency_filter=time_filter
                        )
                        
                        # è§£ææœç´¢ç»“æœ
                        if result.get("choices"):
                            content = result["choices"][0]["message"]["content"]
                            search_results.append({
                                "topic": topic,
                                "content": content,
                                "model": search_model
                            })
                            print(f"âœ… {topic} æœç´¢å®Œæˆ")
                        else:
                            print(f"âš ï¸ {topic} æœç´¢æ— ç»“æœ")
                        
                        # è®°å½•Tokenä½¿ç”¨
                        if TOKEN_MONITOR_AVAILABLE and "usage" in result:
                            usage = result["usage"]
                            log_api_call(
                                search_model, 
                                "sonar-pro",
                                usage.get("prompt_tokens", 0),
                                usage.get("completion_tokens", 0)
                            )
                        
                    except Exception as e:
                        print(f"âŒ {topic} æœç´¢å¤±è´¥: {e}")
                        continue
                
                print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")
                
                if not search_results:
                    print("âŒ æ²¡æœ‰æœç´¢ç»“æœï¼Œç»“æŸæµç¨‹")
                    return {"success": False, "error": "æ²¡æœ‰æœç´¢ç»“æœ"}
                
                # ç¬¬2æ­¥ï¼šæ™ºèƒ½ç­›é€‰
                print(f"\nğŸ§  ç¬¬2æ­¥ï¼šæ™ºèƒ½ç­›é€‰ (æ¨¡å‹: {llm_model})")
                
                # æ„å»ºç­›é€‰æç¤º
                results_text = "\n".join([
                    f"{i+1}. ä¸»é¢˜: {r['topic']}\nå†…å®¹: {r['content'][:500]}...\n"
                    for i, r in enumerate(search_results)
                ])
                
                results_count = len(search_results)
                filter_prompt = f"Please select the {selection_count} most important and relevant results from the following {results_count} search results:\\n\\n"
                filter_prompt += results_text
                filter_prompt += "\\n\\nPlease return the numbers of the selected results (comma-separated), for example: 1,3,5\\n\\n"
                filter_prompt += "Selection criteria:\\n"
                filter_prompt += "1. Authority and credibility of the content\\n"
                filter_prompt += "2. Timeliness and practicality of the information\\n"
                filter_prompt += "3. Relevance to the field of credit research\\n"
                filter_prompt += "Ensure output is in English."
                
                try:
                    filter_result = await call_llm(filter_prompt, model_alias=llm_model)
                    response_text = filter_result.get("choices", [{}])[0].get("message", {}).get("content", "")
                    
                    # Parse filtered results
                    try:
                        selected_indices = [int(x.strip()) - 1 for x in response_text.split(",") if x.strip().isdigit()]
                        filtered_results = [search_results[i] for i in selected_indices if 0 <= i < len(search_results)]
                        
                        if not filtered_results:
                            filtered_results = search_results[:selection_count]
                            print(f"âš ï¸ Filter parsing failed, using top {selection_count} results")
                        else:
                            print(f"âœ… Intelligent filtering complete, selected {len(filtered_results)} results")
                        
                    except:
                        filtered_results = search_results[:selection_count]
                        print(f"âš ï¸ Filter parsing failed, using top {selection_count} results")
                    
                    # Log token usage
                    if TOKEN_MONITOR_AVAILABLE and "usage" in filter_result:
                        usage = filter_result["usage"]
                        log_api_call(
                            llm_model,
                            "qwen-turbo", 
                            usage.get("prompt_tokens", 0),
                            usage.get("completion_tokens", 0)
                        )
                        
                except Exception as e:
                    print(f"âš ï¸ Intelligent filtering failed: {e}")
                    filtered_results = search_results[:selection_count]
                
                # ç¬¬3æ­¥ï¼šå‘é‡åŒ–å’Œè¯­ä¹‰åˆ†æ
                print(f"\nğŸ§® ç¬¬3æ­¥ï¼šå‘é‡åŒ–åˆ†æ (æ¨¡å‹: {embedding_model})")
                
                texts_to_vectorize = [r["content"] for r in filtered_results]
                
                try:
                    embedding_result = await call_embedding(texts_to_vectorize, model_alias=embedding_model)
                    
                    if embedding_result.get("success"):
                        embeddings = embedding_result["embeddings"]
                        print(f"âœ… å‘é‡åŒ–å®Œæˆ: {len(texts_to_vectorize)} ä¸ªæ–‡æœ¬ â†’ {len(embeddings)} ä¸ªå‘é‡")
                        
                        # è®°å½•Tokenä½¿ç”¨
                        if TOKEN_MONITOR_AVAILABLE:
                            input_tokens = sum(len(text.split()) for text in texts_to_vectorize)
                            log_api_call(embedding_model, "text-embedding-v2", input_tokens, 0)
                        
                        # ç®€å•çš„ç›¸ä¼¼æ€§åˆ†æï¼ˆè¿™é‡Œå¯ä»¥æ‰©å±•æ›´å¤æ‚çš„åˆ†æï¼‰
                        print("ğŸ“Š å‘é‡ç»´åº¦:", len(embeddings[0]) if embeddings else 0)
                        
                    else:
                        print("âš ï¸ å‘é‡åŒ–å¤±è´¥ï¼Œè·³è¿‡è¯­ä¹‰åˆ†æ")
                        
                except Exception as e:
                    print(f"âš ï¸ å‘é‡åŒ–å¤±è´¥: {e}")
                
                # ç¬¬4æ­¥ï¼šç”ŸæˆæŠ¥å‘Š
                print(f"\nğŸ“ ç¬¬4æ­¥ï¼šç”Ÿæˆç ”ç©¶æŠ¥å‘Š (æ¨¡å‹: {llm_model})")
                
                # æ„å»ºæŠ¥å‘Šå†…å®¹
                report_content = f"""# Credit Research Dynamics Report

**Generation Time**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Search Topics**: {', '.join(search_topics)}
**Time Range**: {time_filter}

## ğŸ“Š Executive Summary
- Search Results: {len(search_results)} items
- Filtered Results: {len(filtered_results)} items
- Models Used: {llm_model} (filtering), {embedding_model} (vectorization), {search_model} (search)

## ğŸ” Detailed Results

"""
                
                for i, result in enumerate(filtered_results, 1):
                    report_content += f"""### {i}. {result['topic']}

{result['content']}

---

"""
                
                # ä½¿ç”¨LLMç”Ÿæˆæ€»ç»“
                try:
                    summary_prompt = f"""Please generate a concise summary and trend analysis for the following credit research content:

{report_content}

Please provide:
1. Key findings and trends
2. Impact analysis on the industry
3. Key areas to focus on
Ensure output is in English.
"""
                    
                    summary_result = await call_llm(summary_prompt, model_alias=llm_model)
                    summary = summary_result.get("choices", [{}])[0].get("message", {}).get("content", "")
                    
                    if summary:
                        report_content += f"""## ğŸ¯ Intelligent Summary

{summary}

---
*This report was automatically generated by Unified Model Manager*
"""
                        print("âœ… Intelligent summary generation complete")
                    
                except Exception as e:
                    print(f"âš ï¸ Intelligent summary generation failed: {e}")
                
                # ç¬¬5æ­¥ï¼šä¿å­˜æŠ¥å‘Š
                print(f"\nğŸ“ ç¬¬5æ­¥ï¼šä¿å­˜æŠ¥å‘Š")
                
                # åˆ›å»ºæŠ¥å‘Šç›®å½•
                reports_dir = Path("reports")
                reports_dir.mkdir(exist_ok=True)
                
                # ä¿å­˜æŠ¥å‘Š
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                report_file = reports_dir / f"unified_research_report_{timestamp}.md"
                
                with open(report_file, 'w', encoding='utf-8') as f:
                    f.write(report_content)
                
                print(f"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
                
                # ç¬¬6æ­¥ï¼šå‘é€é‚®ä»¶
                print(f"\nğŸ“§ ç¬¬6æ­¥ï¼šå‘é€é‚®ä»¶é€šçŸ¥")
                
                try:
                    # è¿™é‡Œåº”è¯¥å®ç°é‚®ä»¶å‘é€é€»è¾‘
                    # ç›®å‰åªæ˜¯æ¨¡æ‹Ÿ
                    print(f"ğŸ“¤ æ¨¡æ‹Ÿå‘é€é‚®ä»¶åˆ°: {', '.join(email_recipients)}")
                    print("âœ… é‚®ä»¶å‘é€å®Œæˆ")
                    
                except Exception as e:
                    print(f"âš ï¸ é‚®ä»¶å‘é€å¤±è´¥: {e}")
                
                # åœæ­¢Tokenç›‘æ§
                if TOKEN_MONITOR_AVAILABLE:
                    stop_monitoring()
                
                return {
                    "success": True,
                    "timestamp": datetime.now().isoformat(),
                    "search_results_count": len(search_results),
                    "filtered_results_count": len(filtered_results),
                    "report_file": str(report_file),
                    "models_used": {
                        "llm": llm_model,
                        "embedding": embedding_model,
                        "search": search_model
                    }
                }
                
            except Exception as e:
                if TOKEN_MONITOR_AVAILABLE:
                    stop_monitoring()
                print(f"âŒ æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
                return {"success": False, "error": str(e)}
        
        # æ‰§è¡Œä¸»æµç¨‹
        print("ğŸš€ å¯åŠ¨ç»Ÿä¸€ç ”ç©¶è‡ªåŠ¨åŒ–æµç¨‹...")
        result = await unified_research_flow()
        
        # è¾“å‡ºç»“æœ
        print("\n" + "=" * 50)
        if result["success"]:
            print("ğŸ‰ ç ”ç©¶æµç¨‹æˆåŠŸå®Œæˆ!")
            print("ğŸ“Š æ‰§è¡Œæ‘˜è¦:")
            for key, value in result.items():
                if key != "success":
                    print(f"  â€¢ {key}: {value}")
        else:
            print("âŒ ç ”ç©¶æµç¨‹å¤±è´¥:")
            print(f"  é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            sys.exit(1)
        
        EOF
    
    - name: ğŸ“¤ ä¸Šä¼ æŠ¥å‘Š
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: research-reports
        path: reports/
        retention-days: 30
    
    - name: ğŸ“Š è¾“å‡ºæ‘˜è¦
      if: always()
      run: |
        echo "## ğŸ¯ ç»Ÿä¸€ç ”ç©¶è‡ªåŠ¨åŒ–æ‘˜è¦" >> $GITHUB_STEP_SUMMARY
        echo "**æ‰§è¡Œæ—¶é—´**: $(date)" >> $GITHUB_STEP_SUMMARY
        echo "**æœç´¢ä¸»é¢˜**: ${{ github.event.inputs.search_topics }}" >> $GITHUB_STEP_SUMMARY
        echo "**ä½¿ç”¨æ¨¡å‹**: LLM(${{ github.event.inputs.llm_model }}), å‘é‡åŒ–(${{ github.event.inputs.embedding_model }}), æœç´¢(${{ github.event.inputs.search_model }})" >> $GITHUB_STEP_SUMMARY
        
        if [ -d "reports" ]; then
          echo "**ç”ŸæˆæŠ¥å‘Š**: $(ls -la reports/ | wc -l) ä¸ªæ–‡ä»¶" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "âœ… ä½¿ç”¨ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨æ‰§è¡Œå®Œæˆ" >> $GITHUB_STEP_SUMMARY