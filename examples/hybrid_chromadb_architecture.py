#!/usr/bin/env python3
"""
æ··åˆChromaDBæ¶æ„è®¾è®¡
æ”¯æŒæœ¬åœ°è®­ç»ƒ+æœåŠ¡å™¨ç«¯åŠ¨æ€å®Œå–„çš„å‘é‡æ•°æ®åº“ç®¡ç†
"""

import os
import json
import shutil
import tarfile
import hashlib
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass, asdict
import asyncio
import aiofiles
import git
from git.exc import GitCommandError

@dataclass
class ChromaDBMetadata:
    """ChromaDBå…ƒæ•°æ®"""
    version: str
    created_at: str
    model_provider: str
    model_version: str
    vector_dimension: int
    document_count: int
    total_chunks: int
    data_sources: List[str]
    consistency_hash: str
    last_updated: str
    size_mb: float

@dataclass
class DocumentChunk:
    """æ–‡æ¡£å—æ•°æ®ç»“æ„"""
    chunk_id: str
    content: str
    embedding: List[float]
    metadata: Dict
    source: str
    created_at: str
    quality_score: float

class LocalChromaDBManager:
    """æœ¬åœ°ChromaDBç®¡ç†å™¨"""
    
    def __init__(self, local_db_path: str, model_client):
        self.local_db_path = Path(local_db_path)
        self.model_client = model_client
        self.metadata_file = self.local_db_path / "metadata.json"
        self.chunks_dir = self.local_db_path / "chunks"
        self.embeddings_file = self.local_db_path / "embeddings.npy"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.local_db_path.mkdir(parents=True, exist_ok=True)
        self.chunks_dir.mkdir(exist_ok=True)
    
    async def initialize_from_local_documents(self, documents_path: str) -> ChromaDBMetadata:
        """ä»æœ¬åœ°å¾ä¿¡ç ”ç©¶æ–‡æ¡£åˆå§‹åŒ–æ•°æ®åº“"""
        print("ğŸ—ï¸ å¼€å§‹ä»æœ¬åœ°æ–‡æ¡£åˆå§‹åŒ–ChromaDB...")
        
        documents = await self._load_local_documents(documents_path)
        print(f"ğŸ“š æ‰¾åˆ° {len(documents)} ä¸ªæœ¬åœ°æ–‡æ¡£")
        
        # å¤„ç†æ–‡æ¡£ï¼šåˆ‡åˆ†ã€å‘é‡åŒ–
        all_chunks = []
        for doc_path, content in documents.items():
            chunks = await self._process_document(doc_path, content)
            all_chunks.extend(chunks)
        
        print(f"ğŸ“„ æ€»å…±ç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æ¡£å—")
        
        # ä¿å­˜åˆ°æœ¬åœ°æ•°æ®åº“
        await self._save_chunks_to_local_db(all_chunks)
        
        # ç”Ÿæˆå…ƒæ•°æ®
        metadata = ChromaDBMetadata(
            version=f"local_v{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            created_at=datetime.now().isoformat(),
            model_provider="qwen",
            model_version="v1.0",
            vector_dimension=1536,
            document_count=len(documents),
            total_chunks=len(all_chunks),
            data_sources=["local_documents"],
            consistency_hash=self._generate_consistency_hash(all_chunks),
            last_updated=datetime.now().isoformat(),
            size_mb=self._calculate_db_size()
        )
        
        await self._save_metadata(metadata)
        print(f"âœ… æœ¬åœ°ChromaDBåˆå§‹åŒ–å®Œæˆ: {metadata.version}")
        
        return metadata
    
    async def _load_local_documents(self, documents_path: str) -> Dict[str, str]:
        """åŠ è½½æœ¬åœ°æ–‡æ¡£"""
        documents = {}
        docs_path = Path(documents_path)
        
        # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        supported_formats = ['.txt', '.md', '.pdf', '.docx']
        
        for file_path in docs_path.rglob('*'):
            if file_path.suffix.lower() in supported_formats:
                try:
                    if file_path.suffix.lower() == '.txt':
                        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                            content = await f.read()
                    elif file_path.suffix.lower() == '.md':
                        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                            content = await f.read()
                    # å…¶ä»–æ ¼å¼çš„å¤„ç†é€»è¾‘...
                    else:
                        continue
                    
                    documents[str(file_path)] = content
                    
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
        
        return documents
    
    async def _process_document(self, doc_path: str, content: str) -> List[DocumentChunk]:
        """å¤„ç†å•ä¸ªæ–‡æ¡£ï¼šåˆ‡åˆ†å’Œå‘é‡åŒ–"""
        
        # æ™ºèƒ½æ–‡æœ¬åˆ‡åˆ†
        chunks_text = await self.model_client.intelligent_segmentation(
            text=content,
            max_chunk_size=800,
            domain="credit_research"
        )
        
        chunks = []
        for i, chunk_text in enumerate(chunks_text):
            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding_result = await self.model_client.create_embeddings([chunk_text])
            embedding = embedding_result["embeddings"][0]
            
            # è®¡ç®—è´¨é‡è¯„åˆ†
            quality_score = self._calculate_chunk_quality(chunk_text)
            
            chunk = DocumentChunk(
                chunk_id=f"{Path(doc_path).stem}_{i}",
                content=chunk_text,
                embedding=embedding,
                metadata={
                    "source_file": doc_path,
                    "chunk_index": i,
                    "length": len(chunk_text),
                    "domain": "credit_research"
                },
                source="local_document",
                created_at=datetime.now().isoformat(),
                quality_score=quality_score
            )
            
            chunks.append(chunk)
        
        return chunks
    
    def _calculate_chunk_quality(self, text: str) -> float:
        """è®¡ç®—æ–‡æ¡£å—è´¨é‡è¯„åˆ†"""
        score = 0.5  # åŸºç¡€åˆ†
        
        # é•¿åº¦è¯„åˆ†
        if 200 <= len(text) <= 1000:
            score += 0.2
        
        # å¾ä¿¡å…³é”®è¯è¯„åˆ†
        credit_keywords = ["å¾ä¿¡", "ä¿¡ç”¨", "é£é™©", "è¯„çº§", "åˆè§„", "ç›‘ç®¡"]
        keyword_count = sum(1 for keyword in credit_keywords if keyword in text)
        score += min(keyword_count * 0.05, 0.3)
        
        return min(score, 1.0)
    
    async def _save_chunks_to_local_db(self, chunks: List[DocumentChunk]):
        """ä¿å­˜æ–‡æ¡£å—åˆ°æœ¬åœ°æ•°æ®åº“"""
        import numpy as np
        
        # ä¿å­˜åµŒå…¥å‘é‡
        embeddings = np.array([chunk.embedding for chunk in chunks])
        np.save(self.embeddings_file, embeddings)
        
        # ä¿å­˜æ–‡æ¡£å—æ•°æ®
        for chunk in chunks:
            chunk_file = self.chunks_dir / f"{chunk.chunk_id}.json"
            chunk_data = asdict(chunk)
            chunk_data.pop('embedding')  # åµŒå…¥å‘é‡å•ç‹¬å­˜å‚¨
            
            async with aiofiles.open(chunk_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(chunk_data, ensure_ascii=False, indent=2))
    
    async def _save_metadata(self, metadata: ChromaDBMetadata):
        """ä¿å­˜å…ƒæ•°æ®"""
        async with aiofiles.open(self.metadata_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(asdict(metadata), ensure_ascii=False, indent=2))
    
    def _generate_consistency_hash(self, chunks: List[DocumentChunk]) -> str:
        """ç”Ÿæˆä¸€è‡´æ€§å“ˆå¸Œ"""
        content_hash = hashlib.md5()
        for chunk in sorted(chunks, key=lambda x: x.chunk_id):
            content_hash.update(chunk.content.encode('utf-8'))
        return content_hash.hexdigest()[:16]
    
    def _calculate_db_size(self) -> float:
        """è®¡ç®—æ•°æ®åº“å¤§å°ï¼ˆMBï¼‰"""
        total_size = 0
        for file_path in self.local_db_path.rglob('*'):
            if file_path.is_file():
                total_size += file_path.stat().st_size
        return total_size / (1024 * 1024)
    
    async def create_github_upload_package(self, output_path: str) -> str:
        """åˆ›å»ºGitHubä¸Šä¼ åŒ…"""
        print("ğŸ“¦ åˆ›å»ºGitHubä¸Šä¼ åŒ…...")
        
        # è¯»å–å…ƒæ•°æ®
        async with aiofiles.open(self.metadata_file, 'r', encoding='utf-8') as f:
            metadata_content = await f.read()
        metadata = json.loads(metadata_content)
        
        # åˆ›å»ºæ‰“åŒ…ç›®å½•
        package_name = f"chromadb_{metadata['version']}"
        package_path = Path(output_path) / package_name
        package_path.mkdir(parents=True, exist_ok=True)
        
        # å¤åˆ¶å¿…è¦æ–‡ä»¶
        shutil.copy2(self.metadata_file, package_path / "metadata.json")
        shutil.copy2(self.embeddings_file, package_path / "embeddings.npy")
        shutil.copytree(self.chunks_dir, package_path / "chunks")
        
        # åˆ›å»ºREADME
        readme_content = f"""# ChromaDBå‘é‡æ•°æ®åº“

## ğŸ“Š æ•°æ®åº“ä¿¡æ¯
- **ç‰ˆæœ¬**: {metadata['version']}
- **åˆ›å»ºæ—¶é—´**: {metadata['created_at']}
- **æ¨¡å‹**: {metadata['model_provider']} {metadata['model_version']}
- **å‘é‡ç»´åº¦**: {metadata['vector_dimension']}
- **æ–‡æ¡£æ•°é‡**: {metadata['document_count']}
- **æ–‡æ¡£å—æ•°é‡**: {metadata['total_chunks']}
- **æ•°æ®åº“å¤§å°**: {metadata['size_mb']:.2f} MB

## ğŸ”§ ä½¿ç”¨æ–¹æ³•
```python
from chromadb_manager import ServerChromaDBManager

# ä¸‹è½½å¹¶åŠ è½½æ•°æ®åº“
manager = ServerChromaDBManager()
await manager.load_from_github_release("v1.0")
```

## ğŸ“‹ æ•°æ®æ¥æº
{chr(10).join(f"- {source}" for source in metadata['data_sources'])}
"""
        
        async with aiofiles.open(package_path / "README.md", 'w', encoding='utf-8') as f:
            await f.write(readme_content)
        
        # åˆ›å»ºå‹ç¼©åŒ…
        archive_path = f"{package_path}.tar.gz"
        with tarfile.open(archive_path, "w:gz") as tar:
            tar.add(package_path, arcname=package_name)
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        shutil.rmtree(package_path)
        
        print(f"âœ… GitHubä¸Šä¼ åŒ…å·²åˆ›å»º: {archive_path}")
        return archive_path

class ServerChromaDBManager:
    """æœåŠ¡å™¨ç«¯ChromaDBç®¡ç†å™¨"""
    
    def __init__(self, server_db_path: str, model_client, github_repo: str):
        self.server_db_path = Path(server_db_path)
        self.model_client = model_client
        self.github_repo = github_repo
        self.metadata_file = self.server_db_path / "metadata.json"
        self.enhancement_log = self.server_db_path / "enhancement_log.json"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.server_db_path.mkdir(parents=True, exist_ok=True)
    
    async def load_from_github_release(self, version: str) -> bool:
        """ä»GitHub Releaseä¸‹è½½å¹¶åŠ è½½ChromaDB"""
        print(f"ğŸ“¥ ä»GitHubä¸‹è½½ChromaDBç‰ˆæœ¬: {version}")
        
        try:
            # ä¸‹è½½releaseæ–‡ä»¶
            download_path = await self._download_github_release(version)
            
            # è§£å‹åˆ°æœåŠ¡å™¨ç›®å½•
            await self._extract_chromadb(download_path)
            
            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            if await self._verify_database_integrity():
                print("âœ… ChromaDBåŠ è½½æˆåŠŸ")
                return True
            else:
                print("âŒ ChromaDBæ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âŒ åŠ è½½ChromaDBå¤±è´¥: {e}")
            return False
    
    async def enhance_with_search_results(self, search_results: List[Dict]) -> int:
        """ä½¿ç”¨æœç´¢ç»“æœå¢å¼ºChromaDB"""
        print(f"ğŸ”§ å¼€å§‹ä½¿ç”¨æœç´¢ç»“æœå¢å¼ºChromaDBï¼Œæ•°æ®é‡: {len(search_results)}")
        
        enhanced_count = 0
        enhancement_records = []
        
        for result in search_results:
            try:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if await self._is_duplicate_content(result["content"]):
                    print(f"â­ï¸ è·³è¿‡é‡å¤å†…å®¹: {result['title'][:50]}...")
                    continue
                
                # å¤„ç†æœç´¢ç»“æœ
                chunks = await self._process_search_result(result)
                
                # æ·»åŠ åˆ°æ•°æ®åº“
                for chunk in chunks:
                    if chunk.quality_score >= 0.7:  # åªæ·»åŠ é«˜è´¨é‡å†…å®¹
                        await self._add_chunk_to_db(chunk)
                        enhanced_count += 1
                
                # è®°å½•å¢å¼ºæ—¥å¿—
                enhancement_records.append({
                    "title": result["title"],
                    "url": result.get("url", ""),
                    "chunks_added": len([c for c in chunks if c.quality_score >= 0.7]),
                    "timestamp": datetime.now().isoformat(),
                    "quality_scores": [c.quality_score for c in chunks]
                })
                
            except Exception as e:
                print(f"âŒ å¤„ç†æœç´¢ç»“æœå¤±è´¥ {result.get('title', 'Unknown')}: {e}")
        
        # æ›´æ–°å…ƒæ•°æ®
        await self._update_metadata_after_enhancement(enhanced_count)
        
        # ä¿å­˜å¢å¼ºæ—¥å¿—
        await self._save_enhancement_log(enhancement_records)
        
        print(f"âœ… ChromaDBå¢å¼ºå®Œæˆï¼Œæ–°å¢ {enhanced_count} ä¸ªé«˜è´¨é‡æ–‡æ¡£å—")
        return enhanced_count
    
    async def _download_github_release(self, version: str) -> str:
        """ä¸‹è½½GitHub Release"""
        # è¿™é‡Œå®ç°GitHub Releaseä¸‹è½½é€»è¾‘
        # å¯ä»¥ä½¿ç”¨GitHub APIæˆ–gitå‘½ä»¤
        
        # æ¨¡æ‹Ÿä¸‹è½½è¿‡ç¨‹
        download_url = f"https://github.com/{self.github_repo}/releases/download/{version}/chromadb_{version}.tar.gz"
        download_path = f"/tmp/chromadb_{version}.tar.gz"
        
        print(f"ğŸ“¥ ä¸‹è½½URL: {download_url}")
        print(f"ğŸ’¾ ä¿å­˜è·¯å¾„: {download_path}")
        
        # å®é™…ä¸‹è½½é€»è¾‘...
        return download_path
    
    async def _extract_chromadb(self, archive_path: str):
        """è§£å‹ChromaDBæ–‡ä»¶"""
        with tarfile.open(archive_path, "r:gz") as tar:
            tar.extractall(self.server_db_path.parent)
    
    async def _verify_database_integrity(self) -> bool:
        """éªŒè¯æ•°æ®åº“å®Œæ•´æ€§"""
        required_files = ["metadata.json", "embeddings.npy", "chunks"]
        
        for file_name in required_files:
            file_path = self.server_db_path / file_name
            if not file_path.exists():
                print(f"âŒ ç¼ºå°‘å¿…éœ€æ–‡ä»¶: {file_name}")
                return False
        
        return True
    
    async def _is_duplicate_content(self, content: str) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤"""
        content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
        
        # æ£€æŸ¥ç°æœ‰chunksä¸­æ˜¯å¦æœ‰ç›¸åŒå†…å®¹
        chunks_dir = self.server_db_path / "chunks"
        if not chunks_dir.exists():
            return False
        
        for chunk_file in chunks_dir.glob("*.json"):
            try:
                async with aiofiles.open(chunk_file, 'r', encoding='utf-8') as f:
                    chunk_data = json.loads(await f.read())
                
                existing_hash = hashlib.md5(chunk_data["content"].encode('utf-8')).hexdigest()
                if content_hash == existing_hash:
                    return True
                    
            except Exception:
                continue
        
        return False
    
    async def _process_search_result(self, result: Dict) -> List[DocumentChunk]:
        """å¤„ç†æœç´¢ç»“æœä¸ºæ–‡æ¡£å—"""
        content = result["content"]
        
        # æ™ºèƒ½åˆ‡åˆ†
        chunks_text = await self.model_client.intelligent_segmentation(
            text=content,
            max_chunk_size=600,
            domain="credit_research"
        )
        
        chunks = []
        for i, chunk_text in enumerate(chunks_text):
            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding_result = await self.model_client.create_embeddings([chunk_text])
            embedding = embedding_result["embeddings"][0]
            
            # è®¡ç®—è´¨é‡è¯„åˆ†ï¼ˆæœç´¢ç»“æœé€šå¸¸è´¨é‡è¾ƒé«˜ï¼‰
            quality_score = self._calculate_search_result_quality(chunk_text, result)
            
            chunk = DocumentChunk(
                chunk_id=f"search_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{i}",
                content=chunk_text,
                embedding=embedding,
                metadata={
                    "source_title": result.get("title", ""),
                    "source_url": result.get("url", ""),
                    "search_score": result.get("score", 0.0),
                    "chunk_index": i,
                    "length": len(chunk_text),
                    "domain": "credit_research"
                },
                source="search_result",
                created_at=datetime.now().isoformat(),
                quality_score=quality_score
            )
            
            chunks.append(chunk)
        
        return chunks
    
    def _calculate_search_result_quality(self, text: str, result: Dict) -> float:
        """è®¡ç®—æœç´¢ç»“æœè´¨é‡è¯„åˆ†"""
        score = 0.6  # æœç´¢ç»“æœåŸºç¡€åˆ†è¾ƒé«˜
        
        # æ¥æºè¯„åˆ†
        url = result.get("url", "").lower()
        if any(domain in url for domain in ["pbc.gov.cn", "cbirc.gov.cn"]):
            score += 0.3  # ç›‘ç®¡æœºæ„
        elif any(domain in url for domain in ["bank", "finance"]):
            score += 0.2  # é‡‘èæœºæ„
        
        # å†…å®¹è´¨é‡è¯„åˆ†
        if len(text) >= 200:
            score += 0.1
        
        return min(score, 1.0)
    
    async def _add_chunk_to_db(self, chunk: DocumentChunk):
        """æ·»åŠ æ–‡æ¡£å—åˆ°æ•°æ®åº“"""
        import numpy as np
        
        chunks_dir = self.server_db_path / "chunks"
        chunks_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜æ–‡æ¡£å—æ•°æ®
        chunk_file = chunks_dir / f"{chunk.chunk_id}.json"
        chunk_data = asdict(chunk)
        chunk_data.pop('embedding')  # åµŒå…¥å‘é‡å•ç‹¬å¤„ç†
        
        async with aiofiles.open(chunk_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(chunk_data, ensure_ascii=False, indent=2))
        
        # æ›´æ–°åµŒå…¥å‘é‡æ–‡ä»¶
        embeddings_file = self.server_db_path / "embeddings.npy"
        if embeddings_file.exists():
            existing_embeddings = np.load(embeddings_file)
            new_embeddings = np.vstack([existing_embeddings, [chunk.embedding]])
        else:
            new_embeddings = np.array([chunk.embedding])
        
        np.save(embeddings_file, new_embeddings)
    
    async def _update_metadata_after_enhancement(self, enhanced_count: int):
        """å¢å¼ºåæ›´æ–°å…ƒæ•°æ®"""
        if self.metadata_file.exists():
            async with aiofiles.open(self.metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.loads(await f.read())
            
            metadata["total_chunks"] += enhanced_count
            metadata["last_updated"] = datetime.now().isoformat()
            metadata["size_mb"] = self._calculate_db_size()
            
            # æ·»åŠ åˆ°æ•°æ®æº
            if "search_enhancement" not in metadata["data_sources"]:
                metadata["data_sources"].append("search_enhancement")
            
            async with aiofiles.open(self.metadata_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(metadata, ensure_ascii=False, indent=2))
    
    async def _save_enhancement_log(self, records: List[Dict]):
        """ä¿å­˜å¢å¼ºæ—¥å¿—"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "enhanced_count": len(records),
            "records": records
        }
        
        if self.enhancement_log.exists():
            async with aiofiles.open(self.enhancement_log, 'r', encoding='utf-8') as f:
                existing_logs = json.loads(await f.read())
        else:
            existing_logs = []
        
        existing_logs.append(log_entry)
        
        async with aiofiles.open(self.enhancement_log, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(existing_logs, ensure_ascii=False, indent=2))
    
    def _calculate_db_size(self) -> float:
        """è®¡ç®—æ•°æ®åº“å¤§å°ï¼ˆMBï¼‰"""
        total_size = 0
        for file_path in self.server_db_path.rglob('*'):
            if file_path.is_file():
                total_size += file_path.stat().st_size
        return total_size / (1024 * 1024)

class HybridChromaDBOrchestrator:
    """æ··åˆChromaDBåè°ƒå™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.local_manager = LocalChromaDBManager(
            config["local_db_path"], 
            config["model_client"]
        )
        self.server_manager = ServerChromaDBManager(
            config["server_db_path"],
            config["model_client"], 
            config["github_repo"]
        )
    
    async def execute_hybrid_workflow(self, local_docs_path: str, search_results: List[Dict]) -> Dict:
        """æ‰§è¡Œæ··åˆå·¥ä½œæµ"""
        print("ğŸš€ å¼€å§‹æ··åˆChromaDBå·¥ä½œæµ...")
        
        workflow_results = {
            "local_initialization": None,
            "github_upload": None,
            "server_enhancement": None,
            "final_stats": None
        }
        
        try:
            # 1. æœ¬åœ°åˆå§‹åŒ–
            print("\nğŸ“š æ­¥éª¤1: æœ¬åœ°æ–‡æ¡£åˆå§‹åŒ–...")
            local_metadata = await self.local_manager.initialize_from_local_documents(local_docs_path)
            workflow_results["local_initialization"] = asdict(local_metadata)
            
            # 2. åˆ›å»ºGitHubä¸Šä¼ åŒ…
            print("\nğŸ“¤ æ­¥éª¤2: åˆ›å»ºGitHubä¸Šä¼ åŒ…...")
            upload_package = await self.local_manager.create_github_upload_package("./releases")
            workflow_results["github_upload"] = upload_package
            
            # 3. æœåŠ¡å™¨ç«¯åŠ è½½å’Œå¢å¼º
            print("\nğŸ”§ æ­¥éª¤3: æœåŠ¡å™¨ç«¯å¢å¼º...")
            # å‡è®¾å·²ä¸Šä¼ åˆ°GitHubå¹¶å¯ä»¥ä¸‹è½½
            await self.server_manager.load_from_github_release(local_metadata.version)
            enhanced_count = await self.server_manager.enhance_with_search_results(search_results)
            workflow_results["server_enhancement"] = enhanced_count
            
            # 4. ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡
            workflow_results["final_stats"] = {
                "initial_chunks": local_metadata.total_chunks,
                "enhanced_chunks": enhanced_count,
                "total_chunks": local_metadata.total_chunks + enhanced_count,
                "workflow_completed_at": datetime.now().isoformat()
            }
            
            print("\nâœ… æ··åˆChromaDBå·¥ä½œæµå®Œæˆï¼")
            return workflow_results
            
        except Exception as e:
            print(f"âŒ æ··åˆå·¥ä½œæµå¤±è´¥: {e}")
            workflow_results["error"] = str(e)
            return workflow_results

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    
    # é…ç½®
    config = {
        "local_db_path": "./local_chromadb",
        "server_db_path": "./server_chromadb", 
        "github_repo": "username/creditmonitor",
        "model_client": None  # å®é™…ä½¿ç”¨æ—¶éœ€è¦åˆå§‹åŒ–æ¨¡å‹å®¢æˆ·ç«¯
    }
    
    # æ¨¡æ‹Ÿæœç´¢ç»“æœ
    search_results = [
        {
            "title": "å¾ä¿¡é£é™©ç®¡ç†æœ€æ–°å‘å±•",
            "content": "å¾ä¿¡è¡Œä¸šåœ¨æ•°å­—åŒ–è½¬å‹ä¸­é¢ä¸´æ–°çš„æŒ‘æˆ˜...",
            "url": "https://example.com/article1",
            "score": 0.95
        },
        {
            "title": "ESGè¯„çº§ä½“ç³»å»ºè®¾æŒ‡å—", 
            "content": "ESGè¯„çº§ä½œä¸ºå¯æŒç»­å‘å±•çš„é‡è¦æŒ‡æ ‡...",
            "url": "https://example.com/article2",
            "score": 0.88
        }
    ]
    
    # æ‰§è¡Œæ··åˆå·¥ä½œæµ
    orchestrator = HybridChromaDBOrchestrator(config)
    results = await orchestrator.execute_hybrid_workflow(
        local_docs_path="./local_documents",
        search_results=search_results
    )
    
    print("\nğŸ“Š å·¥ä½œæµç»“æœ:")
    print(json.dumps(results, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    asyncio.run(main())
 
 
 