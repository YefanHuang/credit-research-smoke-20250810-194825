#!/usr/bin/env python3
"""
Perplexity APIé›†æˆ - ä½¿ç”¨å®˜æ–¹APIæ ¼å¼è¿›è¡Œæ—¶é—´è¿‡æ»¤æœç´¢
æ”¯æŒå¾ä¿¡ç ”ç©¶é¢†åŸŸçš„ç²¾ç¡®æ—¶é—´èŒƒå›´æœç´¢
"""

import asyncio
import aiohttp
import json
from typing import Dict, List, Optional
from datetime import datetime
import os

class PerplexityAPIClient:
    """Perplexityå®˜æ–¹APIå®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.perplexity.ai/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
    
    async def search_with_time_filter(self, topic: str, time_filter: str = "week") -> Dict:
        """
        ä½¿ç”¨Perplexityå®˜æ–¹APIè¿›è¡Œæ—¶é—´è¿‡æ»¤æœç´¢
        
        Args:
            topic: æœç´¢ä¸»é¢˜
            time_filter: æ—¶é—´è¿‡æ»¤å™¨ ("day", "week", "month", "year")
        
        Returns:
            æœç´¢ç»“æœå­—å…¸
        """
        
        # æ„å»ºé’ˆå¯¹å¾ä¿¡ç ”ç©¶çš„ä¸“ä¸šæœç´¢æç¤º
        system_prompt = """You are a professional financial research assistant specializing in credit research and risk management. 
        Provide accurate, up-to-date information about credit industry developments, regulatory changes, and technological innovations.
        Focus on authoritative sources like central banks, financial institutions, and academic research."""
        
        user_prompt = f"""
        æœç´¢å…³äº"{topic}"çš„æœ€æ–°å¾ä¿¡è¡Œä¸šç ”ç©¶å’Œåˆ†æï¼Œè¦æ±‚ï¼š

        ğŸ“Š å†…å®¹ç±»å‹ï¼š
        - å¾ä¿¡è¡Œä¸šç ”ç©¶æŠ¥å‘Šå’Œç™½çš®ä¹¦
        - æŠ€æœ¯åˆ›æ–°å’Œåº”ç”¨æ¡ˆä¾‹åˆ†æ  
        - ç›‘ç®¡æ”¿ç­–è§£è¯»å’Œåˆè§„æŒ‡å¯¼
        - å¸‚åœºè¶‹åŠ¿å’Œæ•°æ®æ´å¯Ÿ

        ğŸ›ï¸ æƒå¨æ¥æºä¼˜å…ˆï¼š
        - å¤®è¡Œã€é“¶ä¿ç›‘ä¼šç­‰ç›‘ç®¡æœºæ„
        - å¤§å‹é“¶è¡Œå’Œé‡‘èæœºæ„ç ”ç©¶éƒ¨é—¨
        - çŸ¥åå¾ä¿¡å…¬å¸ï¼ˆå¦‚èŠéº»ä¿¡ç”¨ã€è…¾è®¯å¾ä¿¡ç­‰ï¼‰
        - æƒå¨é‡‘èç§‘æŠ€ç ”ç©¶æœºæ„

        ğŸ¯ é‡ç‚¹å…³æ³¨ï¼š
        - æ•°æ®é©±åŠ¨çš„åˆ†æå’Œå®è¯ç ”ç©¶
        - æŠ€æœ¯å®ç°ç»†èŠ‚å’Œæ¶æ„è®¾è®¡
        - æ”¿ç­–å½±å“å’Œè¡Œä¸šå‘å±•è¶‹åŠ¿
        - é£é™©ç®¡ç†å’Œæ¨¡å‹åˆ›æ–°

        è¯·æä¾›è¯¦ç»†çš„å†…å®¹æ‘˜è¦ã€å…³é”®å‘ç°å’ŒåŸæ–‡é“¾æ¥ã€‚
        """

        # æ„å»ºAPIè¯·æ±‚å‚æ•°
        request_data = {
            "model": "llama-3.1-sonar-small-128k-online",
            "messages": [
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user",
                    "content": user_prompt
                }
            ],
            "search_recency_filter": time_filter,  # å®˜æ–¹APIæ—¶é—´è¿‡æ»¤å‚æ•°
            "return_citations": True,
            "return_images": False,
            "temperature": 0.2,
            "top_p": 0.9,
            "max_tokens": 4000,
            "stream": False
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.base_url,
                    headers=self.headers,
                    json=request_data,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    
                    if response.status == 200:
                        result = await response.json()
                        return self._process_search_result(result, topic, time_filter)
                    else:
                        error_text = await response.text()
                        raise Exception(f"Perplexity APIé”™è¯¯ {response.status}: {error_text}")
                        
        except Exception as e:
            print(f"âŒ Perplexity APIè°ƒç”¨å¤±è´¥: {e}")
            return self._create_error_response(str(e), topic, time_filter)
    
    def _process_search_result(self, api_result: Dict, topic: str, time_filter: str) -> Dict:
        """å¤„ç†APIè¿”å›ç»“æœ"""
        
        content = api_result.get("choices", [{}])[0].get("message", {}).get("content", "")
        citations = api_result.get("choices", [{}])[0].get("citations", [])
        
        # æå–ç»“æ„åŒ–ä¿¡æ¯
        processed_result = {
            "query": {
                "topic": topic,
                "time_filter": time_filter,
                "timestamp": datetime.now().isoformat()
            },
            "content": {
                "summary": content,
                "word_count": len(content.split()),
                "language": "zh-CN"
            },
            "citations": self._process_citations(citations),
            "metadata": {
                "model": api_result.get("model", "unknown"),
                "usage": api_result.get("usage", {}),
                "search_filter_applied": time_filter,
                "total_citations": len(citations)
            },
            "quality_metrics": {
                "citation_count": len(citations),
                "content_length": len(content),
                "relevance_indicators": self._extract_relevance_indicators(content, topic)
            }
        }
        
        return processed_result
    
    def _process_citations(self, citations: List[str]) -> List[Dict]:
        """å¤„ç†å¼•ç”¨é“¾æ¥"""
        processed_citations = []
        
        for i, citation in enumerate(citations, 1):
            citation_info = {
                "id": i,
                "url": citation,
                "domain": self._extract_domain(citation),
                "source_type": self._classify_source_type(citation),
                "authority_score": self._calculate_authority_score(citation)
            }
            processed_citations.append(citation_info)
        
        # æŒ‰æƒå¨æ€§æ’åº
        processed_citations.sort(key=lambda x: x["authority_score"], reverse=True)
        
        return processed_citations
    
    def _extract_domain(self, url: str) -> str:
        """æå–åŸŸå"""
        try:
            from urllib.parse import urlparse
            return urlparse(url).netloc
        except:
            return "unknown"
    
    def _classify_source_type(self, url: str) -> str:
        """åˆ†ç±»ä¿¡æ¯æºç±»å‹"""
        domain = self._extract_domain(url).lower()
        
        # ç›‘ç®¡æœºæ„
        if any(keyword in domain for keyword in ["pbc.gov.cn", "cbirc.gov.cn", "csrc.gov.cn"]):
            return "regulatory"
        
        # å­¦æœ¯æœºæ„
        elif any(keyword in domain for keyword in [".edu", "research", "institute"]):
            return "academic"
        
        # é‡‘èæœºæ„
        elif any(keyword in domain for keyword in ["bank", "finance", "credit"]):
            return "financial"
        
        # æ–°é—»åª’ä½“
        elif any(keyword in domain for keyword in ["news", "media", "daily"]):
            return "news"
        
        else:
            return "other"
    
    def _calculate_authority_score(self, url: str) -> float:
        """è®¡ç®—æƒå¨æ€§è¯„åˆ†"""
        domain = self._extract_domain(url).lower()
        
        # æƒå¨æ€§è¯„åˆ†è§„åˆ™
        if any(keyword in domain for keyword in ["pbc.gov.cn", "cbirc.gov.cn"]):
            return 1.0  # ç›‘ç®¡æœºæ„æœ€é«˜æƒå¨
        elif "bank" in domain or "finance" in domain:
            return 0.8  # é‡‘èæœºæ„é«˜æƒå¨
        elif ".edu" in domain or "research" in domain:
            return 0.7  # å­¦æœ¯æœºæ„è¾ƒé«˜æƒå¨
        elif "credit" in domain:
            return 0.6  # å¾ä¿¡ç›¸å…³ä¸­ç­‰æƒå¨
        else:
            return 0.3  # å…¶ä»–æ¥æºè¾ƒä½æƒå¨
    
    def _extract_relevance_indicators(self, content: str, topic: str) -> Dict:
        """æå–ç›¸å…³æ€§æŒ‡æ ‡"""
        content_lower = content.lower()
        topic_lower = topic.lower()
        
        # å¾ä¿¡é¢†åŸŸå…³é”®è¯
        credit_keywords = ["å¾ä¿¡", "ä¿¡ç”¨", "é£é™©", "è¯„çº§", "åˆè§„", "ç›‘ç®¡", "é‡‘èç§‘æŠ€", "å¤§æ•°æ®"]
        
        relevance_score = 0
        matched_keywords = []
        
        # è®¡ç®—ä¸»é¢˜åŒ¹é…åº¦
        if topic_lower in content_lower:
            relevance_score += 0.4
        
        # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
        for keyword in credit_keywords:
            if keyword in content:
                relevance_score += 0.1
                matched_keywords.append(keyword)
        
        return {
            "relevance_score": min(relevance_score, 1.0),
            "matched_keywords": matched_keywords,
            "topic_mentions": content_lower.count(topic_lower),
            "content_density": len(content.split()) / max(len(content), 1)
        }
    
    def _create_error_response(self, error_msg: str, topic: str, time_filter: str) -> Dict:
        """åˆ›å»ºé”™è¯¯å“åº”"""
        return {
            "query": {
                "topic": topic,
                "time_filter": time_filter,
                "timestamp": datetime.now().isoformat()
            },
            "error": {
                "message": error_msg,
                "type": "api_error"
            },
            "content": {
                "summary": f"æœç´¢ '{topic}' æ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                "word_count": 0,
                "language": "zh-CN"
            },
            "citations": [],
            "metadata": {
                "search_filter_applied": time_filter,
                "total_citations": 0
            }
        }

class CreditResearchSearchManager:
    """å¾ä¿¡ç ”ç©¶æœç´¢ç®¡ç†å™¨"""
    
    def __init__(self, api_key: str):
        self.perplexity_client = PerplexityAPIClient(api_key)
    
    async def comprehensive_search(self, topics: List[str], time_filter: str = "week") -> List[Dict]:
        """
        ç»¼åˆæœç´¢å¤šä¸ªå¾ä¿¡ä¸»é¢˜
        
        Args:
            topics: æœç´¢ä¸»é¢˜åˆ—è¡¨
            time_filter: æ—¶é—´è¿‡æ»¤å™¨ ("day", "week", "month", "year")
        
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        
        print(f"ğŸ” å¼€å§‹ç»¼åˆæœç´¢ï¼Œä¸»é¢˜æ•°é‡: {len(topics)}")
        print(f"â° æ—¶é—´è¿‡æ»¤å™¨: {time_filter}")
        
        # å¹¶å‘æ‰§è¡Œå¤šä¸ªæœç´¢
        tasks = []
        for topic in topics:
            task = self.perplexity_client.search_with_time_filter(topic, time_filter)
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰æœç´¢å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âŒ ä¸»é¢˜ '{topics[i]}' æœç´¢å¤±è´¥: {result}")
                continue
            
            processed_results.append(result)
            print(f"âœ… ä¸»é¢˜ '{topics[i]}' æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {result['metadata']['total_citations']} ä¸ªå¼•ç”¨")
        
        return processed_results
    
    def generate_search_summary(self, results: List[Dict]) -> Dict:
        """ç”Ÿæˆæœç´¢ç»“æœæ±‡æ€»"""
        
        total_citations = sum(r["metadata"]["total_citations"] for r in results)
        total_content_length = sum(r["content"]["word_count"] for r in results)
        
        # æ±‡æ€»æƒå¨æ¥æº
        authority_sources = []
        for result in results:
            for citation in result["citations"]:
                if citation["authority_score"] >= 0.7:
                    authority_sources.append(citation)
        
        # å»é‡å¹¶æ’åº
        authority_sources = sorted(
            {source["url"]: source for source in authority_sources}.values(),
            key=lambda x: x["authority_score"], 
            reverse=True
        )
        
        return {
            "summary": {
                "total_topics_searched": len(results),
                "total_citations_found": total_citations,
                "total_content_words": total_content_length,
                "authority_sources_count": len(authority_sources)
            },
            "top_authority_sources": authority_sources[:10],
            "search_timestamp": datetime.now().isoformat(),
            "time_filter_used": results[0]["query"]["time_filter"] if results else "unknown"
        }

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    
    # åˆå§‹åŒ–æœç´¢ç®¡ç†å™¨
    api_key = os.getenv("PERPLEXITY_API_KEY")
    if not api_key:
        print("âŒ è¯·è®¾ç½®PERPLEXITY_API_KEYç¯å¢ƒå˜é‡")
        return
    
    search_manager = CreditResearchSearchManager(api_key)
    
    # å¾ä¿¡ç ”ç©¶ä¸»é¢˜
    research_topics = [
        "å¾ä¿¡é£é™©ç®¡ç†",
        "ESGè¯„çº§ä½“ç³»", 
        "é‡‘èç§‘æŠ€ç›‘ç®¡",
        "æ•°å­—å¾ä¿¡åˆ›æ–°"
    ]
    
    # æ‰§è¡Œæœç´¢ï¼ˆæœ€è¿‘ä¸€å‘¨çš„å†…å®¹ï¼‰
    print("ğŸš€ å¼€å§‹å¾ä¿¡ç ”ç©¶æœç´¢...")
    results = await search_manager.comprehensive_search(
        topics=research_topics,
        time_filter="week"  # ä½¿ç”¨å®˜æ–¹APIæ ¼å¼
    )
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    summary = search_manager.generate_search_summary(results)
    
    print("\nğŸ“Š æœç´¢ç»“æœæ±‡æ€»:")
    print(f"   æœç´¢ä¸»é¢˜æ•°: {summary['summary']['total_topics_searched']}")
    print(f"   æ‰¾åˆ°å¼•ç”¨æ•°: {summary['summary']['total_citations_found']}")
    print(f"   æƒå¨æ¥æºæ•°: {summary['summary']['authority_sources_count']}")
    
    print(f"\nğŸ† Top 5 æƒå¨æ¥æº:")
    for i, source in enumerate(summary['top_authority_sources'][:5], 1):
        print(f"   {i}. {source['domain']} (æƒå¨æ€§: {source['authority_score']:.1f})")
    
    # ä¿å­˜ç»“æœ
    output_file = f"credit_research_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "results": results,
            "summary": summary
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    asyncio.run(main())
 
 
 