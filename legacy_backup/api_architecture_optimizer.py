#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
APIæ¶æ„ä¼˜åŒ–å™¨
æ”¯æŒæ¨¡å‹åˆ‡æ¢ã€ä¸€è‡´æ€§ä¿è¯å’Œæ€§èƒ½ä¼˜åŒ–
"""

import asyncio
import json
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import logging

from .unified_api_manager import UnifiedAPIManager, ModelProvider
from .model_consistency_manager import consistency_manager
from .progress_manager import ProgressManager, TaskType

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SwitchStrategy(Enum):
    """æ¨¡å‹åˆ‡æ¢ç­–ç•¥"""
    FAILOVER = "failover"           # æ•…éšœè½¬ç§»
    LOAD_BALANCE = "load_balance"   # è´Ÿè½½å‡è¡¡
    COST_OPTIMIZE = "cost_optimize" # æˆæœ¬ä¼˜åŒ–
    PERFORMANCE = "performance"     # æ€§èƒ½ä¼˜å…ˆ
    CONSISTENCY = "consistency"     # ä¸€è‡´æ€§ä¼˜å…ˆ

class OptimizationLevel(Enum):
    """ä¼˜åŒ–çº§åˆ«"""
    BASIC = "basic"         # åŸºç¡€ä¼˜åŒ–
    STANDARD = "standard"   # æ ‡å‡†ä¼˜åŒ–
    AGGRESSIVE = "aggressive" # æ¿€è¿›ä¼˜åŒ–

@dataclass
class APIEndpointConfig:
    """APIç«¯ç‚¹é…ç½®"""
    provider: str
    model: str
    endpoint: str
    priority: int
    max_requests_per_minute: int
    max_tokens_per_request: int
    cost_per_1k_tokens: float
    latency_ms: float
    reliability_score: float  # 0-1

@dataclass 
class SwitchDecision:
    """åˆ‡æ¢å†³ç­–"""
    from_provider: str
    to_provider: str
    reason: str
    confidence: float
    expected_benefit: str

class APIArchitectureOptimizer:
    """APIæ¶æ„ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.api_manager = None
        self.progress_manager = ProgressManager()
        self.endpoint_configs: Dict[str, APIEndpointConfig] = {}
        self.performance_metrics: Dict[str, Dict] = {}
        self.switch_history: List[SwitchDecision] = []
        self.current_strategy = SwitchStrategy.CONSISTENCY
        self.optimization_level = OptimizationLevel.STANDARD
        
        # åˆå§‹åŒ–é»˜è®¤é…ç½®
        self._init_default_configs()
    
    def _init_default_configs(self):
        """åˆå§‹åŒ–é»˜è®¤é…ç½®"""
        self.endpoint_configs = {
            "qwen_chat": APIEndpointConfig(
                provider="qwen",
                model="qwen-plus",
                endpoint="https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
                priority=1,
                max_requests_per_minute=60,
                max_tokens_per_request=8000,
                cost_per_1k_tokens=0.4,
                latency_ms=1500,
                reliability_score=0.95
            ),
            "qwen_embedding": APIEndpointConfig(
                provider="qwen",
                model="text-embedding-v2",
                endpoint="https://dashscope.aliyuncs.com/api/v1/services/embeddings/text-embedding/text-embedding",
                priority=1,
                max_requests_per_minute=120,
                max_tokens_per_request=2000,
                cost_per_1k_tokens=0.05,
                latency_ms=800,
                reliability_score=0.98
            ),
            # "deepseek_chat": APIEndpointConfig(  # é…ç½®ä¿ç•™ï¼Œä¾¿äºå°†æ¥æ‰©å±•
            #     provider="deepseek",
            #     model="deepseek-chat",
            #     endpoint="https://api.deepseek.com/chat/completions",
            #     priority=2,
            #     max_requests_per_minute=60,
            #     max_tokens_per_request=4000,
            #     cost_per_1k_tokens=0.14,
            #     latency_ms=1200,
            #     reliability_score=0.92
            # ),
            # "deepseek_embedding": APIEndpointConfig(  # é…ç½®ä¿ç•™ï¼Œä¾¿äºå°†æ¥æ‰©å±•
            #     provider="deepseek",
            #     model="deepseek-embedding",
            #     endpoint="https://api.deepseek.com/embeddings",
            #     priority=2,
            #     max_requests_per_minute=100,
            #     max_tokens_per_request=1500,
            #     cost_per_1k_tokens=0.02,
            #     latency_ms=600,
            #     reliability_score=0.94
            # )
        }
    
    def set_strategy(self, strategy: SwitchStrategy, optimization_level: OptimizationLevel = OptimizationLevel.STANDARD):
        """è®¾ç½®åˆ‡æ¢ç­–ç•¥å’Œä¼˜åŒ–çº§åˆ«"""
        self.current_strategy = strategy
        self.optimization_level = optimization_level
        logger.info(f"ğŸ”§ APIæ¶æ„ç­–ç•¥: {strategy.value}, ä¼˜åŒ–çº§åˆ«: {optimization_level.value}")
    
    def analyze_optimal_provider(self, task_type: str, input_tokens: int, 
                                requirements: Dict[str, Any] = None) -> Tuple[str, float]:
        """åˆ†ææœ€ä¼˜æä¾›å•†"""
        if not requirements:
            requirements = {}
        
        scores = {}
        
        # è¿‡æ»¤ç›¸å…³ç«¯ç‚¹
        relevant_endpoints = {
            name: config for name, config in self.endpoint_configs.items()
            if (task_type in name) or (task_type == "general")
        }
        
        for endpoint_name, config in relevant_endpoints.items():
            score = 0.0
            
            # åŸºç¡€åˆ†æ•° = ä¼˜å…ˆçº§æƒé‡
            score += (3 - config.priority) * 20
            
            # ç­–ç•¥ç‰¹å®šè¯„åˆ†
            if self.current_strategy == SwitchStrategy.COST_OPTIMIZE:
                # æˆæœ¬ä¼˜åŒ–ï¼šå€¾å‘äºä½æˆæœ¬
                cost_score = max(0, 100 - config.cost_per_1k_tokens * 100)
                score += cost_score * 0.4
                
            elif self.current_strategy == SwitchStrategy.PERFORMANCE:
                # æ€§èƒ½ä¼˜åŒ–ï¼šå€¾å‘äºä½å»¶è¿Ÿ
                latency_score = max(0, 100 - config.latency_ms / 20)
                score += latency_score * 0.4
                
            elif self.current_strategy == SwitchStrategy.CONSISTENCY:
                # ä¸€è‡´æ€§ä¼˜åŒ–ï¼šå€¾å‘äºé«˜å¯é æ€§
                score += config.reliability_score * 40
                
            elif self.current_strategy == SwitchStrategy.LOAD_BALANCE:
                # è´Ÿè½½å‡è¡¡ï¼šè€ƒè™‘å½“å‰è´Ÿè½½
                current_load = self._get_current_load(config.provider)
                load_score = max(0, 100 - current_load)
                score += load_score * 0.3
            
            # å®¹é‡æ£€æŸ¥
            if input_tokens > config.max_tokens_per_request:
                score *= 0.1  # ä¸¥é‡æƒ©ç½š
            
            # ç‰¹æ®Šè¦æ±‚æ£€æŸ¥
            if requirements.get("consistency_required") and consistency_manager:
                # æ£€æŸ¥ä¸€è‡´æ€§å“ˆå¸Œ
                current_hash = consistency_manager.get_recommended_model()
                if current_hash:
                    model_info = consistency_manager.get_model_info(current_hash)
                    if model_info and model_info.provider == config.provider:
                        score += 30  # ä¸€è‡´æ€§å¥–åŠ±
            
            scores[config.provider] = score
        
        # é€‰æ‹©æœ€é«˜åˆ†çš„æä¾›å•†
        if scores:
            best_provider = max(scores.keys(), key=lambda k: scores[k])
            return best_provider, scores[best_provider]
        
        return "qwen", 50.0  # é»˜è®¤
    
    def _get_current_load(self, provider: str) -> float:
        """è·å–å½“å‰è´Ÿè½½(0-100)"""
        # æ¨¡æ‹Ÿè´Ÿè½½è®¡ç®—
        import random
        random.seed(hash(provider) % 1000)
        return random.uniform(10, 80)
    
    def should_switch_provider(self, current_provider: str, task_type: str, 
                             input_tokens: int, error_count: int = 0) -> Optional[SwitchDecision]:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ‡æ¢æä¾›å•†"""
        
        # è·å–æ¨èæä¾›å•†
        recommended_provider, score = self.analyze_optimal_provider(task_type, input_tokens)
        
        # å†³ç­–é€»è¾‘
        should_switch = False
        reason = ""
        confidence = 0.0
        
        if error_count > 0:
            # é”™è¯¯è§¦å‘åˆ‡æ¢
            should_switch = True
            reason = f"å½“å‰æä¾›å•†{current_provider}å‡ºç°{error_count}æ¬¡é”™è¯¯"
            confidence = min(0.9, 0.3 + error_count * 0.2)
            
        elif recommended_provider != current_provider:
            # åˆ†æ•°å·®å¼‚è§¦å‘åˆ‡æ¢
            current_score = self._calculate_provider_score(current_provider, task_type, input_tokens)
            score_diff = score - current_score
            
            # åˆ‡æ¢é˜ˆå€¼æ ¹æ®ä¼˜åŒ–çº§åˆ«è°ƒæ•´
            threshold = {
                OptimizationLevel.BASIC: 30,
                OptimizationLevel.STANDARD: 20, 
                OptimizationLevel.AGGRESSIVE: 10
            }.get(self.optimization_level, 20)
            
            if score_diff > threshold:
                should_switch = True
                reason = f"æ¨èæä¾›å•†{recommended_provider}åˆ†æ•°æ›´é«˜ (å·®å¼‚: {score_diff:.1f})"
                confidence = min(0.8, score_diff / 50)
        
        if should_switch:
            expected_benefit = self._calculate_expected_benefit(
                current_provider, recommended_provider, task_type
            )
            
            decision = SwitchDecision(
                from_provider=current_provider,
                to_provider=recommended_provider,
                reason=reason,
                confidence=confidence,
                expected_benefit=expected_benefit
            )
            
            self.switch_history.append(decision)
            return decision
        
        return None
    
    def _calculate_provider_score(self, provider: str, task_type: str, input_tokens: int) -> float:
        """è®¡ç®—æä¾›å•†åˆ†æ•°"""
        relevant_config = None
        for name, config in self.endpoint_configs.items():
            if config.provider == provider and task_type in name:
                relevant_config = config
                break
        
        if not relevant_config:
            return 0.0
        
        # ç®€åŒ–åˆ†æ•°è®¡ç®—
        score = (3 - relevant_config.priority) * 20
        score += relevant_config.reliability_score * 40
        
        if self.current_strategy == SwitchStrategy.COST_OPTIMIZE:
            score += max(0, 100 - relevant_config.cost_per_1k_tokens * 100) * 0.4
        elif self.current_strategy == SwitchStrategy.PERFORMANCE:
            score += max(0, 100 - relevant_config.latency_ms / 20) * 0.4
        
        return score
    
    def _calculate_expected_benefit(self, from_provider: str, to_provider: str, task_type: str) -> str:
        """è®¡ç®—é¢„æœŸæ”¶ç›Š"""
        benefits = []
        
        from_config = self._get_provider_config(from_provider, task_type)
        to_config = self._get_provider_config(to_provider, task_type)
        
        if not from_config or not to_config:
            return "é…ç½®ä¿¡æ¯ä¸è¶³"
        
        # æˆæœ¬æ¯”è¾ƒ
        cost_diff = from_config.cost_per_1k_tokens - to_config.cost_per_1k_tokens
        if abs(cost_diff) > 0.01:
            if cost_diff > 0:
                benefits.append(f"æˆæœ¬é™ä½{cost_diff:.3f}/1k tokens")
            else:
                benefits.append(f"æˆæœ¬å¢åŠ {abs(cost_diff):.3f}/1k tokens")
        
        # å»¶è¿Ÿæ¯”è¾ƒ
        latency_diff = from_config.latency_ms - to_config.latency_ms
        if abs(latency_diff) > 50:
            if latency_diff > 0:
                benefits.append(f"å»¶è¿Ÿå‡å°‘{latency_diff:.0f}ms")
            else:
                benefits.append(f"å»¶è¿Ÿå¢åŠ {abs(latency_diff):.0f}ms")
        
        # å¯é æ€§æ¯”è¾ƒ
        reliability_diff = to_config.reliability_score - from_config.reliability_score
        if abs(reliability_diff) > 0.02:
            if reliability_diff > 0:
                benefits.append(f"å¯é æ€§æå‡{reliability_diff:.1%}")
            else:
                benefits.append(f"å¯é æ€§ä¸‹é™{abs(reliability_diff):.1%}")
        
        return "; ".join(benefits) if benefits else "æ•´ä½“æ€§èƒ½ä¼˜åŒ–"
    
    def _get_provider_config(self, provider: str, task_type: str) -> Optional[APIEndpointConfig]:
        """è·å–æä¾›å•†é…ç½®"""
        for name, config in self.endpoint_configs.items():
            if config.provider == provider and task_type in name:
                return config
        return None
    
    def update_performance_metrics(self, provider: str, task_type: str, 
                                 latency_ms: float, success: bool, tokens_used: int):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        key = f"{provider}_{task_type}"
        
        if key not in self.performance_metrics:
            self.performance_metrics[key] = {
                "total_requests": 0,
                "successful_requests": 0,
                "total_latency": 0,
                "total_tokens": 0,
                "last_updated": time.time()
            }
        
        metrics = self.performance_metrics[key]
        metrics["total_requests"] += 1
        if success:
            metrics["successful_requests"] += 1
        metrics["total_latency"] += latency_ms
        metrics["total_tokens"] += tokens_used
        metrics["last_updated"] = time.time()
        
        # æ›´æ–°ç«¯ç‚¹é…ç½®ä¸­çš„å®æ—¶æŒ‡æ ‡
        config = self._get_provider_config(provider, task_type)
        if config:
            # æ›´æ–°å¹³å‡å»¶è¿Ÿ
            config.latency_ms = metrics["total_latency"] / metrics["total_requests"]
            # æ›´æ–°å¯é æ€§åˆ†æ•°
            config.reliability_score = metrics["successful_requests"] / metrics["total_requests"]
    
    def optimize_architecture(self) -> Dict[str, Any]:
        """æ‰§è¡Œæ¶æ„ä¼˜åŒ–"""
        optimization_report = {
            "timestamp": time.time(),
            "strategy": self.current_strategy.value,
            "optimization_level": self.optimization_level.value,
            "recommendations": [],
            "performance_summary": {},
            "switch_history_count": len(self.switch_history)
        }
        
        # åˆ†ææ€§èƒ½æŒ‡æ ‡
        for key, metrics in self.performance_metrics.items():
            provider, task_type = key.split("_", 1)
            
            avg_latency = metrics["total_latency"] / metrics["total_requests"]
            success_rate = metrics["successful_requests"] / metrics["total_requests"]
            
            optimization_report["performance_summary"][key] = {
                "avg_latency_ms": round(avg_latency, 1),
                "success_rate": round(success_rate, 3),
                "total_requests": metrics["total_requests"],
                "total_tokens": metrics["total_tokens"]
            }
            
            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            if success_rate < 0.9:
                optimization_report["recommendations"].append({
                    "type": "reliability",
                    "provider": provider,
                    "task_type": task_type,
                    "issue": f"æˆåŠŸç‡ä»…{success_rate:.1%}ï¼Œå»ºè®®åˆ‡æ¢æä¾›å•†",
                    "priority": "high"
                })
            
            if avg_latency > 2000:
                optimization_report["recommendations"].append({
                    "type": "performance",
                    "provider": provider,
                    "task_type": task_type,
                    "issue": f"å¹³å‡å»¶è¿Ÿ{avg_latency:.0f}msè¿‡é«˜ï¼Œå»ºè®®ä¼˜åŒ–",
                    "priority": "medium"
                })
        
        return optimization_report
    
    def get_architecture_status(self) -> Dict[str, Any]:
        """è·å–æ¶æ„çŠ¶æ€"""
        return {
            "current_strategy": self.current_strategy.value,
            "optimization_level": self.optimization_level.value,
            "endpoint_count": len(self.endpoint_configs),
            "performance_metrics_count": len(self.performance_metrics),
            "recent_switches": len([s for s in self.switch_history if time.time() - s.confidence < 3600]),
            "total_switches": len(self.switch_history),
            "consistency_manager_active": consistency_manager is not None
        }
    
    def reset_metrics(self):
        """é‡ç½®æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics.clear()
        self.switch_history.clear()
        logger.info("ğŸ”„ æ€§èƒ½æŒ‡æ ‡å·²é‡ç½®")


# å…¨å±€ä¼˜åŒ–å™¨å®ä¾‹
api_optimizer = APIArchitectureOptimizer()

def optimize_api_selection(task_type: str, input_tokens: int, 
                         current_provider: str = None, 
                         requirements: Dict[str, Any] = None) -> str:
    """ä¼˜åŒ–APIé€‰æ‹©ï¼ˆç®€åŒ–æ¥å£ï¼‰"""
    recommended_provider, _ = api_optimizer.analyze_optimal_provider(
        task_type, input_tokens, requirements
    )
    return recommended_provider

def check_switch_recommendation(current_provider: str, task_type: str, 
                              input_tokens: int, error_count: int = 0) -> Optional[Dict]:
    """æ£€æŸ¥åˆ‡æ¢å»ºè®®ï¼ˆç®€åŒ–æ¥å£ï¼‰"""
    decision = api_optimizer.should_switch_provider(
        current_provider, task_type, input_tokens, error_count
    )
    return asdict(decision) if decision else None


# æ¼”ç¤ºå‡½æ•°
async def demo_api_optimization():
    """æ¼”ç¤ºAPIæ¶æ„ä¼˜åŒ–å™¨"""
    print("ğŸ¯ APIæ¶æ„ä¼˜åŒ–å™¨æ¼”ç¤º")
    print("=" * 60)
    
    optimizer = api_optimizer
    
    # è®¾ç½®ç­–ç•¥
    print("\nğŸ”§ è®¾ç½®ä¼˜åŒ–ç­–ç•¥")
    optimizer.set_strategy(SwitchStrategy.CONSISTENCY, OptimizationLevel.STANDARD)
    
    # æ¨¡æ‹Ÿä¸€äº›APIè°ƒç”¨æ€§èƒ½æ•°æ®
    print("\nğŸ“Š æ¨¡æ‹ŸAPIæ€§èƒ½æ•°æ®æ”¶é›†")
    test_scenarios = [
        ("qwen", "embedding", 800, True, 1000),
        ("qwen", "embedding", 1200, True, 1500),
        # ("deepseek", "embedding", 600, False, 800),  # å¤±è´¥æ¡ˆä¾‹ - æ¼”ç¤ºä»£ç ä¿ç•™
        ("qwen", "chat", 1500, True, 2000),
        # ("deepseek", "chat", 1100, True, 1200),  # æ¼”ç¤ºä»£ç ä¿ç•™
    ]
    
    for provider, task_type, latency, success, tokens in test_scenarios:
        optimizer.update_performance_metrics(provider, task_type, latency, success, tokens)
        print(f"   ğŸ“ˆ {provider}/{task_type}: {latency}ms, æˆåŠŸ: {success}")
    
    # æµ‹è¯•æœ€ä¼˜æä¾›å•†é€‰æ‹©
    print("\nğŸ¯ æµ‹è¯•æœ€ä¼˜æä¾›å•†é€‰æ‹©")
    test_requests = [
        ("embedding", 1000, {"consistency_required": True}),
        ("chat", 3000, {"cost_sensitive": True}),
        ("embedding", 500, {})
    ]
    
    for task_type, tokens, requirements in test_requests:
        provider, score = optimizer.analyze_optimal_provider(task_type, tokens, requirements)
        print(f"   ğŸ“‹ {task_type}({tokens} tokens): æ¨è {provider} (åˆ†æ•°: {score:.1f})")
    
    # æµ‹è¯•åˆ‡æ¢å»ºè®®
    print("\nğŸ”„ æµ‹è¯•åˆ‡æ¢å»ºè®®")
    # switch_decision = optimizer.should_switch_provider("deepseek", "embedding", 1000, error_count=1)  # æ¼”ç¤ºä»£ç ä¿ç•™
    switch_decision = optimizer.should_switch_provider("qwen", "embedding", 1000, error_count=0)  # å½“å‰ä½¿ç”¨åƒé—®
    if switch_decision:
        print(f"   ğŸ’¡ å»ºè®®: {switch_decision.from_provider} â†’ {switch_decision.to_provider}")
        print(f"   ğŸ“ åŸå› : {switch_decision.reason}")
        print(f"   ğŸ“Š ç½®ä¿¡åº¦: {switch_decision.confidence:.1%}")
        print(f"   ğŸ’° é¢„æœŸæ”¶ç›Š: {switch_decision.expected_benefit}")
    
    # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    print("\nğŸ“„ ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š")
    report = optimizer.optimize_architecture()
    print(f"   ğŸ“Š ç­–ç•¥: {report['strategy']}")
    print(f"   ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡: {len(report['performance_summary'])} é¡¹")
    print(f"   ğŸ’¡ ä¼˜åŒ–å»ºè®®: {len(report['recommendations'])} æ¡")
    
    for rec in report['recommendations']:
        print(f"      - {rec['type']}: {rec['issue']} (ä¼˜å…ˆçº§: {rec['priority']})")
    
    # æ˜¾ç¤ºæ¶æ„çŠ¶æ€
    print("\nğŸ“Š æ¶æ„çŠ¶æ€")
    status = optimizer.get_architecture_status()
    for key, value in status.items():
        print(f"   {key}: {value}")


if __name__ == "__main__":
    asyncio.run(demo_api_optimization())