"""
ç­›é€‰ç®¡ç†æ¨¡å—
"""

import json
import os
from typing import List, Dict, Any, Optional
import chromadb
from chromadb.utils import embedding_functions
from openai import OpenAI

class FilterManager:
    """ç­›é€‰ç®¡ç†å™¨"""
    
    def __init__(self, embedding_manager, llm_api_key: str, llm_platform: str = "llm"):
        """
        åˆå§‹åŒ–ç­›é€‰ç®¡ç†å™¨
        
        Args:
            embedding_manager: å‘é‡åŒ–ç®¡ç†å™¨
            llm_api_key: å¤§æ¨¡å‹APIå¯†é’¥
            llm_platform: å¤§æ¨¡å‹å¹³å° ("llm", "llm-claude", "llm-gpt")
        """
        self.embedding_manager = embedding_manager
        self.llm_api_key = llm_api_key
        self.llm_platform = llm_platform
        self.chroma_client = None
        self.collection = None
        
        self._init_chromadb()
        self._init_llm_client()
    
    def _init_chromadb(self):
        """åˆå§‹åŒ–ChromaDB"""
        try:
            self.chroma_client = chromadb.PersistentClient(path="data/chroma_db")
            self.collection = self.chroma_client.get_or_create_collection("creditmag")
            print("âœ… ChromaDB åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ ChromaDB åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_llm_client(self):
        """åˆå§‹åŒ–å¤§æ¨¡å‹å®¢æˆ·ç«¯"""
        if self.llm_platform == "llm":
            self.llm_client = OpenAI(
                api_key=self.llm_api_key,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
            self.llm_model = "qwen-turbo"
        # elif self.llm_platform == "deepseek":  # å·²æ³¨é‡Šï¼Œä¸“æ³¨åƒé—®
        #     self.llm_client = OpenAI(
        #         api_key=self.llm_api_key,
        #         base_url="https://api.deepseek.com/v1"
        #     )
        #     self.llm_model = "deepseek-chat"
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¤§æ¨¡å‹å¹³å°: {self.llm_platform}")
    
    def filter_by_vector_similarity(self, search_results: List[Dict[str, Any]], top_k: int = 5) -> List[Dict[str, Any]]:
        """
        åŸºäºå‘é‡ç›¸ä¼¼åº¦ç­›é€‰
        
        Args:
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            top_k: è¿”å›çš„å€™é€‰æ•°é‡
            
        Returns:
            ç­›é€‰åçš„å€™é€‰æ–‡æ¡£åˆ—è¡¨
        """
        # æå–æœ‰æ•ˆå†…å®¹
        valid_docs = []
        for result in search_results:
            if result.get("success") and result.get("content"):
                valid_docs.append({
                    "topic": result["topic"],
                    "content": result["content"]
                })
        
        if not valid_docs:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æœç´¢ç»“æœ")
            return []
        
        # ç”Ÿæˆå‘é‡
        texts = [doc["content"] for doc in valid_docs]
        embeddings = self.embedding_manager.embed_texts(texts)
        
        if not embeddings:
            print("âŒ å‘é‡åŒ–å¤±è´¥")
            return []
        
        # æŸ¥è¯¢ChromaDB
        try:
            query_results = self.collection.query(
                query_embeddings=embeddings,
                n_results=top_k
            )
            
            # æ„å»ºå€™é€‰æ–‡æ¡£
            candidate_docs = []
            for i, (doc, distance) in enumerate(zip(valid_docs, query_results['distances'][0])):
                candidate_docs.append({
                    "index": i,
                    "topic": doc["topic"],
                    "content": doc["content"][:1000],  # é™åˆ¶é•¿åº¦
                    "similarity": 1 - distance,  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                    "original_content": doc["content"]
                })
            
            print(f"âœ… å‘é‡ç›¸ä¼¼åº¦ç­›é€‰å®Œæˆï¼Œæ‰¾åˆ° {len(candidate_docs)} ä¸ªå€™é€‰æ–‡æ¡£")
            return candidate_docs
            
        except Exception as e:
            print(f"âŒ ChromaDB æŸ¥è¯¢å¤±è´¥: {e}")
            return []
    
    def filter_by_llm(self, candidate_docs: List[Dict[str, Any]], final_count: int = 2) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œæ™ºèƒ½ç­›é€‰
        
        Args:
            candidate_docs: å€™é€‰æ–‡æ¡£åˆ—è¡¨
            final_count: æœ€ç»ˆé€‰æ‹©æ•°é‡
            
        Returns:
            æœ€ç»ˆç­›é€‰ç»“æœ
        """
        if not candidate_docs:
            return []
        
        # æ„å»ºç­›é€‰æç¤º
        filter_prompt = f"""
è¯·ä»ä»¥ä¸‹{len(candidate_docs)}ç¯‡å…³äºä¿¡ç”¨è¯„çº§å’Œå¾ä¿¡ç ”ç©¶çš„æ–‡æ¡£ä¸­ï¼Œé€‰å‡ºæœ€ç›¸å…³ã€æœ€æœ‰ä»·å€¼çš„{final_count}ç¯‡ã€‚

ç­›é€‰æ ‡å‡†ï¼š
1. å†…å®¹ä¸ä¿¡ç”¨è¯„çº§ã€å¾ä¿¡ä½“ç³»å»ºè®¾ã€é‡‘èç›‘ç®¡é«˜åº¦ç›¸å…³
2. ä¿¡æ¯æ—¶æ•ˆæ€§å¼ºï¼Œå…·æœ‰å®é™…å‚è€ƒä»·å€¼
3. æ¥æºæƒå¨ï¼Œå†…å®¹è¯¦å®

å€™é€‰æ–‡æ¡£ï¼š
{json.dumps(candidate_docs, ensure_ascii=False, indent=2)}

è¯·è¿”å›JSONæ ¼å¼çš„ç»“æœï¼ŒåŒ…å«é€‰ä¸­çš„æ–‡æ¡£ç´¢å¼•å·å’Œé€‰æ‹©ç†ç”±ï¼š
{{"selected_indices": [0, 1], "reason": "é€‰æ‹©ç†ç”±"}}
"""
        
        try:
            response = self.llm_client.chat.completions.create(
                model=self.llm_model,
                messages=[{"role": "user", "content": filter_prompt}],
                temperature=0.1
            )
            
            # è§£æå¤§æ¨¡å‹çš„é€‰æ‹©ç»“æœ
            selection_result = json.loads(response.choices[0].message.content)
            selected_indices = selection_result.get("selected_indices", [])
            
            # è·å–æœ€ç»ˆé€‰ä¸­çš„æ–‡æ¡£
            final_results = []
            for idx in selected_indices[:final_count]:
                if idx < len(candidate_docs):
                    final_results.append(candidate_docs[idx])
            
            print(f"âœ… å¤§æ¨¡å‹ç­›é€‰å®Œæˆï¼Œé€‰ä¸­ {len(final_results)} ç¯‡æ–‡æ¡£")
            print(f"ğŸ“ é€‰æ‹©ç†ç”±: {selection_result.get('reason', 'æœªæä¾›')}")
            
            return final_results
            
        except Exception as e:
            print(f"âŒ å¤§æ¨¡å‹ç­›é€‰å¤±è´¥: {e}")
            # é™çº§æ–¹æ¡ˆï¼šé€‰æ‹©ç›¸ä¼¼åº¦æœ€é«˜çš„æ–‡æ¡£
            sorted_docs = sorted(candidate_docs, key=lambda x: x["similarity"], reverse=True)
            final_results = sorted_docs[:final_count]
            print(f"âš ï¸  ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦é™çº§æ–¹æ¡ˆï¼Œé€‰ä¸­ {len(final_results)} ç¯‡æ–‡æ¡£")
            return final_results
    
    def filter_documents(self, search_results: List[Dict[str, Any]], 
                        vector_top_k: int = 5, final_count: int = 2) -> Dict[str, Any]:
        """
        å®Œæ•´çš„æ–‡æ¡£ç­›é€‰æµç¨‹
        
        Args:
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            vector_top_k: å‘é‡ç›¸ä¼¼åº¦ç­›é€‰æ•°é‡
            final_count: æœ€ç»ˆé€‰æ‹©æ•°é‡
            
        Returns:
            ç­›é€‰ç»“æœå­—å…¸
        """
        print("ğŸ” å¼€å§‹æ–‡æ¡£ç­›é€‰æµç¨‹...")
        
        # æ­¥éª¤1ï¼šå‘é‡ç›¸ä¼¼åº¦ç­›é€‰
        candidate_docs = self.filter_by_vector_similarity(search_results, vector_top_k)
        
        if not candidate_docs:
            return {
                "success": False,
                "error": "æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å€™é€‰æ–‡æ¡£",
                "selected_documents": [],
                "statistics": {
                    "total_candidates": 0,
                    "final_selection": 0
                }
            }
        
        # æ­¥éª¤2ï¼šå¤§æ¨¡å‹æ™ºèƒ½ç­›é€‰
        final_results = self.filter_by_llm(candidate_docs, final_count)
        
        # æ„å»ºç»“æœ
        result = {
            "success": True,
            "selected_documents": final_results,
            "total_candidates": len(candidate_docs),
            "final_selection": len(final_results),
            "selection_method": "vector_similarity + llm_filtering",
            "statistics": {
                "total_candidates": len(candidate_docs),
                "final_selection": len(final_results),
                "average_similarity": sum(doc["similarity"] for doc in final_results) / len(final_results) if final_results else 0
            }
        }
        
        print(f"âœ… æ–‡æ¡£ç­›é€‰å®Œæˆ")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  - å€™é€‰æ–‡æ¡£æ•°: {result['total_candidates']}")
        print(f"  - æœ€ç»ˆé€‰æ‹©æ•°: {result['final_selection']}")
        print(f"  - å¹³å‡ç›¸ä¼¼åº¦: {result['statistics']['average_similarity']:.3f}")
        
        return result
    
    def save_filter_results(self, filter_results: Dict[str, Any], 
                          filepath: str = "data/filtered_results.json"):
        """
        ä¿å­˜ç­›é€‰ç»“æœ
        
        Args:
            filter_results: ç­›é€‰ç»“æœ
            filepath: ä¿å­˜è·¯å¾„
        """
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(filter_results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ ç­›é€‰ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
    
    def test_components(self) -> Dict[str, bool]:
        """æµ‹è¯•å„ä¸ªç»„ä»¶"""
        test_results = {
            "chromadb": False,
            "embedding": False,
            "llm": False
        }
        
        # æµ‹è¯•ChromaDB
        try:
            if self.collection:
                test_results["chromadb"] = True
                print("âœ… ChromaDB è¿æ¥æ­£å¸¸")
        except Exception as e:
            print(f"âŒ ChromaDB è¿æ¥å¤±è´¥: {e}")
        
        # æµ‹è¯•å‘é‡åŒ–
        if self.embedding_manager.test_connection():
            test_results["embedding"] = True
        
        # æµ‹è¯•å¤§æ¨¡å‹
        try:
            response = self.llm_client.chat.completions.create(
                model=self.llm_model,
                messages=[{"role": "user", "content": "æµ‹è¯•"}],
                max_tokens=10
            )
            test_results["llm"] = True
            print("âœ… å¤§æ¨¡å‹è¿æ¥æ­£å¸¸")
        except Exception as e:
            print(f"âŒ å¤§æ¨¡å‹è¿æ¥å¤±è´¥: {e}")
        
        return test_results 