#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å‘é‡æ¨¡å‹ç‰ˆæœ¬ç®¡ç†å’Œæ•°æ®è¿ç§»æœºåˆ¶
æ”¯æŒæ¨¡å‹å‡çº§ã€å‘é‡é‡æ–°è®¡ç®—å’Œæ•°æ®è¿ç§»
"""

import asyncio
import json
import hashlib
import shutil
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
from datetime import datetime
from enum import Enum
import logging

from .model_consistency_manager import consistency_manager, ModelConfig
from .unified_api_manager import UnifiedAPIManager, ModelProvider
from .progress_manager import ProgressManager, TaskType

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MigrationStrategy(Enum):
    """è¿ç§»ç­–ç•¥"""
    FULL_REBUILD = "full_rebuild"       # å®Œå…¨é‡å»º
    INCREMENTAL = "incremental"         # å¢é‡è¿ç§»
    PARALLEL_BUILD = "parallel_build"   # å¹¶è¡Œæ„å»º
    SELECTIVE = "selective"             # é€‰æ‹©æ€§è¿ç§»

class MigrationStatus(Enum):
    """è¿ç§»çŠ¶æ€"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class ModelVersion:
    """æ¨¡å‹ç‰ˆæœ¬ä¿¡æ¯"""
    version_id: str
    provider: str
    model_name: str
    api_version: str
    dimension: int
    created_at: str
    deprecated_at: Optional[str] = None
    migration_path: Optional[str] = None
    compatibility_score: float = 1.0

@dataclass
class VectorData:
    """å‘é‡æ•°æ®"""
    text_id: str
    text_content: str
    vector: List[float]
    model_version: str
    created_at: str
    metadata: Dict[str, Any]

@dataclass
class MigrationTask:
    """è¿ç§»ä»»åŠ¡"""
    task_id: str
    from_version: str
    to_version: str
    strategy: MigrationStrategy
    status: MigrationStatus
    progress: float
    start_time: Optional[str] = None
    end_time: Optional[str] = None
    error_message: Optional[str] = None
    stats: Dict[str, Any] = None

class VectorModelVersioning:
    """å‘é‡æ¨¡å‹ç‰ˆæœ¬ç®¡ç†å™¨"""
    
    def __init__(self, storage_path: str = "vector_versions"):
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        self.model_versions: Dict[str, ModelVersion] = {}
        self.vector_storage: Dict[str, Dict[str, VectorData]] = {}  # version_id -> {text_id -> VectorData}
        self.migration_tasks: List[MigrationTask] = []
        self.progress_manager = ProgressManager()
        
        # åŠ è½½ç°æœ‰ç‰ˆæœ¬
        self._load_versions()
    
    def _load_versions(self):
        """åŠ è½½ç°æœ‰ç‰ˆæœ¬ä¿¡æ¯"""
        versions_file = self.storage_path / "versions.json"
        if versions_file.exists():
            try:
                with open(versions_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                for version_id, version_data in data.items():
                    self.model_versions[version_id] = ModelVersion(**version_data)
                
                logger.info(f"ğŸ“– åŠ è½½äº† {len(self.model_versions)} ä¸ªæ¨¡å‹ç‰ˆæœ¬")
            except Exception as e:
                logger.error(f"âŒ åŠ è½½ç‰ˆæœ¬ä¿¡æ¯å¤±è´¥: {e}")
    
    def _save_versions(self):
        """ä¿å­˜ç‰ˆæœ¬ä¿¡æ¯"""
        versions_file = self.storage_path / "versions.json"
        try:
            data = {vid: asdict(version) for vid, version in self.model_versions.items()}
            with open(versions_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ’¾ ä¿å­˜äº† {len(self.model_versions)} ä¸ªæ¨¡å‹ç‰ˆæœ¬")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç‰ˆæœ¬ä¿¡æ¯å¤±è´¥: {e}")
    
    def register_model_version(self, provider: str, model_name: str, 
                             api_version: str = "v1", dimension: int = 1536,
                             compatibility_score: float = 1.0) -> str:
        """æ³¨å†Œæ–°çš„æ¨¡å‹ç‰ˆæœ¬"""
        # ç”Ÿæˆç‰ˆæœ¬ID
        version_content = f"{provider}:{model_name}:{api_version}:{dimension}"
        version_id = hashlib.sha256(version_content.encode()).hexdigest()[:16]
        
        if version_id not in self.model_versions:
            model_version = ModelVersion(
                version_id=version_id,
                provider=provider,
                model_name=model_name,
                api_version=api_version,
                dimension=dimension,
                created_at=datetime.now().isoformat(),
                compatibility_score=compatibility_score
            )
            
            self.model_versions[version_id] = model_version
            self.vector_storage[version_id] = {}
            
            # åˆ›å»ºç‰ˆæœ¬ç›®å½•
            version_dir = self.storage_path / version_id
            version_dir.mkdir(exist_ok=True)
            
            self._save_versions()
            logger.info(f"ğŸ“ æ³¨å†Œæ–°æ¨¡å‹ç‰ˆæœ¬: {provider}/{model_name} â†’ {version_id}")
        
        return version_id
    
    def store_vectors(self, version_id: str, text_data: List[Tuple[str, str, List[float]]], 
                     metadata: Dict[str, Any] = None):
        """å­˜å‚¨å‘é‡æ•°æ®"""
        if version_id not in self.model_versions:
            raise ValueError(f"æ¨¡å‹ç‰ˆæœ¬ {version_id} ä¸å­˜åœ¨")
        
        if metadata is None:
            metadata = {}
        
        # å­˜å‚¨åˆ°å†…å­˜
        for text_id, text_content, vector in text_data:
            vector_data = VectorData(
                text_id=text_id,
                text_content=text_content,
                vector=vector,
                model_version=version_id,
                created_at=datetime.now().isoformat(),
                metadata=metadata
            )
            self.vector_storage[version_id][text_id] = vector_data
        
        # æŒä¹…åŒ–å­˜å‚¨
        self._save_vector_batch(version_id, text_data, metadata)
        logger.info(f"ğŸ’¾ å­˜å‚¨äº† {len(text_data)} ä¸ªå‘é‡åˆ°ç‰ˆæœ¬ {version_id}")
    
    def _save_vector_batch(self, version_id: str, text_data: List[Tuple[str, str, List[float]]], 
                          metadata: Dict[str, Any]):
        """æ‰¹é‡ä¿å­˜å‘é‡æ•°æ®"""
        version_dir = self.storage_path / version_id
        
        # ä¿å­˜å‘é‡åˆ°numpyæ–‡ä»¶
        vectors = np.array([vector for _, _, vector in text_data])
        vectors_file = version_dir / f"vectors_{int(datetime.now().timestamp())}.npy"
        np.save(vectors_file, vectors)
        
        # ä¿å­˜æ–‡æœ¬å’Œå…ƒæ•°æ®
        text_data_file = version_dir / f"texts_{int(datetime.now().timestamp())}.json"
        text_info = [
            {
                "text_id": text_id,
                "text_content": text_content,
                "metadata": metadata
            }
            for text_id, text_content, _ in text_data
        ]
        
        with open(text_data_file, 'w', encoding='utf-8') as f:
            json.dump(text_info, f, indent=2, ensure_ascii=False)
    
    def analyze_migration_requirements(self, from_version: str, to_version: str) -> Dict[str, Any]:
        """åˆ†æè¿ç§»éœ€æ±‚"""
        if from_version not in self.model_versions or to_version not in self.model_versions:
            raise ValueError("æŒ‡å®šçš„æ¨¡å‹ç‰ˆæœ¬ä¸å­˜åœ¨")
        
        from_model = self.model_versions[from_version]
        to_model = self.model_versions[to_version]
        
        # è®¡ç®—å…¼å®¹æ€§
        compatibility_issues = []
        migration_complexity = "simple"
        
        # ç»´åº¦æ£€æŸ¥
        if from_model.dimension != to_model.dimension:
            compatibility_issues.append({
                "type": "dimension_mismatch",
                "description": f"å‘é‡ç»´åº¦ä¸åŒ¹é…: {from_model.dimension} â†’ {to_model.dimension}",
                "impact": "high",
                "requires_rebuild": True
            })
            migration_complexity = "complex"
        
        # æä¾›å•†æ£€æŸ¥
        if from_model.provider != to_model.provider:
            compatibility_issues.append({
                "type": "provider_change",
                "description": f"APIæä¾›å•†å˜æ›´: {from_model.provider} â†’ {to_model.provider}",
                "impact": "medium",
                "requires_rebuild": True
            })
            if migration_complexity == "simple":
                migration_complexity = "moderate"
        
        # æ¨¡å‹æ£€æŸ¥
        if from_model.model_name != to_model.model_name:
            compatibility_issues.append({
                "type": "model_change",
                "description": f"æ¨¡å‹å˜æ›´: {from_model.model_name} â†’ {to_model.model_name}",
                "impact": "medium",
                "requires_rebuild": True
            })
            if migration_complexity == "simple":
                migration_complexity = "moderate"
        
        # ä¼°ç®—è¿ç§»å·¥ä½œé‡
        vector_count = len(self.vector_storage.get(from_version, {}))
        estimated_time_hours = vector_count * 0.001  # ä¼°ç®—æ¯ä¸ªå‘é‡1æ¯«ç§’
        
        # æ¨èè¿ç§»ç­–ç•¥
        if migration_complexity == "simple":
            recommended_strategy = MigrationStrategy.INCREMENTAL
        elif vector_count > 10000:
            recommended_strategy = MigrationStrategy.PARALLEL_BUILD
        elif compatibility_issues:
            recommended_strategy = MigrationStrategy.FULL_REBUILD
        else:
            recommended_strategy = MigrationStrategy.SELECTIVE
        
        return {
            "from_version": from_version,
            "to_version": to_version,
            "compatibility_issues": compatibility_issues,
            "migration_complexity": migration_complexity,
            "vector_count": vector_count,
            "estimated_time_hours": estimated_time_hours,
            "recommended_strategy": recommended_strategy.value,
            "requires_api_calls": any(issue["requires_rebuild"] for issue in compatibility_issues)
        }
    
    async def create_migration_task(self, from_version: str, to_version: str, 
                                  strategy: MigrationStrategy = None) -> str:
        """åˆ›å»ºè¿ç§»ä»»åŠ¡"""
        # åˆ†æè¿ç§»éœ€æ±‚
        analysis = self.analyze_migration_requirements(from_version, to_version)
        
        if strategy is None:
            strategy = MigrationStrategy(analysis["recommended_strategy"])
        
        # åˆ›å»ºä»»åŠ¡
        task_id = f"migration_{int(datetime.now().timestamp())}"
        task = MigrationTask(
            task_id=task_id,
            from_version=from_version,
            to_version=to_version,
            strategy=strategy,
            status=MigrationStatus.PENDING,
            progress=0.0,
            stats={
                "total_vectors": analysis["vector_count"],
                "processed_vectors": 0,
                "failed_vectors": 0,
                "api_calls": 0,
                "estimated_time_hours": analysis["estimated_time_hours"]
            }
        )
        
        self.migration_tasks.append(task)
        logger.info(f"ğŸ“‹ åˆ›å»ºè¿ç§»ä»»åŠ¡: {task_id} ({strategy.value})")
        
        return task_id
    
    async def execute_migration(self, task_id: str, api_manager: UnifiedAPIManager = None) -> bool:
        """æ‰§è¡Œè¿ç§»ä»»åŠ¡"""
        task = self._get_migration_task(task_id)
        if not task:
            raise ValueError(f"è¿ç§»ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
        
        if task.status != MigrationStatus.PENDING:
            raise ValueError(f"ä»»åŠ¡çŠ¶æ€ä¸æ­£ç¡®: {task.status}")
        
        try:
            task.status = MigrationStatus.RUNNING
            task.start_time = datetime.now().isoformat()
            
            # åˆ›å»ºè¿›åº¦ç®¡ç†å™¨ä»»åŠ¡
            progress_task_id = f"migration_{task_id}"
            self.progress_manager.create_task(
                task_id=progress_task_id,
                task_type=TaskType.VECTORIZATION,
                total_items=task.stats["total_vectors"]
            )
            
            logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œè¿ç§»ä»»åŠ¡: {task_id}")
            
            if task.strategy == MigrationStrategy.FULL_REBUILD:
                success = await self._execute_full_rebuild(task, api_manager)
            elif task.strategy == MigrationStrategy.INCREMENTAL:
                success = await self._execute_incremental_migration(task, api_manager)
            elif task.strategy == MigrationStrategy.PARALLEL_BUILD:
                success = await self._execute_parallel_build(task, api_manager)
            elif task.strategy == MigrationStrategy.SELECTIVE:
                success = await self._execute_selective_migration(task, api_manager)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è¿ç§»ç­–ç•¥: {task.strategy}")
            
            if success:
                task.status = MigrationStatus.COMPLETED
                task.progress = 100.0
                logger.info(f"âœ… è¿ç§»ä»»åŠ¡å®Œæˆ: {task_id}")
            else:
                task.status = MigrationStatus.FAILED
                logger.error(f"âŒ è¿ç§»ä»»åŠ¡å¤±è´¥: {task_id}")
            
            task.end_time = datetime.now().isoformat()
            
            # å®Œæˆè¿›åº¦ç®¡ç†å™¨ä»»åŠ¡
            self.progress_manager.complete_task(progress_task_id, success=success)
            
            return success
            
        except Exception as e:
            task.status = MigrationStatus.FAILED
            task.error_message = str(e)
            task.end_time = datetime.now().isoformat()
            logger.error(f"âŒ è¿ç§»ä»»åŠ¡å¼‚å¸¸: {task_id} - {e}")
            return False
    
    async def _execute_full_rebuild(self, task: MigrationTask, api_manager: UnifiedAPIManager) -> bool:
        """æ‰§è¡Œå®Œå…¨é‡å»ºè¿ç§»"""
        from_vectors = self.vector_storage.get(task.from_version, {})
        to_version = task.to_version
        
        if to_version not in self.vector_storage:
            self.vector_storage[to_version] = {}
        
        processed = 0
        failed = 0
        
        for text_id, vector_data in from_vectors.items():
            try:
                # ä½¿ç”¨æ–°æ¨¡å‹é‡æ–°ç”Ÿæˆå‘é‡
                if api_manager:
                    # è°ƒç”¨APIé‡æ–°ç”Ÿæˆå‘é‡
                    to_model = self.model_versions[to_version]
                    # provider = ModelProvider.QWEN if to_model.provider == "qwen" else ModelProvider.DEEPSEEK  # åŸä»£ç ä¿ç•™
                    provider = ModelProvider.QWEN  # å½“å‰ä¸“æ³¨åƒé—®API
                    
                    embedding_result = await api_manager.create_embeddings(
                        texts=[vector_data.text_content],
                        provider=provider
                    )
                    
                    if embedding_result.get("success"):
                        new_vector = embedding_result["embeddings"][0]
                        task.stats["api_calls"] += 1
                    else:
                        logger.warning(f"âš ï¸ å‘é‡ç”Ÿæˆå¤±è´¥: {text_id}")
                        failed += 1
                        continue
                else:
                    # æ¨¡æ‹Ÿæ–°å‘é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
                    to_model = self.model_versions[to_version]
                    new_vector = np.random.random(to_model.dimension).tolist()
                
                # å­˜å‚¨æ–°å‘é‡
                new_vector_data = VectorData(
                    text_id=text_id,
                    text_content=vector_data.text_content,
                    vector=new_vector,
                    model_version=to_version,
                    created_at=datetime.now().isoformat(),
                    metadata=vector_data.metadata
                )
                
                self.vector_storage[to_version][text_id] = new_vector_data
                processed += 1
                
                # æ›´æ–°è¿›åº¦
                task.progress = (processed / len(from_vectors)) * 100
                task.stats["processed_vectors"] = processed
                task.stats["failed_vectors"] = failed
                
                # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
                await asyncio.sleep(0.01)
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†å‘é‡å¤±è´¥: {text_id} - {e}")
                failed += 1
        
        return failed == 0
    
    async def _execute_incremental_migration(self, task: MigrationTask, api_manager: UnifiedAPIManager) -> bool:
        """æ‰§è¡Œå¢é‡è¿ç§»"""
        # å¢é‡è¿ç§»åªå¤„ç†æ–°å¢çš„å‘é‡
        # è¿™é‡Œç®€åŒ–å®ç°
        return await self._execute_full_rebuild(task, api_manager)
    
    async def _execute_parallel_build(self, task: MigrationTask, api_manager: UnifiedAPIManager) -> bool:
        """æ‰§è¡Œå¹¶è¡Œæ„å»ºè¿ç§»"""
        # å¹¶è¡Œå¤„ç†å‘é‡
        from_vectors = self.vector_storage.get(task.from_version, {})
        
        # åˆ†æ‰¹å¤„ç†
        batch_size = 10
        batches = [list(from_vectors.items())[i:i + batch_size] 
                  for i in range(0, len(from_vectors), batch_size)]
        
        for batch in batches:
            # å¹¶è¡Œå¤„ç†å½“å‰æ‰¹æ¬¡
            tasks_batch = []
            for text_id, vector_data in batch:
                tasks_batch.append(self._process_single_vector(
                    task, text_id, vector_data, api_manager
                ))
            
            await asyncio.gather(*tasks_batch, return_exceptions=True)
            
            # æ›´æ–°è¿›åº¦
            task.stats["processed_vectors"] += len(batch)
            task.progress = (task.stats["processed_vectors"] / len(from_vectors)) * 100
        
        return True
    
    async def _process_single_vector(self, task: MigrationTask, text_id: str, 
                                   vector_data: VectorData, api_manager: UnifiedAPIManager):
        """å¤„ç†å•ä¸ªå‘é‡"""
        try:
            to_model = self.model_versions[task.to_version]
            
            if api_manager:
                # provider = ModelProvider.QWEN if to_model.provider == "qwen" else ModelProvider.DEEPSEEK  # åŸä»£ç ä¿ç•™
                provider = ModelProvider.QWEN  # å½“å‰ä¸“æ³¨åƒé—®API
                embedding_result = await api_manager.create_embeddings(
                    texts=[vector_data.text_content],
                    provider=provider
                )
                
                if embedding_result.get("success"):
                    new_vector = embedding_result["embeddings"][0]
                    task.stats["api_calls"] += 1
                else:
                    task.stats["failed_vectors"] += 1
                    return
            else:
                new_vector = np.random.random(to_model.dimension).tolist()
            
            # å­˜å‚¨æ–°å‘é‡
            new_vector_data = VectorData(
                text_id=text_id,
                text_content=vector_data.text_content,
                vector=new_vector,
                model_version=task.to_version,
                created_at=datetime.now().isoformat(),
                metadata=vector_data.metadata
            )
            
            if task.to_version not in self.vector_storage:
                self.vector_storage[task.to_version] = {}
            
            self.vector_storage[task.to_version][text_id] = new_vector_data
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å‘é‡å¤±è´¥: {text_id} - {e}")
            task.stats["failed_vectors"] += 1
    
    async def _execute_selective_migration(self, task: MigrationTask, api_manager: UnifiedAPIManager) -> bool:
        """æ‰§è¡Œé€‰æ‹©æ€§è¿ç§»"""
        # åªè¿ç§»é‡è¦çš„å‘é‡
        from_vectors = self.vector_storage.get(task.from_version, {})
        
        # é€‰æ‹©æ€§æ¡ä»¶ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºé€‰æ‹©å‰50%ï¼‰
        selected_vectors = dict(list(from_vectors.items())[:len(from_vectors)//2])
        
        processed = 0
        for text_id, vector_data in selected_vectors.items():
            await self._process_single_vector(task, text_id, vector_data, api_manager)
            processed += 1
            task.progress = (processed / len(selected_vectors)) * 100
        
        return True
    
    def _get_migration_task(self, task_id: str) -> Optional[MigrationTask]:
        """è·å–è¿ç§»ä»»åŠ¡"""
        for task in self.migration_tasks:
            if task.task_id == task_id:
                return task
        return None
    
    def deprecate_model_version(self, version_id: str, migration_path: str = None):
        """åºŸå¼ƒæ¨¡å‹ç‰ˆæœ¬"""
        if version_id in self.model_versions:
            self.model_versions[version_id].deprecated_at = datetime.now().isoformat()
            self.model_versions[version_id].migration_path = migration_path
            self._save_versions()
            logger.info(f"ğŸ—‘ï¸ åºŸå¼ƒæ¨¡å‹ç‰ˆæœ¬: {version_id}")
    
    def get_version_info(self, version_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ç‰ˆæœ¬ä¿¡æ¯"""
        if version_id not in self.model_versions:
            return None
        
        version = self.model_versions[version_id]
        vector_count = len(self.vector_storage.get(version_id, {}))
        
        return {
            **asdict(version),
            "vector_count": vector_count,
            "storage_size_mb": self._calculate_storage_size(version_id),
            "active_migrations": len([t for t in self.migration_tasks 
                                    if (t.from_version == version_id or t.to_version == version_id) 
                                    and t.status == MigrationStatus.RUNNING])
        }
    
    def _calculate_storage_size(self, version_id: str) -> float:
        """è®¡ç®—å­˜å‚¨å¤§å°(MB)"""
        version_dir = self.storage_path / version_id
        if not version_dir.exists():
            return 0.0
        
        total_size = 0
        for file_path in version_dir.rglob("*"):
            if file_path.is_file():
                total_size += file_path.stat().st_size
        
        return total_size / (1024 * 1024)  # è½¬æ¢ä¸ºMB
    
    def list_versions(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰ç‰ˆæœ¬"""
        return [self.get_version_info(vid) for vid in self.model_versions.keys()]
    
    def get_migration_history(self) -> List[Dict[str, Any]]:
        """è·å–è¿ç§»å†å²"""
        return [asdict(task) for task in self.migration_tasks]
    
    def cleanup_old_versions(self, keep_count: int = 3):
        """æ¸…ç†æ—§ç‰ˆæœ¬"""
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„å‡ ä¸ªç‰ˆæœ¬
        sorted_versions = sorted(
            self.model_versions.items(),
            key=lambda x: x[1].created_at,
            reverse=True
        )
        
        for version_id, version in sorted_versions[keep_count:]:
            if version.deprecated_at:  # åªæ¸…ç†å·²åºŸå¼ƒçš„ç‰ˆæœ¬
                # åˆ é™¤å­˜å‚¨æ•°æ®
                version_dir = self.storage_path / version_id
                if version_dir.exists():
                    shutil.rmtree(version_dir)
                
                # åˆ é™¤å†…å­˜æ•°æ®
                del self.model_versions[version_id]
                if version_id in self.vector_storage:
                    del self.vector_storage[version_id]
                
                logger.info(f"ğŸ§¹ æ¸…ç†æ—§ç‰ˆæœ¬: {version_id}")
        
        self._save_versions()


# å…¨å±€ç‰ˆæœ¬ç®¡ç†å™¨å®ä¾‹
version_manager = VectorModelVersioning()

def register_model(provider: str, model_name: str, dimension: int = 1536) -> str:
    """æ³¨å†Œæ¨¡å‹ç‰ˆæœ¬ï¼ˆç®€åŒ–æ¥å£ï¼‰"""
    return version_manager.register_model_version(provider, model_name, dimension=dimension)

def create_migration(from_version: str, to_version: str) -> str:
    """åˆ›å»ºè¿ç§»ä»»åŠ¡ï¼ˆç®€åŒ–æ¥å£ï¼‰"""
    return asyncio.run(version_manager.create_migration_task(from_version, to_version))


# æ¼”ç¤ºå‡½æ•°
async def demo_vector_versioning():
    """æ¼”ç¤ºå‘é‡æ¨¡å‹ç‰ˆæœ¬ç®¡ç†"""
    print("ğŸ¯ å‘é‡æ¨¡å‹ç‰ˆæœ¬ç®¡ç†æ¼”ç¤º")
    print("=" * 60)
    
    vm = version_manager
    
    # æ³¨å†Œæ¨¡å‹ç‰ˆæœ¬
    print("\nğŸ“ æ³¨å†Œæ¨¡å‹ç‰ˆæœ¬")
    v1 = vm.register_model_version("qwen", "text-embedding-v1", dimension=1536)
    v2 = vm.register_model_version("qwen", "text-embedding-v2", dimension=1536)
    # v3 = vm.register_model_version("deepseek", "deepseek-embedding", dimension=1536)  # æ¼”ç¤ºä»£ç ä¿ç•™
    
    print(f"   ç‰ˆæœ¬1: {v1}")
    print(f"   ç‰ˆæœ¬2: {v2}")
    # print(f"   ç‰ˆæœ¬3: {v3}")  # å¯¹åº”æ³¨é‡Šçš„v3
    
    # å­˜å‚¨ä¸€äº›æµ‹è¯•å‘é‡
    print("\nğŸ’¾ å­˜å‚¨æµ‹è¯•å‘é‡")
    test_vectors = [
        ("text1", "è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬", [0.1] * 1536),
        ("text2", "è¿™æ˜¯ç¬¬äºŒä¸ªæµ‹è¯•æ–‡æœ¬", [0.2] * 1536),
        ("text3", "è¿™æ˜¯ç¬¬ä¸‰ä¸ªæµ‹è¯•æ–‡æœ¬", [0.3] * 1536)
    ]
    
    vm.store_vectors(v1, test_vectors)
    print(f"   å­˜å‚¨äº† {len(test_vectors)} ä¸ªå‘é‡åˆ°ç‰ˆæœ¬ {v1}")
    
    # åˆ†æè¿ç§»éœ€æ±‚
    print("\nğŸ” åˆ†æè¿ç§»éœ€æ±‚")
    analysis = vm.analyze_migration_requirements(v1, v2)
    print(f"   è¿ç§»å¤æ‚åº¦: {analysis['migration_complexity']}")
    print(f"   æ¨èç­–ç•¥: {analysis['recommended_strategy']}")
    print(f"   å‘é‡æ•°é‡: {analysis['vector_count']}")
    print(f"   é¢„ä¼°æ—¶é—´: {analysis['estimated_time_hours']:.3f} å°æ—¶")
    
    # åˆ›å»ºè¿ç§»ä»»åŠ¡
    print("\nğŸ“‹ åˆ›å»ºè¿ç§»ä»»åŠ¡")
    migration_id = await vm.create_migration_task(v1, v2)
    print(f"   è¿ç§»ä»»åŠ¡ID: {migration_id}")
    
    # æ‰§è¡Œè¿ç§»ï¼ˆæ¨¡æ‹Ÿï¼‰
    print("\nğŸš€ æ‰§è¡Œè¿ç§»")
    success = await vm.execute_migration(migration_id)
    print(f"   è¿ç§»ç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
    
    # æŸ¥çœ‹ç‰ˆæœ¬ä¿¡æ¯
    print("\nğŸ“Š ç‰ˆæœ¬ä¿¡æ¯")
    for version_id in [v1, v2, v3]:
        info = vm.get_version_info(version_id)
        if info:
            print(f"   {version_id}: {info['provider']}/{info['model_name']} ({info['vector_count']} å‘é‡)")
    
    # æŸ¥çœ‹è¿ç§»å†å²
    print("\nğŸ“œ è¿ç§»å†å²")
    history = vm.get_migration_history()
    for task in history:
        print(f"   {task['task_id']}: {task['status']} ({task['progress']:.1f}%)")


if __name__ == "__main__":
    asyncio.run(demo_vector_versioning())